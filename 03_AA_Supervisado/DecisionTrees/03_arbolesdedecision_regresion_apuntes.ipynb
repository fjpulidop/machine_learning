{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nota importante\n",
    "La teoría de árboles de decisión queda vista en el apartado de clasificación.\n",
    "\n",
    "Esto es una continuación, donde estudiaremos la regresión en los árboles de decisión.\n",
    "\n",
    "Dejo como puntos a repasar generales:\n",
    "- Ventajas y Desventas\n",
    "- Consejos prácticos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación del entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"decision_trees\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión en árboles de decisión\n",
    "Podemos aplicar árboles de decisión a problemas de regresión utilizando la clase```DecisionTreeRegresor```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que en la configuración de clasificación, el método de ajuste tomará como argumentos las matrices X e y, solo que en este caso se espera que y tenga valores float en lugar de int:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "X = [[0, 0], [2, 2]]\n",
    "y = [0.5, 2.5]\n",
    "clf = tree.DecisionTreeRegressor()\n",
    "clf = clf.fit(X, y)\n",
    "clf.predict([[1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: Tree Pages: 1 -->\r\n",
       "<svg width=\"216pt\" height=\"165pt\"\r\n",
       " viewBox=\"0.00 0.00 216.00 165.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 161)\">\r\n",
       "<title>Tree</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-161 212,-161 212,4 -4,4\"/>\r\n",
       "<!-- 0 -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\r\n",
       "<path fill=\"#f2c09c\" stroke=\"black\" d=\"M139,-157C139,-157 68,-157 68,-157 62,-157 56,-151 56,-145 56,-145 56,-101 56,-101 56,-95 62,-89 68,-89 68,-89 139,-89 139,-89 145,-89 151,-95 151,-101 151,-101 151,-145 151,-145 151,-151 145,-157 139,-157\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"103.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">[0, 0] &lt;= 1.0</text>\r\n",
       "<text text-anchor=\"middle\" x=\"103.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">mse = 1.0</text>\r\n",
       "<text text-anchor=\"middle\" x=\"103.5\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\r\n",
       "<text text-anchor=\"middle\" x=\"103.5\" y=\"-96.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 1.5</text>\r\n",
       "</g>\r\n",
       "<!-- 1 -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\r\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M83,-53C83,-53 12,-53 12,-53 6,-53 0,-47 0,-41 0,-41 0,-12 0,-12 0,-6 6,-0 12,-0 12,-0 83,-0 83,-0 89,-0 95,-6 95,-12 95,-12 95,-41 95,-41 95,-47 89,-53 83,-53\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"47.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">mse = 0.0</text>\r\n",
       "<text text-anchor=\"middle\" x=\"47.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\r\n",
       "<text text-anchor=\"middle\" x=\"47.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 0.5</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;1 -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M83.9139,-88.9485C78.7639,-80.2579 73.1953,-70.8608 67.9988,-62.0917\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"70.8605,-60.0553 62.7514,-53.2367 64.8384,-63.624 70.8605,-60.0553\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"56.5555\" y=\"-73.7568\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 2 -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\r\n",
       "<path fill=\"#e58139\" stroke=\"black\" d=\"M196,-53C196,-53 125,-53 125,-53 119,-53 113,-47 113,-41 113,-41 113,-12 113,-12 113,-6 119,-0 125,-0 125,-0 196,-0 196,-0 202,-0 208,-6 208,-12 208,-12 208,-41 208,-41 208,-47 202,-53 196,-53\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"160.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">mse = 0.0</text>\r\n",
       "<text text-anchor=\"middle\" x=\"160.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\r\n",
       "<text text-anchor=\"middle\" x=\"160.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = 2.5</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;2 -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>0&#45;&gt;2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M123.436,-88.9485C128.733,-80.1664 134.465,-70.6629 139.802,-61.815\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"142.808,-63.6074 144.976,-53.2367 136.814,-59.9919 142.808,-63.6074\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"150.983\" y=\"-73.8043\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x1bcd68df790>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Source\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(\n",
    "        clf,\n",
    "        out_file=os.path.join(IMAGES_PATH, \"clf_tree.dot\"),\n",
    "        feature_names=X,\n",
    "        class_names=y,\n",
    "        rounded=True,\n",
    "        filled=True\n",
    "    )\n",
    "\n",
    "Source.from_file(os.path.join(IMAGES_PATH, \"clf_tree.dot\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ventajas VS Desventajas\n",
    "\n",
    "Algunas ventajas de los árboles de decisión son:\n",
    "\n",
    "- Sencillo de entender e interpretar. Los árboles se pueden visualizar.\n",
    "\n",
    "- Requiere poca preparación de datos. Otras técnicas a menudo requieren la normalización de datos, es necesario crear variables ficticias y eliminar valores en blanco. Sin embargo, tenga en cuenta que este módulo no admite valores perdidos.\n",
    "\n",
    "- El costo de usar el árbol (es decir, predecir datos) es logarítmico en el número de puntos de datos usados para entrenar el árbol.\n",
    "\n",
    "- Capaz de manejar datos tanto numéricos como categóricos. Otras técnicas suelen estar especializadas en analizar conjuntos de datos que tienen un solo tipo de variable.\n",
    "\n",
    "- Capaz de manejar problemas de múltiples salidas.\n",
    "\n",
    "- Utiliza un modelo de caja blanca. Si una situación dada es observable en un modelo, la explicación de la condición se explica fácilmente mediante lógica booleana. Por el contrario, en un modelo de caja negra (por ejemplo, en una red neuronal artificial), los resultados pueden ser más difíciles de interpretar.\n",
    "\n",
    "- Posible validar un modelo mediante pruebas estadísticas. Eso permite tener en cuenta la fiabilidad del modelo.\n",
    "\n",
    "- Se desempeña bien incluso si el modelo real a partir del cual se generaron los datos viola sus suposiciones.\n",
    "\n",
    "Las desventajas de los árboles de decisión incluyen:\n",
    "\n",
    "- Los aprendices de árboles de decisiones pueden crear árboles demasiado complejos que no generalizan bien los datos. Esto se llama sobreajuste. Para evitar este problema son necesarios mecanismos como la poda, el establecimiento del número mínimo de muestras necesarias en un nodo de la hoja o el establecimiento de la profundidad máxima del árbol.\n",
    "\n",
    "- Los árboles de decisión pueden ser inestables porque pequeñas variaciones en los datos pueden resultar en la generación de un árbol completamente diferente. Este problema se mitiga mediante el uso de árboles de decisión dentro de un conjunto.\n",
    "\n",
    "- Se sabe que el problema de aprender un árbol de decisión óptimo es NP-completo en varios aspectos de la optimalidad e incluso para conceptos simples. En consecuencia, los algoritmos prácticos de aprendizaje del árbol de decisiones se basan en algoritmos heurísticos, como el algoritmo codicioso, donde se toman decisiones localmente óptimas en cada nodo. Dichos algoritmos no pueden garantizar la devolución del árbol de decisiones globalmente óptimo. Esto se puede mitigar entrenando varios árboles en un aprendiz de conjunto, donde las características y muestras se muestrean al azar con reemplazo.\n",
    "\n",
    "- Hay conceptos que son difíciles de aprender porque los árboles de decisión no los expresan fácilmente, como XOR, problemas de paridad o multiplexor.\n",
    "\n",
    "- Los aprendices del árbol de decisiones crean árboles sesgados si dominan algunas clases. Por lo tanto, se recomienda equilibrar el conjunto de datos antes de ajustarlo al árbol de decisiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consejos de uso práctico\n",
    "- Los árboles de decisión tienden a sobreajustarse a los datos con una gran cantidad de características. Es importante obtener la proporción correcta de muestras con respecto al número de características, ya que es muy probable que un árbol con pocas muestras en un espacio de gran dimensión se sobreajuste.\n",
    "\n",
    "- Considere realizar la reducción de dimensionalidad ( PCA , ICA o selección de características ) de antemano para darle a su árbol una mejor oportunidad de encontrar características que sean discriminatorias.\n",
    "\n",
    "- Comprender la estructura del árbol de decisiones ayudará a obtener más información sobre cómo el árbol de decisiones hace predicciones, lo cual es importante para comprender las características importantes de los datos.\n",
    "\n",
    "- Visualice su árbol mientras entrena utilizando la export función. Use max_depth=3 como una profundidad de árbol inicial para tener una idea de cómo el árbol se ajusta a sus datos y luego aumente la profundidad.\n",
    "\n",
    "- Recuerde que la cantidad de muestras necesarias para poblar el árbol se duplica por cada nivel adicional al que crece el árbol. Use max_depth para controlar el tamaño del árbol para evitar el sobreajuste.\n",
    "\n",
    "- Utilice min_samples_splito min_samples_leaf para asegurarse de que varias muestras informan cada decisión en el árbol, controlando qué divisiones se considerarán. Un número muy pequeño generalmente significará que el árbol se ajustará en exceso, mientras que un número grande evitará que el árbol aprenda los datos. Pruébelo min_samples_leaf=5 como valor inicial. Si el tamaño de la muestra varía mucho, se puede utilizar un número flotante como porcentaje en estos dos parámetros. Si bien min_samples_split puede crear hojas arbitrariamente pequeñas, min_samples_leaf garantiza que cada hoja tenga un tamaño mínimo, evitando nodos de hojas de baja varianza y sobreajuste en problemas de regresión. Para la clasificación con pocas clases, min_samples_leaf=1 suele ser la mejor opción.\n",
    "\n",
    "- Equilibre su conjunto de datos antes del entrenamiento para evitar que el árbol esté sesgado hacia las clases dominantes. El balance de clases se puede hacer muestreando un número igual de muestras de cada clase, o preferiblemente normalizando la suma de los pesos de las muestras ( sample_weight) para cada clase al mismo valor. También tenga en cuenta que los criterios de prepoda basados en el peso, como min_weight_fraction_leaf, estarán menos sesgados hacia las clases dominantes que los criterios que no son conscientes de los pesos de la muestra, como min_samples_leaf.\n",
    "\n",
    "- Si se pesan las muestras, será más fácil optimizar la estructura del árbol utilizando un criterio de poda previa basado en el peso, como min_weight_fraction_leaf, que asegura que los nudos de las hojas contienen al menos una fracción de la suma total de los pesos de las muestras.\n",
    "\n",
    "- Todos los árboles de decisión utilizan np.float32 matrices internamente. Si los datos de entrenamiento no están en este formato, se realizará una copia del conjunto de datos.\n",
    "\n",
    "- Si la matriz de entrada X es muy escasa, se recomienda convertirla a escasa csc_matrix antes de llamar a ajuste y escasa csr_matrix antes de llamar a predecir. El tiempo de entrenamiento puede ser varios órdenes de magnitud más rápido para una entrada de matriz escasa en comparación con una matriz densa cuando las entidades tienen valores cero en la mayoría de las muestras."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
