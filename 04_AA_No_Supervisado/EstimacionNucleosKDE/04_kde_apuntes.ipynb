{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este apartado lo voy a estudiar con preguntas y respuestas dado que tiene aspectos muy teóricos y con mucha complejidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios del Máster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KDE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ¿Qué elecciones pueden llevar a histogramas con distinta apariencia usando los mismos datos y cómo se puede evitar este problema?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El tamaño y posición de los interalos (bins). Un histograma se puede ver como pilas debloques, uno por cada punto Si en lugar deapilar los bloques alineados con los intervalos se alinean con los puntos que representan y se suman las alturas en de cada posición se consigue una mejor representación de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ¿Qué es un kernel, qué parámetros tiene y cuál es su efecto?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un kernel es una función positiva controlada por el parámetro bandwitdh (ancho de banda) que especifica la forma de la distrución que se usa para suavizar la función de densidad de la probabilidad que se obtiene a partir de los datos. El parámetro controla la compensadcion sesgo-varianza en la estimacióon de la función de densidad, un ancho de banda demasiado estrecho conduce a una estimación de alta varianza (es decir, overfitting), donde la presencia o ausencia de un solo punto crea una gran diferencia. Un ancho de banda demasiado amplio conduce a una estimación de alto sesgo( es decir, underfitting) donde el núcleo ancho elimina la estructura de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ¿Cuáles son los tipos de kernel disponibles en scikit-learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gaussian, tophat, epanechnikov, exponential, linear, cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Describir un procedimiento para buscar un valor apropiado para el parámetro bandwith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando el procedimiento GridSearchCV al que se le manda el tipo de kernel, un rango de valores para el parñametro y utilizando un procedimiento de valicación cruzada de manera que se obtenga el valor del parámetro que de la mejor puntuación (parámetro scoring) para la métrica elegida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propagación y extensión de etiquetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ¿En qué consiste el aprendizaje semi-supervisado?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El aprendizaje semi-supervisado es un tipo de aprendizaje en el que el entrenamiento se efectúa con datos etiquetados y no-etiquetados. Se hace uso de los datos adicionales sin etiquetar para capturar mejor la forma de la distribución subyacente de los datos y generalizar mejor a nuevas muestras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ¿Cuáles son los modelos disponibles en scikit-learn para el aprendizaje semi-supervisado y cómo funcionan?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los modelos disponibles son LabelPropagation y LabelSpreading. Ambos construyen un grafo de similitud sobre todos los elementos del conjunto de datos de entrada, si la estructura de los datos no-etiquetados es consistente con la estructura de los datos etiquetados, estas etiquetas se propagan por el árbol a los datos no-etiquetados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios del libro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ¿Cómo definirías el agrupamiento? ¿Puedes nombrar algún algoritmo de agrupamiento?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el machine learning, el agrupamiento es la tarea no supervisada de agrupar juntas instancias similares. La noción de similitud epende de l tarea en la que se trabaja: por ejemplo, en algunos casos, dos instancias cercanas se considerarán similares, mientra que otrtas instancias similares pueden estar alejadas, siempre y cuando pertenezcan al mismo grupo abarrotado. Los algoritmos de agrupamiento populares incluyen K-Medias, DBSCAN, agrupamiento por aglomeración, BIRCH, Mean-Shift, propaagación de afinidad y agrupamiento espectral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ¿Cuáles son algunas de las aplicaciones principales de los algoritmos de agrupamiento?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las principales aplicaciones de los algoritos de agrupamiento incluyen análisis de datos, segmentación de mercado, sistemas de recomendación, motores de búsqueda, segmentación de imágenes, aprendizaje semisupervisado, reducción de la dimensionalidad, detección de anomalías y detección de novedades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Describe dos técnicas para seleccionar el número adecuado de grupos cuando se utiliza K-Medias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La regla del codo es una técnica simple para seleccionar el número de grupos cuando se utiliza K-medias: traza l inercia (la distancia cuadrática media entre cada instancia y su centroice más cercano) como una función del número de grupo s y encuentra el punto en la curva donde la incercia deja de caer deprisa(el \"codo\"). Esto suele acercarse al número óptimo de grupos. \n",
    "\n",
    "Otro enfoque es trazar la puntuación de la silueta como una función del número de grupos. A menudo, habrá un pico, y  el número óptimo de grupos suele estar cerca. La puntuación de la silueta es el coeficiente medio de la silueta sobre todas las instancias. Este coeficiente varía desde +1 para instancias que están muy metidas en sus grupos y lejos de otros grupos, hasta -1 para instancias que están muy cerca de otro grupo. También puedes trazar los diagramas de silueta y realizar un análisis más exhaustivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. ¿Qué es la propagación de etiquetas? ¿Para qué la implementarías y cómo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etiquetar un conjunto de datos es costoso y lleva tiempo. Por tanto, es habitual tener muchas instancias sin etiquetar, pero pocas instancias etiquetadas. La propagación de etiquetas es una técnica que consiste en copiar algunas etiquetas ( o todas ) de las instancias etiquetadas en instancias sin etiquetar similares. Esto puede incrementar mucho la cantidad de instancias etiquetadas y por tanto permite que un algoritmo supervisado consiga un rendimiento mejor (se trata de una forma de aprendizaje smisupervsiado). Un enfoque es utilizar un algoritmo de agrupamiento como K-medias en todas las instancias y, después, para cada grupo, encontrar la entqieuta más común o la etiqueta de la instancia más representativa (es decir, la que está más cerca del centroide) y propagarla a las instancias sin etqiuetar en el mismo grupo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. ¿Puedes nombrar a dos algoritmos de agrupamiento que puedan escalar a conjuntos de datos grandes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-medias y BIRCH escalan bien a conjuntos de datos grandes\n",
    "\n",
    "DBSCAN y Mean-Shift buscan regiones de alta densidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. ¿Puedes pensar en un caso dónde sería útil el aprendizaje activo?¿cómo lo implementarías?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El aprendizaje activo es útil cuando tenemos muchas instancias sin etiquetar, pero el etiquetado es costoso. En este caso (que es muy común), en vez de seleccionar instancias de manera aleatoria para etiquetarlas, a menudo es preferible llevar a cabo un aprendizaje activo, donde expertos humanos interactúan con el algoritmo de apredizaje, proporcionando etiquetas para instancias específicas cuando el algoritmo lo solicita. Un enfoque común es el muestreo de incertidumbre (véase la descripción en la sección \"Aprendizaje activo\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. ¿Cuál es la diferencia entre la detección de anomalías y la detección de novedades?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mucha gente utiliza los términos \"detección de anomalías\" y \"detección de novedades\" de manera indistinta, pero no son exactamente lo mismo. En la detección de anomalías, el algoritmo se entrena con un conjunto de datos que contiene valores atípicos y el objetivo suele ser identificar estos valores atípicos (dentro del conjunto de entrenamiento), así como los valores atípicos entre instancias nuevas. En la detección de novedades, el algoritmo se entrena con un conjunto de datos que se presupone que está \"limpio\" y el objetivo es detectar novedades estrictamente entre instancias nuevas. Algunos algoritmos funcionan menjor para la detección de anomalías (por ejemplo, Isolation Forest), mientras que otros son más adecuados para la detección de novedades (por ejemplo, one-class SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. ¿Qué es una mezcla Gaussiana?¿Para qué tareas puede utilizarse?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un modelo de mezcla gaussiana (GMM) es un modelo probabilístico que da por supuesto qu elas instancias se han generado a partir de una mezcla de varias distribuciones gaussianas cuyos parámetros se desconocen. Dicho de otro modo, la suposición es que los datos se agrupan en un número finito de grupos, cada uno con forma elipsoidal (pero los grupos pueden tener diferentes formas elipsoidales, tamaños, orientaciones y densidades) y no sabemos a qué grupo pertenece cada instancia. Este modelo es útil para la estimación de densidad, el agrupamiento y la detección de anomalías."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. ¿Puedes nombrar dos técnicas para encontrar el número adecuado de grupos cuando se utiliza un modelo de mezclas gaussiana?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una manera de encontrar el número adecuado de grupos cuando se utiliza un modelo de mezclas gaussiana es trazar el criterio de información bayesianas (BIC) o el criterio de información Akaike (AIC) como una función del número de grupos y, a continuacion, eleegir el núemro de grupos que minimiza BIC o AIC. Otra técnica es utiliza un modelo Bayesiano de mezcla gaussiana que selecciona automáticmente el número de grupos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. El clásico conjunto de datos de caras Olivetti contiene 400 imágenes de 64x64 píxeles en escala de grises. Cada imagen se aplana a un vector ID de tamaño 4096- Se fotografió a 40 personas diferentes (10 veces a cada una) y la tarea habitual es entrenar un modelo que pueda predecir qué persona está representada en cada imagen- Cara el conjunto de datos utilizando la función sklearn.datasets.fetch_olivetti_faces() y a continuación divídelo en un conjunto de entrenamiennto, un conjunto de validación y un conjunto de prueba (ten en cuenta que el conjunto de datos ya está escalado entre 0 y 1). Puesto que el conjunto de datos es bastante pequeño, es probable que quieras utilizar muestreo estratificado para garantizar que hay el mismo número de imágenes persona en cada conjunto. A continuación, agrupa las imágenes utilizando K-Medias y aségurate de que tienes un buen número de grupos. Visualiza los grupos: ¿ves caras simiulares en cada grupo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Siguiendo con el conjunto de datos de caras Olivetti, entrena un clasificador para predecir qué persona estará representada en cada imagen y evalúalo en el conjunto de validación. A continuaicón, utiliza .MNedias como herramienta de reducción de dimensionalidad y entrena un clasificador en el conjunto reducido. Busca el número de grupos que permite al clasificador tener el mejoor rendimiento posible: ?qué rendimiento consigues? ¿Qué pasa sia gregas las características del conjunto reducido a las características originales (de nuevo, buscando el mejor número de grupos?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
