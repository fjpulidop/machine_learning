{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### código testeo de GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/device:GPU:0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "print('Available GPUs ')\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### código para liberar GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numba'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e838ab5890b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcuda\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numba'"
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probar cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de un clasificador de imágenes utilizando la API secuencial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Utilizaremos el dataset Fashion MNIST, que sustiuye a MNIST.\n",
    "- Tiene el formato de MNIST(70k imágenes en escala de grises de 28x28 íxeles, con 10 clases)\n",
    "- Las imágenes representan artículos de moda en lugar de números manuscritos, así que las clases son más diferenets y el problema se vuelve mucho más difícil que MNIST.\n",
    "    - MNIST: un modelo lineal sencillo llega a 92% de exactitud\n",
    "    - Fashion MNIST: en este caso solo llega a 83% de exactitud\n",
    "\n",
    "### Uso de Keras para cargar el conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cada imagen se representa como una matrix de 28x28. \n",
    "- Las intensidades de los píxeles se representn como enteros (0 a 255) en vez de flotantes(0.0 a 255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- El conjunto está dividido en entrenamiento y pruebas, pero no hay validación, tendremos que crear uno.\n",
    "- Dado que vamos a entrenar la red neuronal con descenso de gradiente, debemos escalar las características de entrada.\n",
    "    - Escalaremos las instensidades de los píxeles hasta el 0-1 diviéndolas entre 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000]/255.0, X_train_full[5000:] / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con MNIST, cuando la etiqueta es igual a 5, significa que la imagen representa el número 5 manuscrito. Sin embargo, con Fashion MNIST, necesitamos la lista de nombres de clase para saber con qué estamos tratando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coast\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\",\"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coast'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqkAAAEjCAYAAAAR5ZjkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAADYK0lEQVR4nOydd5hkRbn/PzV5ZndnZvOyy7IsOSwZCQoCkhEEEyCgoteA6FWRqxhAxYSXqxh/IiqgiICIiCCCiZwk5yCwbGJzmN1JO7F+f9T5Vlef7pmdnZ1weqnv88wz3X1Onz5Vp+qtt75vMtZaIiIiIiIiIiIiIrKEstG+gYiIiIiIiIiIiIg0opIaERERERERERGROUQlNSIiIiIiIiIiInOISmpERERERERERETmEJXUiIiIiIiIiIiIzCEqqREREREREREREZlDVFJHAcaYecaYI/o4drAx5qWRvqfNBf31bdZhjLHGmO029tgGrnmmMea+Tb+7kUfsj3zE/oiIiHijYViVVGPMacaYR40xLcaYJcaY24wxBw3Tbw27sE3aob9eY0x78P70ofgNa+291todN3AfRRWxpL+vMcZsnSxaFUNxT4OFMeYgY8wDxpi1xpjVxpj7jTFvGs17GgkYY+4yxqwxxlSP9r0MF4wxhxpjFg3w3Ngf+efG/hi639ms1phNReyPHJJ1st0Y02yMaUrWorOMMW9Ycq4Ux8ewPSxjzOeAHwLfAaYCWwE/A04crt8cblhrx+oPWACcEHz2u+H+/QEonccBfx3u+xgIjDH1wF+AnwATgBnAhUDHaN7XQLApyr0xZmvgYMAC7xiqeypVxP7IR+yPocPmuMZsCmJ/FMUJ1tpxwCzgu8B5wOXFTjTGlI/kjY00SnZ8WGuH/A9oAFqA9/ZxvBrXWYuTvx8C1cmx8TjlZgWwJnm9ZfDdM4G5QDPwGnA6sDOwHuhJfrdpONqVasM84Ih+jk9K7r0JWA3cC5QF3/0f4GlgLfB7oCY5diiwKPU75yXndgDXAr1Ae9LWLyTnlQHLkt9dgFsEW5K/A5Pj5wPzgeXAVUBD8t2tk/M/ljyPJcC5m9g/+/b1HJJneB/wveQZvwYcmxo/lyf38TrwLaA8ObYtcAewClgJ/A5oLPZcgJ2Sa5+avD8eeDJ5Jg8Au/fTzxWDbPdXgfuBS4C/pI79Gvh/wK3J+P03sG1w3ALbJa8PAhYChxU5Vp303YLkmf8cqO2nr+/HbRbWAi8ChwfHpwM348boK8BHNzRPgTHJ+OsNxtj02B+xPza2PzZBvmz2a0zsj03uk3mk1mhgv2Rczknm26U4YqcVOCIZ739M+uI14NOp7z4KrEvm1SXJ5zXA1bg1qQl4BJg62u3fXMbHcHXIMUA3fSz0wDeAh4ApwGScwvDN5NhE4N1AHTAO+ANwU3JsTDJAdkzebwHsGnTUfaM5AVLHL8ItDpXJ38GACb77cDIhJgAvAGclxw6lUEl9EphJstD0MfkOAB5MXm+NW7QqguMfxi0y2wBjgRuB36bOvzbp492SAdln+wbQP/XJpP0NcCwwPjWou4CPAuXAJ5KJof65CbgsuZcpSV99PDm2HXAkblJNBu4Bfph+LsDeuEX6+OTzvXHK+f7Jb34wObe6r34eZLtfAc4G9knaODU49mvcYr8fUIFTsK8LjtukfUfjFJD90seS1z/EKQ4TcHPkFuCiPu7nTNxcPAc3Dk/BKSMTkuN343bTNcCeyXM/fADz9FCCcRr7I/bHYPpjE+bZZr/GxP7Y5D6ZR5E1DLcufCKZb2uBt+BInDrgMdxGsgq3Vs4Fjk6+9yDw/uT1WOCA5PXHkzlWh1tb9gHqR7v9m8v4GK4OOR1Y2s/xV4HjgvdHA/P6OHdPYE3QIU1Jh9WmzhvRCdPXBEg99D+TLBxFvntG8P5i4OfJ60MpVFI/vKHfBr4JXJC83ppCJfVfwNnB+x1xi2RFcP5OqXu6fBP7aOdEECxKJsjNODPDmcArwXl1ye9PS453hM8XeB9wZx+/cRLwRKpvLkx+87Dg80s16YLPXgIO6aufB9Heg5I+nZS8fxE4Jzj+a+BXwfvjgBeD9xb4Eo7t3i11bSkoBrfrDxm2A4HX+rinMwk2AMlnDwPvxynkPcC44NhFwK+T133O0/Q4jf0R+2Nj+2MT59pmv8bE/tjkPplHcSX1IeAryXy7Kvh8f2BB6twvAVcmr+/BrS2TUud8mJRlLmt/pTw+hssndRUwqR/fvuk4QSvMTz7DGFNnjLnMGDPfGLMONzAajTHl1tpW3E7/LGCJMeZWY8xOw9SGAcMYs1UYVJV8/H841uTvxpi5xpgvpr62NHjdhtuZ9YWFA7iNDfmjFuvzCpxSWOx3/DMZLKy1L1hrz7TWbokzr0zHsTwQtN9a25a8HIvzHarEPd8mY0wTjlWdAmCMmWKMuc4Y83oyPq7GuTiEOAt4wFp7Z/DZLOBcXTO57sxUGwfSz/3hg8DfrbUrk/fXJJ+F2NBz/yxwvbX2mT5+YzLJjj9ox+3J533hdZtIjQR6ttOB1dba5tSxGcnrPufpABH7Ix+xP4YOb6g1ZgCI/TFwzMBZLCBf5s8CpqfWiC+TWyP/C9gBeNEY84gx5vjk898CfwOuM8YsNsZcbIypHPZWbBxKdnwMl5L6IM4f4aQ+ji/GDQhhq+QzgHNxLN/+1tp64K3J5wbAWvs3a+2ROFr5ReCXyfFQyI4orLULbH5QFdbaZmvtudbabYATgM8ZYw4f7E/0994YMw3XH4/3cT4U7/NunG+NMDN1fDFDBGvti7id65wNnLoQx6ROstY2Jn/11tpdk+MX4dq3ezI+ziAZGwHOArYyxvwgdd1vB9dstNbWWWuvDW9zcK0DY0wtcDJwiDFmqTFmKc6EuocxZo+NuNR7gZOMMZ/t4/hKnL/frkE7GjTu+sAMY0zYR3q2i4EJxphxqWOvJ6/7m6f99lXsj3zE/hhyvKHWmAEg9scAYFx2mRm4mAjIb8NCnMUhXCPGWWuPA7DWvmytfR+OMPlf4AZjzBhrbZe19kJr7S7Am3GxDx8YsUYNDCU7PoZFSbXWrsX5dfw/Y8xJiSZeaYw51hhzMc738XxjzGRjzKTk3KuTr4/DCdkmY8wE4Gu6rjFmqjHmHcaYMThFpgVnjgKnbG1pjKkajjZtLIwxxxtjtkuE/zrcffZs4GsDxTKcv4xwHHB7wIaswDmHh+dcC5xjjJltjBmLi/D7vbW2OzjnguRZ7Qp8CBfQNSgYY3YyxpxrjNkyeT8TZ7Z/qL/vWWuXAH8Hvm+MqTfGlBljtjXGHJKcMo7EEdsYMwP4fJHLNON8cN5qjPlu8tkvgbOMMfsbhzHGmLenFuBNwUm457sLzhyyJ87d4V42TmAtBg4HPm2MOTt90Frbi2vLD4wxYpdnGGOO7ueaU5LrVRpj3pvc11+ttQtxZqqLjDE1xpjdcWyBMlX0N0+XARONMQ19/OZJxP4IcRKxP4YMcY3JR+yP/pGsJccD1wFX92GJeBhYZ4w5zxhTa4wpN8bMSRRbjDFnGGMmJ3OsKflOjzHmMGPMbsZlB1iHc+kZqrV+SFDS42NT/QX6+8P5QTyK85FaiotafTPOCf/HuOjtJclrRbdPB+5KGvsfnFOyxZmmt8A58q/FDZK7gF2S71Ul118NrBzOdiW/N4/+fVLPSc5pxflHXtDXd4Gv4yYOFPdJTfufnohz/m7CZQm4AXhP6pxv4JTVJlxQVRlu4C1MPr+aJJiJwuj+pSRZAzahf2YA1+NYl9bk/2W4gKozSfmqkB/40YDzIV2UPOsnyEXo74pzbm/BBTqd21d/4QJHniLnAH4MLvKyKRl3fyDxt9vQ8xxAe28Hvl/k85OT/qzAMcnfCo6ln3XYB7NxJpePFDlWg9tkzMUJxRcIolBTv38mLnr7p0lf/gc4Kji+JS5aczXOL+ms4Fif8zQ5fgW5iNbpsT9ifwy0P4bqj814jYn9scl9MQ+nXDUn9/8g8ElymWJ+TTDfgr64Num7NThSRevJ1bjg2xbgOeCk5PP34eIbWnGK2Y8ZZHaYOD4K/xRNHVGiMM7HZCkuUGLtIK+xNS51RKXNZ1YjIiIiIiIiIkYFb9jKC5sRJuBY2kEpqBERERERERERWURkUiMikxoRERERERGROUQlNSIiIiIiIiIiInOI5v6IiIiIiIiIiIjMISqpERERERERERERmUNf1QcGglH3E7DWYvJyUG8UBv3FPrBR/fHss88C0NraygsvvADApZdeCsA111wDwLbbbtvvNe67z+Uj/ta3vgXAN7/5TcrLywGYPXs2AOPHjx/oLY1qf2QQsT/yMdT9AbFP0oj9kY9B9UfowpZeH4477jjGjnV1Dbq7nfv90Ucfzcc//vG883p7ewEoK9skHmdU+6O/frjzzjsB+OQnP0l1dTUA69ev99+75ZZbANh+++3zvtfb2+uvNYi1NxPjI8S//vUvAL8G77zzzmy33XZ55zQ1NdHU1ATADTfcAMChhx4KwDHHHMOYMWMG+/OZGB/FnqPmQ29vLyeeeCIAq1e7Il233347K1asAOAf//jHRl13Ayj6hU3xSR0ygSqF7Y9//CP//ve/Aejpcflgp02bxs477wzAYYcdBsD+++8/FD87KgPk6qtdftyWFlc9dfLkyey4444AfOlLXwLgrrvuAmDLLbfkzW9+MwC1tbX+2CuvvAJAR0cH4IQswA9/+EOefvppAJYtc4WkZs2axTve8Y6B3FrmBMgoI/ZHPqKSWog4RvKR2UX38593NT8uu+wyr4Ro0a2qquLXv/41gJe3Q4TMjY8//vGPALznPe8BYI899mDNmjUAXtmqrq7m+eefB+Dmm28GcmtM3s1svDIyqv3R2toKwBe/+EVefPFFILcOb7311oBbczU+pIi9+uqrfkMjzJs3z7/Wpue2227byNvPzvhYudJVan7f+94HwP333w+4uaENm55zb2+vJ8P02c9//nMATjnllIJr9/T0+PM3gOwoqZoA//Vf/wXAo48+CridbUWFI3e1gy0rK/M7PH22ww47AHDuuefykY98ZLC3MeID5C9/+Qt33HEHAGeccQYAixcvprGxEcArq9rFXnLJJX5iaeI888wzTJrkStV/4QtfAOC0004D4JFHHvF9VVdXB8B1113HMcccAxQXNAEyM2Eygtgf+YhKaiHiGMlHZvrjM5/5DAAPP/wwkFuEJ0yYwMKFrly75O64ceNob28HnJIC8OlPfxpwTNkmsKoj3h/FrIuXXnopf/jDHwD4z3/+A7g2A5xwwgleMZcu8Ic//IEnnngCyLHNM2e6itnvfOc7+e///u+86/f29g60b0Z1fOi+m5qa/BoqSFmtqanxSqfGR0VFhSeGBOkpLS0t/rtS/Ispan1gxPqj2IbigQceAJwe8eSTTwJQX18PwJQpUwBYvny5P1+MO+CZ5WnTpgH4OTV+/Hi+9rWvAQxGNyvaH9EnNSIiIiIiIiIiInMYdia12C506tSpQG5329DQ4C5oLZWVlUBuB1deXu5N/4LME1tuuaXX4IveYP/miBHf1f30pz/l9ddfB2CXXXYBYKuttvLHa2pqgNxuvre31/t8rFu3DoD99tuPyZMnA44VAJg7dy4AXV1dvr8XLVrkj4lV/exnP9vf7WWGBckIYn/kIzKphYhjJB+Z6I9LL72Uiy++GIA5c+YAuTVj9erVni1qa2sDnJl7iy22AGDp0qV5x8QwDRIj3h8hq/nLX/4ScK4OYkK1rspCt3DhQr8uaB25+eabmTFjBpBjE7UGv/7663zyk58E4KKLLtrY+x+V8aHYjQsvvBBwLKB8StNmfDGkkDPjr1+/3usq6g+Nk4qKCv8dsa2/+tWvNhhPkmBU+uPKK68Ecv3R29vr9S7pD9JFli5dyqxZs4DcGHj22Wc9g6q51NXVBThdS7qK4mJkzYDB6WSRSY2IiIiIiIiIiMgcNiW6f4Mo5qvS1NTkmVRp62L6dtppJ++vKk176tSpXoNfsGABkO9L9PjjjwOw99575/0ubHJk5pDjqaee8n6nzc3NgNulKSiqqqoKyO3I6uvr/Y5Pjsfd3d2sXesqoC5evBjI9SPkdjRy+q6pqfF+SBGbF0L/M40Jay2dnZ1Azk9I77u6urxfkebQlClT/PxK+2mtWLHCM/977rnn8DUkImIIcffdd3uZKHk4ceJEwMlYzQFlPqmqqvJzQLJYsvWxxx5jn332Gbmb30SEa971118POL9BrR8KttX7WbNmecZV6+YOO+zgZYb6RWvTFltswd133z3czRhSHHTQQUAuruOuu+4qYEbFmoaQr+n69ev9eBLzKp/MSZMm+Zgasatf//rX+e1vfzsMLRkaXHDBBUBO7+rp6fHjRkynYlsmT57s+0iBhrNmzfJWXI0PjSdrrbf0av155JFHeNOb3jTo+82WFhcRERERERERERHBMDGpxZjMAw88EID58+cXpDQQ61dXV+ePvfrqq4BjT8U+Kk2ENPTly5dz5JFH5v3WihUr/Ou0lj/aqKmp8dFyatOSJUu874bYVe1wxo4d6z9Tv0yZMqWgPdodd3R0eEZN5yxevNh/dxPyl2UO/bUlPCYmRX1QVVW1WbQf8tv+oQ99CIDXXnvNfyYmQH2wdOlSvyvWdydPnuxZAfke7bvvvgAcf/zx/O53vwPgiiuuGKZWDA7p578p1pPNaV6MFObPnw/ATTfdxKc+9SkgO3J23bp1ngmSPBSTOnbs2Lz1BpxFTnNA//X9hx9+uKSYVIBVq1YBOYtcTU2Nnx9ikUPmS5HcynBQVlbmmUWtofpfVlbmrSvy892IXNyjAt27fGm/+MUvel/ls846C8hZkcSwQr5/quSmxoV8MufNm+etTBo73/ve94ahFUODzs5On2pM8q6np8f7KKfncE9Pj+8T6WTTpk3zPtsaV0JXV5e3Ruj6f/rTnzyTOhgZOyxKangj5513HpCbMFtttZWnzEWh68EvXLjQDx4Jl8bGRn88zE0GsM022/igKzl9f+xjH+MXv/gFkB2hKXOAtdYr2qLOt9lmG99WtV1YvHix7yOZX5577jmfPkRuE5ocbW1tXvBqEM2cOdMrK8qhusceewxtA0cB4RiTO4NSk33/+98HnLvExz72sZG/uRFCV1eXd3hX7uDnn3/eCwn91zzYbbfdvMDW5qe1tdULY7nTaBFva2vz+SWzhrSwC5XUe+65B8ilaNt+++19uzX/lAJul112KbhWS0uLdzuSzNGi9Na3vnWIWzJy0Ga2urqav//97wCcfvrpQC5/5obad9VVVwH4FEWf/exnuffee4FcgvPRhkz1kNuo6RlvueWWfs5oXlRVVflgD8lN4f777+cTn/jEsN/zUELBXnrelZWVfg2VsiXzfUdHR0Fqx/nz5/vj6g/NH2utv5bWk0MOOWQ4m7PJ0HMO11cpp6HZHvLTLElPqaio8J9rPOn8lpYWzjzzTCDnTqB1OYt44IEH/FjXWGhra/NyUWNGG5Gamhqvq2ijV1NTk+dCBuTpNWkXkdtvv53vfOc7g77naO6PiIiIiIiIiIjIHIadSX3wwQcBxxjqmHYoMrNJoy8vL/fHZGJ59dVX/W5HlaeULqS9vd3T1HLkfeaZZ4ajSZsEJeefNm2a38WL/Vu7dq1PQxUGQIFLzaVdrpyRJ0+ezJIlSwB8dS4xo6tXr/aMssxx22yzje8vVcTYHJjUEKqKIrOE+uyll17iZz/7GZDrv+23357jjjsOyLmgaIdYagjTxynlSU1NTYGFIjTriAnQjrmiosLvijU2Va1s4sSJfs5lDf2Z6HX/Yj9ra2s9u6Zqbkrtts0223DdddcB8OMf/xhw1VM0XsQUiCU58MADfT+VGkLTnNyOxKJ/9KMfBdw4kkzVuIBcfyuVkb73j3/8g5NOOml4b3yAEOsnKwHkmC+1qb29vSCl4aJFi/xcEfSMX3755WG73+GCWG49s9AFRv0hZrCsrMz3h6wGXV1dnn1Uv6g/jDF+zj300ENA9pnUYhATKtZZsmLs2LF5AVPg+kwyVbqIdJaWlpaSav8tt9ySN6/BzXPpCKFVG9wY0viRlRZy40GsrPQvyLGw+l7ogjYYRCY1IiIiIiIiIiIicxjWFFQ9PT3en0H+cfX19V4jl0av/9XV1Z7hCYOrFMghZ27tZubOnetZMO3sV65c6X3rwkT5owk5DT/yyCPe101l6o466ii/C5HD+1577QU4ZjnczYHb2SjZv/pUO9q6ujrP0N54440AfPjDH/aO0vvtt99wNXHUsGrVKt+nSlKskobV1dXe31fM2MqVKz3zKt9lMcvvete7fN+XGsT0GWP69Cerq6vzx+R32t3d7Xe86dKQm+JHNNxIM6ih/7kKWeicMIBOwR5iP/785z/79HWSKzNnzvRzUn0j5qBUWVTIyQvIJTgXIyR5++CDD/p+k0zt6urywTG77rorkPNfnjZtWkHqstGCWM+Ojo6C8aG1o6KiwsvUcMykmbJSft5Ky5gOKoScn6WOhX2gz6y1Xmao/Zo/VVVVflxo3SkVhIHUGsdiUvXcq6urvUVO7Crk5oL+63z5YpYKJP9CVFZWcv/99wM5RlRrwPr16/0aKgvFuHHjvM6m9j/11FOAswxrrRWTX19f7/XAkHEdKIZVSZ0/f75vmIREV1eXf9AyOWjwdHd3+88UcdjZ2elNNTJRaaEdP368/66U27A6RFaU1OOPP97/1yD561//CjiT/dve9jYgp1SoQsNuu+3m2y7Ffs2aNf4aEjhafKZOnepdALT4nH/++ZmPvkxjINHWeu5jx471/afPlDnhu9/9rq+IoWCzSZMmedO4zlPU4te//nX+/Oc/D2lbhhPFqsXV1NR4xapY/0m4hnnxdJ4Ej84pJYRjRouLFti1a9d69xcFTJ1wwgmAM1drHilwpKqqqkA5kQJfiiiW5URuWGqn2jdlyhT/mcbD+vXrvTzRBkAmPbnOZAG6p56eHj8GJD+1rowZM8abKCU3e3p6/EKsoBedI3KglKANhGCt9Tk81b5QNmj8S5mtrKz080ljR2tOmHNV/V0qCKtYyo1Jn0ln6O7u9mMnnC/pylQaF6GrS9ayCRVDa2trQVXP1tZWryOoXdLbJk6c6NdLuVF1dXXlKaCQ07+WLl3q+1LKaltbG8899xwABx988EbfczT3R0REREREREREZA7DyqQqiAdyLGFra6tnVbW7lUbf3t7ud7fS6Nva2jzzKgZVO5WWlha/45VJu6enx2vtYRWqrEA7FgUxfepTn/I7WO1yX3jhBcDlq9T5+mz69OmeMv/Xv/4F5FjCl19+2e8Qv/Wtb+X9XqnAWuv7I8zlB/m7fzHRv//973nkkUcAF/ACOVPmmDFj/LjQ7u6QQw7xrJDGjsajmNVSQZjHL2RVtasVW6o63OvXr/e7YfVBd3e3n1e6no6VEsKxIauDHPa33nprP6ZeeuklIBfI2dzc7OePAhJ7e3u9JUdm4KwGkA0EaUb95Zdf9q5FGhtiTsrKygpyC7e3t3tWNUxjpfOzAs3xpUuX+rzBkrNiiteuXevbINlQU1PDs88+C8A73/lOwKXqCa9ZSpDcDNfXd7/73QA+PZuecXV1tR8fkn8vvfSSlwV67gcccADgrE565mG6plJAKC81HvSZzNvd3d2+/8TCV1RUeH1E52u8FGNbs8ykvvbaawXyoK2tzT9n5coWK7xixQqvu0mX6Ojo8OZ79ZHmydixY707jeRHd3e3r1IWmdSIiIiIiIiIiIjNAsPKpD733HN+1yWfltdff53ddtsNyO04tKvp7Oz02rfYje7ubn9c2r12cCEzJOf98vJy72/1/ve/fxhbt/EI/f/U9oqKCs/oibURs/XQQw9x2mmnATkmeu7cuX4XLOdvMQNz5871fRSmsyoFX5mQLU3fZ7jz0zhSENg//vEPz4h8/etfB3IMSUNDg/czFObOnevHlhhUnb969erMBd31h7BfNAZaWlrYfvvtgdyuX8dWrFjhLRXa7VZUVPjrpK0YpYSwLxRMqXEU+sHffvvtANx6662Aa7/Gg9rd3d3tr6d5l5XgoMEgzXbedNNN3i9N40DzL5RRYQCVZLDGknxTwwIio40wKGSXXXYBcqnGtK6EyevFHo0dO9YfF7OsIjHz5s3zbJHkRNYhlktrwIsvvsj1118P4K2M8lENa9bL8qD1B3KMq6o1nXzyyZ51LDXf9ZDpVCo1QYGUy5Yty9MlBOkZkiPq2zBwKmRqs4pFixb5+1Tw7Kc//Wl+85vf5H0mmVhdXe3HgI5Brt2SERoLJ510krfKqIhRZWWlD24eDCKTGhERERERERERkTkMq+q/aNGiov6FYkK1Q5WmHibzD/3q0pG2Omf9+vX+GtL86+rqePHFF4etTUMF+Z82NDT4PpJfhwoZPPnkk/zkJz8Bcv5Czz77rN8pazcoZqCnp8fvgsUIhMezgv6i97u6ujx7JTZDTHNVVZVPrfW3v/0NcFHaYptVX17+ZI2NjZ7lEeu8cuVKzyzruur/q666ypeJHGomdbD14bu7uwt26Jov4by47LLLAJfiQ9GpYrtC/yG1XdfQb0BhmpWw7OpIY0P9Faao6+u8MDOG2nTqqacCObbo3nvv9eNNkarl5eU+ubV8z8J0NFlEX/3V29tbMP+vueYazxapj/rrR8jNFZU+FQO7YsUK78c22pBPMeSsAvK9DWvPh+wh5PoAchlkxMQ+8cQTvhCELBRZxvr16/3aKbavsrLSr4/qDx3r6OjwczzMsqP1Wp8Vq72eLoBQSpDMU7l2lQWWzAwRyl/564uJLjU2ed26dd7fXtaRH/zgB77IiRhPycJwLGjMLF++3F9Da7Ss1wceeKAff1qjw6xDg8GwKqkvvPBCUeGZngBp01OI3t5ev6BqsOh7FRUVBUFYVVVVfmHJMvTAGxsb/WvR6aLL5RYBOTPNscce6wWwamarjydOnOgHTZZND6EQTY+Prq4uPw40iWQ2ePbZZ/nUpz6Vd42nn36af/7zn0AuKEDO2cYYr6Tq/7777usVHClvEjRHHnlk5sz84XMsppxec801gDPhApx44ol+k6b2SUkpKyvzY02my/b29rxqMuH3FixY4NOPZAk9PT1+3BQb50ojJrP/woULvWlXaVEkL8rLy/3CrXHR09PjgwRC02eW0ZdyGSqoyiP86quv+mprGlOac2EFIiHM13zkkUcCuU32k08+mRklVQpm+FpjOZSLWn/UN2HwnPJmKl9yWVmZT09VCliwYEHBeimFAnJK2E477QS4Ma9xH278NO61OdHznjx5ckE6phUrVvh5lWWEskKyQf0gORduYMJgKX0eEmRQOkGm2oiFRFaYP1ducVoXlF6st7fXyxa1uaqqyh9XcLyU2j333NPLg89//vP+fG30BoNsUWwRERERERERERERDDOT+swzz+QFLwgyr4UJxcHt4LTbKcbApnd8NTU1niEJdwViJFV9KR08M1ooxnZMmjSpoLqJTAkdHR3e7Ki+eu655wrSwajPKisri+5oN9bEPNwIg7rCuvLgdqpyUhcjrvQpP/3pT73TvlICLV68mMceewxwuzjIpcqYPHmyHwsKoKiurvbMwuzZs4Fc/02bNq2g7vtQIXwG6ST84VgPA1j0P131SLjyyiv52te+BjiGHVz6mHSBC7HI3d3dBWZyyE/iDbln8dxzz40akxreXzrwLwxokFxRSrJbb73Vm33VnqlTp/qd/7XXXgvk3IMWL17s54xYhM7OTs8gqe8VeKLURlmHZERVVZV/rbGy5557+v6V1Sach+lE7hUVFd7aoMoyOv+aa67hxBNPHO7mDAhixMOqNmKQ9IxDy5zGURisKdZUMmT58uUlZdJdunSpf35q55ZbbumtTTomNi1kzcOUVeoPjZ3rrrsOcIyjip9oDCxYsKAkmNQQSt8odlDWyYMOOsifo2Mhc5x2ifr973/PnDlzgGwHJiuF5ZQpU7y8D60sYsplnVV/lJWV+XERWjv1meaL1prXXnutIM1UdXW1Z5y1Hm/MeIlMakREREREREREROYwrEzqkiVL/K429OGQJq/dnHZrNTU1fvcnzRwo2NnrWOhzqO+Fux75cWaFSS2GMWPG+L5R+8TyWGu9L0fIPmtHk/aX6+joGFRt3OGGWCn5Q4mtXLZsmWe75P9z4IEHelbsu9/9LpDb3X7pS1/yjIAS9y9btsz7j+2+++5Arq+qqqq8r4w+C1kC1frWOb29vZ5x22OPPYas/SF6enr6TbHVH+sty8Dll18OwM033+xTp8ybNw/ITx+VZqlXrFhR4MsZskgad3r/xBNP8I53vGNjmzgkCHf76f5qbm7mT3/6E1CYrL2+vt4z5Nq1v/LKK55l185fAQITJ070QUTqu+rqas/2i1FQeqNizy9LkPwLZYPKBssK0dDQ4PshzZ6HTKrGT5i0XfNDvnwrVqzITIo7Pe/6+nrPiqeDAkOLm8Z7VVWV92HVWhT6cZYSk7ps2bICpmzcuHFepoohTlts0tA1NP4VjLrddtt5JlUIfYFLBTfccAOQGxea+88++6yfJ7JohpBvapjeqxSg+wwt2iGbqQI4sjxoTezp6fFrp+ZUa2urtzRqbkiOPPzww3zgAx/I++3e3l4vI+TzLd/2gWBYldSweklYqUQPODS3gJswEiphMEda0ITfC+vPQr5wlrN8ltHT0+MXgXRgWG9vrxek6sfQVC7lL6z+kg56GG289tprPrJPk0IuDHvuuadfFO68807AmauV3UC5237605/6a2lSSFiEGxBNRP3OxIkT/WKj8TdhwoSCbAp639zcPGw12jd2Idfzfvrpp72pWWNcbT7kkEMKai5PmDChICBMqKys9PNDcy4MpkrfW1gxbriRVoxCU5SUD2V2uPvuuwuUsTBXpxYXXXPmzJne3CWZcNhhhwHOJUnnh7mZ0xHSGit33323V/pGA2FFtlDRCPMuhzjhhBP8HFCGi0cffTQvYwbk5kexHLHLly/3Y05R7xpvLS0tvuKbqhKNFjR3p06dyvz584HcOAo3bFK8tMa0tbX555yeO1OnTvUyqhTQ2trKwoULgdxGYt26dQVVg8I1Jj33ysvL8+QluA0ruEpt6lONQ7lZZB2hfJOytN122wG58Tx27Fg/FiRjxo4d22/uaLnAiNjI4kY2JMLS7oWQ23ymcx6Xl5fnVZ6DfL0r7Sr2+OOP+++m8xHD4Cq4RXN/RERERERERERE5jCsTGqYykI708mTJxdQ7NrZtre3+92cqOWwyoGOSdtfs2aN3wmJRSsrK/O7RZlHR5P52BAqKyvzmOQQPT09fheinW9bW1uBmX9DFVEGm6NzU6BnW1dX503GCtLQ829qavJmBh2rra31zKuuoVx2a9as8W3ULm3lypV+56vdosbaFltsUVCRbOXKlb6qVLr60rp164aNSdXOetmyZVx00UVA/q4TYPr06b5duo8xY8aw7777AnDEEUcAuf64995784KiwLGIMuXqWurvqVOneoZWY6K5udm/Tu+iwwo+w41i9eXBsacyzWs8NDY2FuzS1f7m5mbfXj3Xjo4OXyFF7LDSmu2zzz6+neq37u5u/1u6L7Hzd91117DJk5AZTZtji7lnFIOCDT/60Y8CzpogBvr3v/894NKViVF+5plngJw1Zvz48X7+SI5uvfXW3nIh1kzs0SuvvOJZ6tFmUsUWLl261PeDAkLCwMzQ8gRuDKWr/d1///2AGzvqo1JAyPhpzCxevNi3Kx0w1VeAohgyyVs990WLFvm+ktlfsjurCK244FLUyVyt/gotJhrrodVO52k8CY2Njdxyyy1AjknNGosKufWkubnZ94MsmpBra1hBC5z8SVt4oe9UXE8//bSXWbK+NDU1edkqC87GIDKpERERERERERERmcOwMKna0ZaXlxfswMMa2OlKKOH7sKZ02rlfO4Hq6mpfCUN1hxsaGrzmLzYmKwjrYqt9XV1dfueVdngPdy461t3dXZCKKEzynvYfqa2tHZUUVKG/l+4v3b7u7m6fvkO+Ki+//LLfiem8t7zlLYBj9uQ7I/a4rKysoNay/re3t+elngHHsqQZRu0C6+vr8yp1DQcuvvhiPyc+/elPAzlGdcmSJd5hXazmlClTfPvEOosNrKys9IFkYgs6Ozv989auX0zH+vXr/Q5YTFpYQCP9fEayIIT8Qn/1q18BOStIRUWFZ38079vb2wvSCOl9S0uLH3vqk9bWVt8nYgqUpuquu+7izW9+M5BfzEAMkn5b19+UyikDRX8V4qy1/nkqYOXBBx/0xRyUju2MM84A4Fvf+hY//OEPAfjRj34EuPGuKm0qGHLppZcCbv6JVfrMZz4DuGcj5lTBj7JgTZ06tc/gm5GG+u2AAw7IC/yA/EINab/nUD5qfogVvuOOO7yvcilgxYoVfvyHgcn9WdnUflmWOjs7C9YgXWvNmjX+teTDSFpcBoMw8A/g6quv9uNXVqnQIpsOhpo0aZI/T5Y5zZHtttvOy66sBBAWQ+gLquclufeHP/yhoDKoZGcYJxSmBw3TlQF5Fk5ZIZTUf+XKlX48DWasRCY1IiIiIiIiIiIicxgWqkQsV2tra4GmPWXKFJ9WSZGDYdm5NOsXRpJJG9cuZtGiRX4Xr53z/Pnz/a4grOWcNSj6tKurqyBCWzuympoav7MJ2Yp0H6k/ysrK8pL+A96fcaShaP36+nqfGkqRs3r+jY2NTJ8+HcCXIz344IO9n2HIBgvqh3BM6HmHacsEXUM+N8cee2xeXeIQ1dXV/bJYmwL5VS5ZssSz/i+99BKQ8w0aN26cf94aExUVFd4KoR2+dvPl5eW+bzTnwjGjfhT7HPrbhnNDzK76T6zhSJWDXLVqFd/4xjeA3BxXGpju7m7f/tCXPb27DxGmi4L86HdZecSYT5o0yT8bWWV6enq8L7wYA7FTS5Ys8WNpqEsihvP67rvvBnLPXCnGXn/9dc+kqq+mTp3KO9/5TiCXdF33+9WvfpWf/OQnAOy3336AY9aVwk0svaK3W1tb/ZgSAzt27Fj/PGSZ0Hz9+9//npmyqPKNBTj33HMBCopXhHI0zCSjsSKfORVtUGnHUkFTU5O3oGi+l5eX+3mSLvDR0dFRsJ50dXUVWLPCKG4d0+9keZ0Noba/8sor/tlrfskiFfpf638obzRf9H7evHnewqVCIbJiZAlhnIKen6xpt9xyi9fFwgIekF8WVd+bPHmyX7uke+ic8ePHe2uYxlh4jcGkcxsWJVU3UltbW2Bu2WabbQqquaQnAuRT9LqGGq2OGTdunBeoOtba2uqVkLAOb9YQOhCnhUTYD6E5Ftzk0ARJ53Ds7u72fSPzxGgpqaoIdeGFF/q0NRJqqmJUV1eXV8UCnAImhUsTR0pKdXW17xspG9XV1X5BTh+rra3140KLa6jUChp/69ev94JsqCuo3HHHHYAzsag/pKwrZczKlSsLUqlVV1f7xUbtCnO+qo/CNqVzREqZ2mmnnfymQN8bP368P1//pQRVVlYW3SgMFTS2v/a1r/k+EDTnW1tbC8zJra2tvr3pIKmenp6CfMo9PT2+L9KmuN7eXt9eLVjTpk3LC7SB3Kahra2Niy++GIDvfOc7g2x5cSjY53Of+5wfk5oXWhx322039t5777xjM2fO9IvGl7/8ZSCXvm3MmDH+3p9++mn/W5IT6lspsJMnT/bPXJubl19+2buY7L///nnf7+joYPvttx+S9g8lpDj1J1vD4KDQvAn44MrQPa0UEOae1tyorq4uSBkkhFXuwk1sX5v1ioqKgjzdGidZh8zyS5cu9eZv6QgPPfQQ4IgvnafgqvXr13v5oj7VXF20aJF3IZJLTBaVVLkGVVZWelmusfD444/74xonoetCOtDQGOMVXLVZ56xYscKTKNLDqqqq/NgqRiRtCNHcHxERERERERERkTkMC5Mamsq0SxNzuH79er/TC6sfCGnGo7q6Oq/aEuSYpIqKigITOeRYn6w5MIe7eu026urqCpzThYqKin77Su0rZvoUIzdakAnwiiuu8K4NqgssBmrs2LE+KEH/w6AHMfKhSUE7Mu3kFi5c6PtNbQ4ZBO3y1Y+NjY0Fuzn1f3t7OyeddNKmN74Izj77bMBVblFQkBhOBa+0t7fnJUkHN9blJpFO9VJZWen7Rteqrq727MDEiROBHAtYXl7ux5Ha3NzcXFCZSteaO3euNxMNB5P6rW99C3DPWc9Oz1y7/NWrVxcUG6isrCwww4fWFZ0Xpm1LM5NqT7HArIaGBs8qi3HWeKusrPTPa6gRVvJRP4jBEbP35JNP+uIOQldXV14yfshPR6f+EDM0duxY3x8aWw8//DDg+kPMqK45Y8YMf77klqw3zz333LC5yGwKigVFCWpXOHZ0XrHzsxwQk0ZTU5MfO5pTocUqXbyhqqqqoD/CqnzpIF39BuTkbKlUXdJ9z5kzx88TyQ0da2lp8Uyq2NZ9992X22+/Hci53WidaGpq8nP0i1/84nA3YdBQW6qqqgpkxLPPPss111wD5FhxyZtVq1b5FGy6RkNDg3eXUiU/WWz33Xdfbyk866yzADd/JD8HE2SZPekSERERERERERHxhsewpqCqqqryO7iQ3VGggpiLMHF1mk00xuTt8KAw7QzkSsD95S9/8f6E6cCYLEGsWFVVVQEToZ17Z2en7w+1uRhrod1gV1dXv0FEowWxqvof+uxoF660FU1NTX4Xp11dyIyK+TnnnHOA/J2+GEOxX42NjT6ATEztunXr/DXCWsQ6Z7hK6eq5HXTQQRx00EFA7hnJN3XJkiXel1iWh46ODt9verYaE2HSdzGD48eP9+Uvxfj9+te/BuCSSy7x7Kq+V1lZWVCeWLvk5cuXF6RvGUoo7cv8+fO9n7KeSeisr/EdltfTfYWptyA/6EPjJmSc1Ydhuq20321HR4e/rnwbw2ck/0w9x6HCiSee6P8rkONf//oXkPP9qqio8Cym2myMKfBdF2pqary/WWhdUd/Lx1TBja+88goXXnhh3jkrV670TJNKrGqOrV692veHgquyAD1TjQXJ1PB5a00KLVA6P+yr0UjhN1hcfvnleUG5AJ/85Cf9mqx5EMaKqH1pf9Vin61du9anOxNjVgrlx8GlnoL80uKCrEeLFi3y7KD0h5UrV3LooYcCubESjh0xjA8++CAAxx9//LDc/6ZAsqKurs5b6UJdQinphhKSp11dXX7NGoxONixKqhSw0Iwi52JjjA+E2WabbYCcSWv9+vV+8ZEisXLlSm/+1WIa1h/XIvL+978fcEpq2gSYRUiA1NXV+YGvgRQGlOmhhlWj0hWW9L+8vNwLDPVf1qHNhf4PNaTMjjZCYa85obE+e/Zs/1+560JIqGgMhHlnBxLY8fa3vx1wCqwUUClhvb29edWMIN91Ror/cODkk08G3MZVipfGr3LH1tXV+TkuoTdx4kSfr1MKvNrT0NDgFaqwioo2QYpK1/mPP/64D0RSYNG0adO8jJF5W4tSWEFtOKHqYvofQuNBymdTU5NXQopBZn4pnRuCsk9IzoQRzwq807ibPn16JgOn0gjlZ7jZ2dD5kFtY0wRKFjFjxoyCXM89PT2+PelsB+FcF8rLywtcHEKXqLe+9a3Dc/PDjCeffBJw8yBUQCE31ufMmeMVUpn9X3zxRb8+SZnV95uamrxu8//+3/8DsqmkSqew1vpnKb0KcmuKxkWauOgL6U1gb2+vnydh1o+0e9FG3ftGfyMiIiIiIiIiIiJimDGseVIbGhp8EJXqXU+bNs2bXtOm6dB0Gwa6pBmkkIWUpn/44Yf774bpa7KOsrIy3/50pZ9i5pfu7u6C6hBhOhilXCqFXf8bCZsSXLKpqdRkoh2uoLDBQmP1hBNOKDgm0/dQ4ROf+MSQXm80kc7jONRQ+qpSRl8WhrKysoKcqMaYfvOplhJ6enoKAryWLFlS4OYjeRSypiHbLKSZtNmzZxecF7qlZRFaX7VOVlRUePZTLiqyzCxdurSgmp3M/5BjUnWtLbfc0o81sbLz58/PTO7gNMrKyrx+EVrJ9CyL5Z0uNhfSgYb6XuiCKctNVVVV0eMDvueN/kZERERERERERETEMGNYmNQwmbC08L322gtwta9V3UR+HnJkN8Z4ljVkTdMpqORT1NbW5v2zlCh+8uTJflecZSY1DC5LV5XSrqOnp6eAgevq6irwE5JvSVtbm++3sFiAUGynHBEREbG5QT6DknVhwY/0OlJRUVFQq34wjE8WUEy2z549O2+NhRxLGLKuoeVOa1A60KWsrKzgN7KemusjH/kIkEvAv379es96KqWU2NCWlhafCk66RWNjo/dPVUCjkv+HkGXjs5/9LH/605+GoymDhp5ZWJgg1C3CudDXdweCcAxpznV2dvp4AvnRbwwikxoREREREREREZE5DAuTql1omBbl5ZdfBuDKK6/0EbaK6BXjuX79ep8ZQNr7Ntts47XzcGcDTlN/y1vekvfbnZ2dftcY1nLOGubMmQO4iOJ0KUztTEMmWsxrV1eX94dJ11ZftWqV9z0aaCRvRERExOYArTuVlZW85z3vAeDGG28Ecv6C5eXlRRPVy29RqdDCrArF2KWsIvQfFCu8Zs0az5Qpo4gsbWPHji2I/A/Z0jRL2t7e7tdt+TRm3X9XWT7kW7rPPvtw9913AxRE+Xd3d3PDDTcAuej+7u5uPvvZzwL4Y0o/19LSwjHHHAPA+eefD+RS/mUJysARZrZQ1g8YOjY8ZGeVTnDWrFl+PIUZBQYKswkDrM8vilb/7ne/6/NUHnbYYYDL1TicuPDCC31HycWgj5QQQ23zHnRHyrygVDtK09DW1uaVUwmcnp4e79ogZV0BKJMmTfJCdhDITH9kBLE/8jEcPiKxT/IR+yMfG9UfxdyZtBbdd999gMt3++ijjwK5FIgHHHCAV1gVsCcioLu7e1OU1BHvj9CdQTj//PN9Ltuw0hw4pULKqRS27u7uom4S4AKFrrjiirzrFwvW6gOZmC/z588vqMp3+eWXA25zkg56+u///m/vMqC83qeccoo/rnzeUvo2QuHLRH9AZlwBi/54NPdHRERERERERERkDpvCpEZEREREREREREQMCyKTGhERERERERERkTlEJTUiIiIiIiIiIiJziEpqRERERERERERE5hCV1IiIiIiIiIiIiMwhKqkRERERERERERGZQ1RSIyIiIiIiIiIiMoeopEZERERERERERGQOI6KkGmPmGWOO6OPYwcaYl0biPiIiSh3GmDONMff1c/w2Y8wHR/KeIrKDOD4iIvpHeo4YY6wxJtYRzyj6VVKNMS3BX68xpj14f/pQ3IC19l5r7Y4buI+iSq4x5jRjzDXGmK2TgTbqRZZHos82ZyTPWn22xhhzqzFm5mjf10jDGHOQMeYBY8xaY8xqY8z9xpg3beh71tpjrbW/6ee6/SoxWUEwDpqNMU1JX5xljInWH+L4EJI14NFEXixJlPCDhum3Mt03b8Q5k1ovlhljrjTGjB3t+yoVlMJ62+/gtdaO1R+wADgh+Ox3w31zA1A6jwP+Otz3sTEYaJ9lRKEe9XvoAyck/bcFsAz4ySjfz4jCGFMP/AXX7gnADOBCoGMTr5vV590XTrDWjgNmAd8FzgMuL3aiMWbABbNLHXF8OBhjPgf8EPgOMBXYCvgZcOIo3tZo4404Z7Re7A28CTh/lO+nX2RwnmV7vbXWDugPmAcc0c/xSTjB2QSsBu4FyoLv/g/wNLAW+D1Qkxw7FFiU+p3zknM7gGuBXqAdaAG+kJxXhuvQSThl0CbHW4ADk+PnA/OB5cBVQEPy3a2T8z8GLAaWAOcOtC8G02dqZ9K2pcBvgWqckF2c/P0QqE7OPxO4L3U9C2yXvD4OeB5oBl4H/ic473jgyeRZPADs3k//Vgx1u4eqz4J2/id5/XbgCWAdsBD4euq7H0ie9yrggg2N2az+AfsCTX0cOxO4D/gesAZ4DTg2OH4X8JHg3PuBH+Dm5B+B9UBPMk+K/kYW/oo9O2C/RBbMAX4NXIrbpLYCRwDTkzauSPrl06nvPpqMnWXAJcnnNcDVyZhpAh4Bpo52++P42GAfNCT3+N4+jvcnW8fj1qoVSR/9Bdgy1YdzcbL1NeB0YOes980bcc6k2wz8X/I8LcHaVmTc3xccC9fVBpyusAK3lpyP0yWqk7bOCb43GaeXTEnel9y6W6T/MrfeDqUZ4FycEjYZt6v9Mu7hCycDxwCzgd1xA6UvvA/XQY3W2veRz0henJyzHzDXWrsSeGvyWWNyzoPJ9c8EDgO2AcYCP039zmHA9sBRwBf78psdQkzDMR+zcAryV4ADgD2BPXBtGugu8HLg49btmucAdwAYY/YGrgA+DkwELgNuNsZUB98N+7d705o0fDDG1AGnAA8lH7XiJkYj7v4/YYw5KTl3FxyLcjpuR9iAY5hKEf8BeowxvzHGHGuMGZ86vj/wEm6DdjFwuTHG9HGt/XEL7hTgDOAs4MFknjQOy90PE6y1D+NkzMHJR6cB3wbG4RaFW4CncM/9cOCzxpijk3N/BPzIWlsPbAtcn3z+QdxYmYmbL2fhFp4sI44PR0TUAH/q43h/srUMuBInh7fCPe+fAhhjxgA/xin244A3A09aa1+gdPrG4400ZxIz9XG4jcdg8RNc27YBDsGtNx+y1nYAN+LWTuFk4G5r7fLNYd3N6no7lEpqF+5mZ1lru6zzNQ2V1B9baxdba1fjJsae/Vzrx9bahdba/gb+2+nf1H86buc311rbAnwJODVFtV9orW211j6DE1rvK3ahIUQv8DVrbUfSttOBb1hrl1trV+BMdu8f4LW6gF2MMfXW2jXW2seTzz8KXGat/be1tsc6/7MOnMAWBtK/o4mbjDFNuB3ckbjdMdbau6y1z1hre621T+NY9kOS77wHuMVae5+1thP4KvmbpJKBtXYdcBDu/n8JrDDG3GyMmZqcMt9a+0trbQ/wG9y8m1r8aiy21v7EWtud4ee9MViM2+gB/Nlae7+1thfYDZhsrf2GtbbTWjsX13enJud2AdsZYyZZa1ustQ8Fn0/EMSk91trHkv7PLOL4ANwzW9nPYt+nbLXWrrLW/tFa22atbcYpbYcE3+0F5hhjaq21S6y1zw1jO0YCm/uc0XpxH3A3zv1jo5G4P5wCfMla22ytnQd8n9yafA35OsJpyWdQ2utuptfbQSmpxpitwgCh5OP/A14B/m6MmWuM+WLqa0uD1204ZrMvLBzAbWzIH3U6jooW5gMV5Avrhanj0wfwu5uCFdba9cH7Yvc40Ht4N64P5htj7jbGHJh8Pgs4N3Gcb0oG38zUdQfSv6OJkxKmohr4FHC3MWaaMWZ/Y8ydxpgVxpi1uB38pOQ70wnaZa1tw5khShLW2hestWdaa7fEMeXTcSZLCOZS0k7oez5l/VlvLGbgTNOQ37ZZwPTUuP8yufn+X8AOwIvGmEeMMccnn/8W+BtwnTFmsTHmYmNM5bC3YhMRxwergEn9+Pf1KVuNMXXGmMuMMfONMeuAe4BGY0y5tbYVp6icBSxJAkl2Gr5mjAg29zlzkrW20Vo7y1p7NoNndScBVRSOGzGEdwC1yTo0C0e0ickv5XU30+vtoJRUa+0Cmx8gRLLzONdauw1wAvA5Y8zhg7yvtEae994YMw3HDjzex/ngdo+zgvdbAd043xphZur44sHc7EYgfZ/F7lH30ArU6UDS5tyFrH3EWnsizkx3EzlTzELg28mk1V+dtfbafu4jk0h2pDfi/MAOwu1abwZmWmsbgJ8DMmMuAbbUd40xtbjdfsnDWvsizp9szmC+voH3JQPjotdn4BgTyG/LQuC11LgfZ609DsBa+7J1rkNTgP8FbjDGjLHO6nOhtXYXnGn3eJyJq2TwBh0fD+J8RE/q43h/svVcYEdgf+tM2XIXMwDW2r9Za4/ErTEv4thFKJ2+8XiDzpnW5H9d8Nm0YiemsBLHEqfHzesACft8PY5NPQ34S8LEw2aw7mZ1vR0yc78x5nhjzHaJ79M6XEN7hujyy3A+IsJxwO3WeneCFTgTTXjOtcA5xpjZxqWk+A7w+5R56IJkV70r8CFcQNdI4lrgfGPMZGPMJBxlfnVy7ClgV2PMnsaYGuDr+pIxpsoYc7oxpsFa20Wuv8EJ1LOSXZAxxowxxrzdGDNuxFo1REju/0RcoMMLOD+q1dba9caY/XCCQrgBOMEY82ZjTBXOvNeXH16mYYzZyRhzrjFmy+T9TJxgfKj/bw4Iy4Atkz4qCRhj6hMW5zrgauvcc9J4GFhnjDnPGFNrjCk3xsxJFmmMMWcYYyYnC01T8p0eY8xhxpjdElPfOtwiNVRya1gQxwdYa9fi5OX/M8aclMjxSuN8dC+mf9k6Dse2NRljJgBf03WNMVONMe8wzje1AxckpfFQEn0Db+w5k7h3vA6ckbTpwzif2g19rwenhH7bGDMuYUs/R27cgFPcTsG5k1wTfF7y625W19uh9EndHvgnblI/CPzMWnvXEF37IpzAaTLG/A8pU39CNX8buD855wCcE/Nvcaac13C77v9OXfdunIvCv4DvWWv/PkT3O1B8Cxc9+TTwDI4Z/haAtfY/wDdwffoyuZ2w8H5gnnHmqrNwQQ9Yax/F+cf8FOdA/gr9B6llEbcY50ayDvdcP5j4hZ0NfMMY04xbdMQekxz/b5xQXoKLzF3OJqblGSU04wJa/m2MacUpH8/iGKBNxR3Ac8BSY8zKIbjecOKW5FkvxAXCXILbTBYgWWBOwJngXsOxIr/COfSDC9p8LhlXPwJOTVxvpuEE7jqcYL6b/EUpi4jjA7DWXoJTIs7HERULcebKm+hHtuLcImpxY+Qh4PbgsmW4flyMM5EfgpM7UBp9E+eMw0eBz+NM0LvigsQGgv/GMbFzcWvuNThdAgBr7b+T49OB24LPS3ndzfR6a6zNNANdAON8kJYC2ya76cFcY2vcpKy0GYyyi9h0JOx5E7C9tfa1Ub6diIiIiIiIzRLDud6WYiWKCcAFg1VQIzZfGGNOSMx+Y3B5Ip/B5W6LiIiIiIiIGCKM1HpbckqqdSlFLh3t+4jIJE4kl7x7e5x5qrRMBREREREREdnHiKy3JWfuj4iIiIiIiIiI2PxRckxqRERERERERETE5o+opEZERERERERERGQOfVXrGAg22U/gr391WaSOO+64fs9bu9bFSP3zn/8E4N3vfnfhzSRuC6bPEtUFGOqcXpvcH/fd57JMPfvsswBUV1dTXl4OwA477ABAW1sba9a40sQHHXQQgH8/bdo0GhsbB/vzI94f1tqC59XZ2cn8+a7gR29vLwCrV7tiKevWraOrqyvv/N7eXioq3DDWtcaMGQPA7Nmzqax0hVCmTSvM5dzd7RI76PspZG58jDKGIwfeJvfJD37wAwCam11O7UsuuYQDDnCVCN/1rncB8Oqrr1JV5VJbaq5MmuQKp5x99tlMmTJlsD+fmTHSl/xbvXo1//rXvwDYckuXe7utrc3LiX322afgOhshQ9PIRH/09PR4uZnGqlWr+N3vfgfAzjvvDMCLL77I66+/DsB3v/vdwfxkX8hEf7S1tTF37lwA386eHpfWtLy8nLo6l/P+3//+NwBvf/vbufPOOwHYaSdXbKuszPFZBxxwADU1NYO9/0z0RzFce63Luf/UU08xdqwrzqb/q1at8jrIt7/9bQDGjRuS9KeZ7Y9RQtH+2BSf1I364quvvgrA97//fR577DEAXnvNZSrQglFeXs4ee+wB5BSUF154gZUrXUo63ev2228POCFz0UUXAdDQ0OC/pwm1AWRugHzsYx8D8IvKzjvv7PttzhxXTGbcuHFeqfrAB1yRj87OTgBqamp485vfPNifH7H+KLag3n67S1W4YMECFixYAOCV1ZYWV3m3t7fXLz5SPru6uvx19Jme/7hx49h7772B3JjZZptt2HrrrYveT+qeRnV8tLa6oim33nqrX2Duv/9+APbaay/AjY958+YBeOX9TW96E4sXu8I66tPJkycDsPfeezN1qqt4+Pa3vx1goHMFMqakPvroowAcfPDBAJx2msszXV1dzaWXurjKe++9158juXLkkUcC8Ktf/QqAT3ziE3znO4Mq9Q2jNEYkGwfy7M4++2yefvppACZMcOXbJ06cyPr1rjqzFucN/V4pyNT++kUK2BlnnOHlxKGHHgrAkiVL/Nz6/Oc/n/c/72ZKjAj55je/CcDy5ctZtcpVrNTmZMmSJYCTIU8++SSA//+73/2On/zkJ3nnS2n95Cc/yd//7tKJX3DBBUBuDg4AmVhzFy1a5OeElPVvfcul0O3q6mK33XYD4KqrrgJcm7Xmtre7iqsaO9tttx277LILkCNHNgKZ6I8MYXSU1AcffBCAD3/4wwDMmzfP78Tq6+uBHJM1YcIEJk50lbUkRBsbG70SpsVawrahoYHDDjsMcAMJ3EAZoBDP3AD5+Mc/DuD7Z8yYMV54ake73377eWGy5557AnjFtKysjB133HGwPz/s/VFMyGuRlDK+cOFCL0Bqa2uBnEBtbGz0ysYjjzwC5IQM5BjXLbbYwn9f1xXrfNxxx/nXs2fP7vO+GKXxobZ+73vfA2D8+PHMmuWq9DU1NQE5Brizs5MnnngCcCwz5C8YUlylmIbXl7A955xzirLMRZApJfX5558H4PDDXeVlyZLTTz/dj4nly5cDjmVVv1x55ZV537/88st573vfO9jbyIwMefHFFwG47TaXX1xKWVdXl7dYSY729vZ65eOYY44B8H1w+OGH+w3/IJCZ/vj5z38OwPXXu/zjUkx7e3t5+OGHgZxSYa31GzkpHC+88AIA73znO/nyl78M4Nn4jcCo9IfG/0c+8hHArZeyNOh533HHHQBstdVWXpZKkb344ou54YYbgNy6o3F1xBFH8Kc//clfF+Dqqwecx39U+uOZZ1yxLVliOzo6/PjXevncc88BjiASsTF+/HjAbepEmEjOiGVdvHix1ze0Xp111ln+9QYwYv0R6kQhi56GGOM3velNgGPhRSKqz2bOnMmPf/xjINdHQ4Si/RF9UiMiIiIiIiIiIjKHTfFJLUCakWppafE+MGJ8VqxY4V9r9/++970PcDsWfVdm/COPPNLvdsSuTp8+3d18RYXfKX/oQ67y2/XXX78xJsxMQL6o2s3Lp+7JJ5/0LFjYJu3i9FlbWxtQ3O8yS0iPj4ULF3pXDrHpe+21l9+1nnzyyQD+nJqaGj796U8DeHaxvLzcs+8dHa4im1iTyspKvwt86qmnANfHYorEpOp+NtEfb0hw6623Ajn3hDFjxvj2637FmnZ1dXHqqacCuR3+3LlzWbp0KYD3Ndtqq60AWLZsmR9b6uObb77Zu5mUEsQMpC1Bl1xyCbvvvjuQ88Hs6uryJn1ZKcQiiGEqJYgFl0n6+eef92Na7HloYdhvv/0AePnllwHnEiGmRDJV15o8ebKXr3Ix+sxnPuPnWJbxyiuvAHDeeef5OSL2M2RB1VfyT25pafHzTZgxYwbgXGxOPPFEINdHb3vb24arCUMCjWnNjXXr1vl1WG2XVWbSpEmeQdX68+yzz3p5rPEh1nnZsmXekqM5mGU0Nzd7S4LkZ3l5uWc65Va17777As5HW1YIra+rVq3yfuvqI60TIWMqK9UvfvELPvOZzwxfozYCxSzlxfQjWZZ23XVXAI4++mjAWSDVR7JU/va3v/UWXlm3hY1wDRowSkubi4iIiIiIiIiIeENgWJnU1157jQceeACAe+65B3C+T+94xzuAXPCGWI3169d7puOMM84AHNuW3r1Is7/88su9/6F2AitXrvTs2SAc3UcF6iPtVBTd39XV5dmwsO36TLtcBc+Ul5d7BiBL0HNI77CWLVvmd2La5dbX13tG9JJLLgGcDwy4XauYVLXZWuuvK1b9U5/6FADbbrutv5aY15aWFs80FrvP0R4rYlLlaz1lyhQ/3sV+iBWqrKz0DLHm0OTJkz1zKp8xfa+xsdGPGbXz6aef3lCWg0wjzeZMmTKF//znP0AuuKqystL7Tqmf1H6x1KUEWZ4kK+fMmePnmKKONcZvu+02P7e22WYbwLFiYpc0t97znvcAjqUVGytL10c/+lFuvPHG4W3UEEB+lqtWrfIWKLGKkgPTp0/3rGDILm633XZAbq5oLjQ2NvpriD3KOpMqhljz31rrGb/q6mogx8Y3Njb69VXt7Ozs9OfJp19jraOjw7P0kiXr1q3zlpysYd68eZ491v+enh5/75IHGh/Nzc2eTZRsqaio8My8ZHHozyn5KbZ15cqV/nrqx9GC5FxoLSymF8m3X9YnBd0Ww2WXXca2224LwPnnnw/kAs+Gw4odmdSIiIiIiIiIiIjMYUipkzQL1dDQwFve8hYg5ze5xx57eE1+2bJlQC5qbN68eX6XKx+o+vp6f135zujYCSecwD/+8Q8gF+G+evVqz6SWChRZmPb7Wr9+vWdGtINbu3Ztwc5X/ZlFFhVy/nFppm7x4sV+DChnoTHGv37rW98K5CINv/Wtb/H1r38dcH5nANdcc41nBX76058COf+p1tZWf0yYNm2a9+lV5gmxKJMnTx5V9n3FihU+PZbYkLKyMp8jVyyP7nHMmDEF0cjhGAjZIHAMiRgBYfz48d6XSqxaKUAsh8aW2L+QNRLjXIz1SMugUsHcuXN9u/S8qqurvXxVf4g9/fa3v+19NXWss7OTAw88MO+6IfMjS43k6Pz5832EtNLzZBFij6urq30mA8kCWana2tq8z7LG/dSpUz0TqDkWphPS67QsySquuOIKIMd0dnV1+TR+Su+n+fPqq6/6dUT/Fy9e7M9TmsijjjrKH9OapD695pprOOuss4a3UYNEa2trgf+pMaYgU0PINKqv9Fl1dbWXL/pMjGFPT4+fV/ps/fr1nqWX9WK0EUb0p+/3Jz/5iZ87RxxxRN73Qh/T0OKmzEI//OEPgRyTOhwYViX1+eef90FSixYtApySpYVV5iQ56ldVVXnKXOalpUuXctJJJwHwhz/8AXAuAODSEMmMJyr/sssu4/vf/37R+8kqtIhqUZAS1dXV5QWCgh5Wr17t0zDJ5C2lVYI5SwhdOIQwZ5+ed6hwS7nS5FA6lNWrV3slVXj66ae9eV/t/+pXvwo49wAtPkpHtGzZMl88Qma73/zmN4AL1NJEVPDVSEJzBHJKw8KFC/29aKzrf2dnZ15aEEECWMI2FKhyr9A1Zs6c6RW1UlJSpUhprEjp7OnpKVhQQpcQCWidr81AqWDhwoUFbk9lZWW+P9TOdNq+ENOmTfPjK614VVRU+O+G8qQUlNQVK1YAbu6mNzGSratXr/abYCn2EydO9P2mftS8amtr89fQupN1aD1RKqU5c+Zw8803A3DLLbcAuVRUV155pXe7+8tf/gI4WSm5ecghhwA5V4rjjz/ekylKhZjloLp169YVuJtVVFT4jYfGeKiYag3QOhwGWmm+hONLa5fkbn19vQ9MzYqSGsrCNFkkvQrg/e9/f96xrq4u365Qn/rEJz4B5PQ0FVU555xz+iV61G9pN4T+EM39ERERERERERERmcOwREpoJ3Leeed5U6pMKxdddJFn/mSaVIqplStXepZNaXFCs4tMVErZ9Jvf/Mabyo8//nggl2qolKCdmJyWxSyvXLnSs1xKkfGzn/3M94mCAxQ8lkXU1NT43ec111wD5JJM/+pXv/JBPmG5UzGHShistt94441+t68Aqve///2+/KXYnq985SuA2zmLVdCO+fHHH+eEE04AcgFWIYM4Ggyq8OKLL/rdvsxF9fX1ng3T8xaDNmbMGG9yk4N+W1ubP66xE6ZLSZu3Z8+e7Xf96udSQDoNW5isWs88TMO0ueDll1/2YzRMD5RmJMLgOrl4iDkMC56kSwpba/1cCVlWmcGzDCUiLy8vL2BsNE7Gjx/vWWQlqt922229/EkHAIWBeZpXWUfa2gS59Fwy40vm1dXVeSukGNHp06d7q47Sl4mJffvb3+7X9FJAe3t7gYyw1hYworL2GWP8PBEjGAbUSqaEqadkKtdaVlVVlTkLTWhhkWzQWLj33nv9mqwqlkIY+BW6TcktQP2mQMxzzjnH91XoHrApbnSRSY2IiIiIiIiIiMgchoVJ1e78V7/6VUEd8XBnky7h2djY6P1+xCTtsMMOfvfy0ksvAblk5vPnz/cJpz/60Y8CbldQSil1urq6/C5e/iva0c6fP9+zzfIN+sUvflHArmqXkvb9zArkU6x2KtjtwAMP9CUHFTA0ZswYv0sVI6CgoL/85S988YtfBHJO+1OmTOH0008HCoNgjDEFqXkWLlzomcP/+7//A+CXv/wl4FIW/fd///dQNHlQUOokyLHCBxxwgL93sVna0fb29vrXmmeVlZWeadeuVTvnadOmefZNPs677LJLUb/FrEM+ZGLFNPZ7enqK7tbTSa03xicqS1i6dKlnkcXWdHd3ezmRTllXXl7un78sNRUVFb7fJCt1fmdnp5+nYg7Hjh3r+zvLUDqx2tpab7nTXAh9TmW5UmxERUWFZ8/Svs5NTU1+bOlYKUIyV/2hPjDG+PVV42Tt2rVeToi1l4/uI4884pnUUkjx2N7e7tussd7W1ubTc4kpDItg6DnLohDqEZLFGmvV1dV+Lul3WlpafP+NNtIl4sOAsdDqrDVwY6HUjmH6N+kvYUGIYr8/UAyLFhcGJUjZFCV+8803+xyocmaXMKypqfFRY1o4m5ub/aCRIBHV/Itf/IIvfOELQM6J++abb/ZVdUohyr+pqakgek5CY+XKld6pXQ9+7dq13h0gXFggZwLNEtrb270Cpfv77Gc/C7h8sNqUaAFdvny5V0D1PeEb3/iGFyrnnHOO/1zuH3ruMmWVlZX5vg3z/KVNl7/4xS8AN6FHU0lds2aNb5+UgtDEKIVU/8M6zJpztbW1fgFScJSuVVVV5QWHBNSpp57qhWwpQYFwaZO+tdZ/VqwiTlpZLTWXgNWrV3tBr4W2ubm5QImUchEGjemc0FSpa2lRbW1t9a/DYKysLLr9QetDQ0ODN+dqbEu+tLa2elkgOTNmzBjfD5Kl6qO1a9f6MSNFLesopjzqMwWq6lhTU1NB/fViMkf9I+UfcvOrWA34rKCzs9OPf42Jnp4e/5zVBr3v6enx46MYyaVraC7V1NT4zaLGTE1NTUEWldFCsbylf/vb34CcW1xNTY0PspPeJdeZGTNmFJjv165d69dYzS+5yRx//PF8+9vfBnKBycXIs43Z4ERzf0REREREREREROYwrPbwLbfc0pvjw/QWf/rTn4Ccpv2rX/0KcLs6pTKQGXjChAk+X6ZSBck03NbW5llFMSLbbLOND74qBSZ11apVnj3T7kLve3p6mDZtWt75Cxcu9Me1A9ZuUDudLMFa683w2nHKZPL000/7XbiY9tmzZ/vXYpH32WcfAK699lpuuukmAJ+Xr7a21lcbS1dTgkLzQjFz8CmnnALkTGKjhbCamNjQ8vJyP471WRg4pfZpR/vEE08UpNjRbrq6urqg7YsWLSq5XKGQa29/TGgY+FCshjXQZ/WxrGLVqlV+/oi56ejo8M9VcyBkKvRa46GystIzR5p/Yn56enp834o5tNZ6ZiXLCOe/ZGE6L+6YMWO8HJIcra6u9qyZZKn6YP369b6fs2zW7g9dXV0+OEr9IOaru7u7wF2spaXFW73ENkumyFUKss2gCmHeX7GgZWVlee4wkHu2YRCixkCYW1R9Fbp+6Dx91traOupMqlzHLr30UgCuu+46oLi8q62t9alCBY0FyQzIzZftt9/ezxPpIJojr7/+uq9eddBBBwEugF6WnaOPPhrID9Tc0LyKTGpERERERERERETmMCxM6mmnneZfK/G8KldMnDjRV4eS0+2FF14IuApA0uBVyaCtrc3XiU0nfv/Qhz7E9773PSCnmT/xxBPcdtttANx9993D0LqhxSuvvFLAhqnIQbHk6ocddpj3h9FuRru2LCZVrqur80Ed2nXJ7+Wf//yn39HL1yncnYslF1N6yCGH+AC8Rx55BHBjRumolIZMVc7q6+sL/GGqq6v9byqp/+c//3kA/vznPw9Bizceen7jxo3zu1WxpZ2dnX5XrF2wKuZ0dXX5MaOx8NJLL3nmWc798t+sr6/3u/3Qb1xMq+4jq3W4Q0gGFGNS035YoW+q2i3mLPSxKwW0tLT4eS7msL293c+tMGE5uPkkNkTzsK2tzcuckCkBN1/FrIcprsIUcVlF6Ier/tDY1/9p06b5MSOfXihkXHWtfffdl6effhrIT3c2HDXKhwtr1qzxcjVdwbC5ubkgHVNXV5fvD80ztbdU/Nd1n11dXX6Mh36nGgP6r+fd29tbUCExTICva2j+dHd357GC+l4xf/iRwqWXXuoDjCXTtQ6OHz/eV6NTQDLk0rGFqbjArZdiSWVZGTt2bAGj/PjjjwPO0nPYYYcBOZ3vgx/8oLcGKh7pggsuyPud/lA6My0iIiIiIiIiIuINgyFlUsXcqNb6Jz7xCe8Lobrq++23n4/4l7+L/J16e3t9qU/thJX4H2CvvfYCclHiv/3tbz27Kj+jU045xZd8KwWsWbPG79LULjFnxUqq7bvvvp7h0PlKypvVFFS6LyU71q5rxYoVnsnSbm3JkiV+F69yqI899hgA559/vh8z8lOGXK1q+dUoK0BFRYUfR2JlFy9e7CN90z6eSpU10tDYfeSRR7zVQDvV3t5en8Rfu3e97+zsLPANampq8p+pnfqetdb7cz/77LP+t/Vd3UcpMKl9JVYvlni7pqamgAkUM1RqPqnW2gK2vaGhwc8ZMWahP53aGqZoSpeODf3wdL7mR11dXUkkspcc7e7u9tHK//73v4F8X12xZhrnYdEC9Yv6ePr06QX+eqEMKQV0d3f7uS0WPvTXFoolr9daPRAf8CxBjGfI/Go8jx071q836fPkuwv52XY0tkKGFvLnl/qmo6NjVJhUzdGzzz7b36/YT43/MPNFuK6ks3wIFRUVfuyoTU1NTb6fZNXVXNp55539NXbYYQd/vsaWIv/DgjvpNFlpDKmSqoeqfGG1tbV87WtfA+Cd73wnAIcffriniKVgXn311YBLISSTlBTXiooK//D1PQmeGTNmcP/99wM5E/Ill1ziqWulWpCzbhaxdOlS7/agB63JJGf1EGFd4HSNe5lysga5fGiwSwAuW7bMK6maMNXV1X5iffCDHwRyz+/CCy/kHe94B5BzKXn44Yd9/WBVRZG5f9KkSd5Up9x+nZ2dfhKp/+ReoeplIw0pD9OmTfNKk8wjEydO9P2RDgIzxnilRObM9vZ2L4DTNagnTZqUFxCi89UPug+NxywjrCsP+SZ9ja9iC0U6YGK0AxwGijBwQ/JBz/eggw7yeQolU3Vs/fr1fkENa5hrHEh2qD+WLFniAx8eeOABf61Qkcsa1C61s7e3129KNWfCPLpSUiV76uvr/ZxRPyhN3Vve8pa8aj3gyJUsK6lpE2pbW5tXTvUc9b+7u7sgaKysrMz3l2SPTLelkis2NOOn04uFOoUQ9lk6FWSYj7pYhba0e0BY8WwkITfHyspKn4Nerjsaz93d3f516J4geaC5of81NTV+/VAfdHV1+TarT7Wpq62tZcWKFUBu7jU0NORteCG3Vp900kkxcCoiIiIiIiIiIqL0MKRMqkyIcsgNU7+IXe3q6vKBMDL/is1YvHhxQe30V155xbNg0rhlBr/nnnv8eUpdNXPmzKIBR1nFmjVrfOWTdM1gsSIhJk6c6E3e2s2IdctqShDVBb7qqquA3P3Onz/fP3v9P/DAA/33xILKnWHs2LF+d3b55ZcD+aYpmbJPPPFEwAURqfa0WJaKigpvqhCbqGs+/vjjvrjASI4hmUymT5/Ov/71LyDHqk+fPr2gFr3MOr29vQW70PLycr+rTVeo6uzsLCgIsGLFCm+9yDJblkZ/DGg6kKFY4JTanw4cyirEdBhj/FiWTN1tt918ir+02S5MQSW3js7OTi9ndV7ILMut6tFHHwUKCyBkDZo/ITsma4D6Q//LysoK+i8MdBEbFrJj6faX0jwBN8Y1zhUsJqtWWVmZHwthMQRZZjQWxIRl1VqXRnothRzrN3v2bD9mdF5YxU/nifULmUP9F9va29ublxIT3FgTEz2S1S8lx7u6uth+++0BfDpOoa2tjYMPPhjIravjxo1j0aJFefepMd7c3OytDJo3lZWVvl1aa3R+ZWWl1+tClwrJHumGSkMamdSIiIiIiIiIiIiSxJCq9ypXqv+AZ6aEj3/84/61Spm+8sorgNv1i2V7+OGHAafJa1egz1TX/Omnn/Y7wg9/+MNA6fjMCHV1dd5/RH4/YT3tdPmwCRMmeGf2tJO/mIGsYd999wXgzjvvBHI7rNra2gImq7Ozs6Auu95vscUW/ryQTdTu/ne/+x2QGwsTJkzwLLVYxc7OTt+nur6+39bW5tOWKVXGSEB9UF9f7xlCPdtddtmlYA6FvmPpFDFhMup0Wp1169Z5Xzqd097eXuALXQpIMwQhaypWpD8GcCDnZAnyF66oqPBjQ36GO+ywQ1HmSNDY0PeKpZMKS6cq6ChkOLKcckmyoJh8SPva9fb2evZYLOHq1av9+elE7s8884z/TCxTKQSRhVi8eHGevy7k+6QLoW+vzhMrJgaxoaGhYE0aSEL2kUY4xvXs5Vu5xRZbeF1CDHsYGKXX+l5nZ6efX2krVU9Pj2cmJT+rq6t9/2ntGomSuqEVUsHDaZ1ghx128OuvYncAn5ZKa4bGRbieqE8bGxsLCudoDZk2bZpvs3SzRYsWefkhC6Wsqj//+c/zyjQXw7By0D09PXnOtuBMUwqiuv7664FcxamwDq6CXyorK72ATkdfK4sA5CunG1MXdrQROu2LflelBihsy7bbbusHSLoGfRYROuYrn+l5550HuOhztT00L0hwKJ+uzBjXXXed39jIPLFy5Uo+8IEPADmBo43OtGnT8gJIwAldmR5C5RDceFUk70gqqaFwk0Kqfqmrq/MCTwIydOvQuNcGp7m52QsE9UdoqtJrObfPnDnTu4+ka3hnGWlFoZhyFn6mPtT/UDZIIVH/ZhEKJC0rK/NjWUpqfX19QbU5PedQ4ZAcramp8WMkvQCFn8ktpqmpyV8/i30VBriAmxOax+kAls7OTq+sPPjgg4CTL5JRWmC1uK9YsaLAbaRUcoUKS5cuLciPGmY2CCsxQX7gT1pW9vb2etlRzB0ta+jq6vLjXrKytrbWP0OZ5UN3EBEF4XNPB63q/dq1a/36pADcMKhT+sxIKKkh1D6REiICq6urefXVVwF4+eWXATj44IO9y4KC0nW/kydP9muLnvfSpUv9PFH/af489dRTBa4hTz31lL8vrc26/s9//nPOOeecftuS3e1xRERERERERETEGxZDyqSmWb/QRBQGt4gVkLlFprtwlxsGUOm1drdikt797ncX/GZYm70UmNSKigrPZIkhEZt2wAEHFOQQq6ys9Lk0tXMTq7H//vtnzt0hDDzQfYpJvfTSS/39aqf+n//8x+/yL7nkEiC3G7zzzjv55z//CeTng/zyl78M4HPsfvWrXwXcDlg7vDBwT7tb/Vd/Qn4VjpFCmNMunW6qp6fH95HGR1j9Re0KU4AI6lN9v7u7u4BxC6tWpdmCLKMvK0Ioc0ImJM20hrJBTIsYkSwiTJ8jNkyuLOHxdI3xsrIy/8yVc7qjo8O3X2MkHWwGOfPo8uXL/fXFMoX5q0cbmhdhrtg0e6Z51dXV5c8Pg6PUfrE/YoOmTJni01kpnU8pVN8KoSAoyMk8uUaEFZOEME2fxkBobdL1ssykhq5OYUAYuLUg/Znkp9ZiyMmStrY23w/pPKLhXJIcX7dunV+f0ucPJ8SQQs5SFrrAgLO4HXXUUQC+MmFLS4tnS9UfCjxsa2vz64Oee2h5UPuki1RVVXn2XXNw3rx53m1E81H9feONN0YmNSIiIiIiIiIiovQwpExqekcWvtcOvaqqymvhSkUlBqOsrMzvQKSFP/jgg96PQY7BYmK32WYbv6sNfUpKgUEVampqfBJe7S7CeuLptFLLly/3fbPrrrsCuVRNYbWMLOM73/kO4JjUNHOxevVqz5LJuVq73mXLlvGFL3wByLV16dKleQ7jkHManz17dkGAwOLFiwsCs+TbN2bMGH9sJKH7qKioYLfddgNyjFWYukRzSKxWGAQidq2iosIf1/khe6r+Fnu8ZMkSv8sdzXrTG4uQ8egLG/JTFUohEEbj1xhTUNElRDpJeW9vr3+uukbom6yxIdYo7J9p06YBTk5LvmpuZolJTVfECdcYyRVZY6qrq/0cF5PU1NRUwBxqDk2aNMmzSooZyHIQWTE8+eSTXl7quYcBq2F6PsgvCJIu+lBTU+Pl65w5c4BsWizVptBfOwww1ljRc9bzb25u9uM+ZNp1XrpQyJo1a/zcEFvf2tpawDSOBFTQCHLWaY1ntWnBggW++I3kXldXl3++0rX+8Y9/AG6ea30UW7rFFlv48SFdRH01btw4z7hKPu2+++5+vKV9dJW2tD+U1myLiIiIiIiIiIh4Q2D4M8wmCNM5SMMWqyO/id7eXq9pK2ps6dKlPlVCOjJxn3328YyKdgphCppSQFh3W7t+7WagcJdaU1PjI1d33313IFeXPos7Wij0Vdb/devW+fRbant7e7t/zjfddBOAT3C/ePFiz+CoZOPxxx/vk44rFYgiFSsrK/1va1y1trbmlWuD/Oj3kUi63BeKJdsPy/ql05r09PT4OaT7rqqq8n4/aVatra3NMwIqQ/vqq6/68SMLRSkgZBahOEOaTsEVIpwr6RKrWYTaO27cOO9vFvrQ6rgYpDDiXf0gdr6ysjJvfIXnh2NG8uXqq68uSOWWJWg+6DlPnTrVt1myMvRX1ThXW0JmNB0D0Nzc7K8rRin08SwFrFy50vuPpp9zRUWFZ/vENLa0tHhfdx1TH9TU1Hjf3CxDDLC11st+pT8K2XStFZKfY8eO9euP5EJXV5dnJDUupIM0NTX5Oac+lmUTKIgBGE7IrzT8Xfmpak2oqanxc19saFVVlW+f+kVFH6qqqgrKZTc3N/txkc6OUFtb69ckjbEwTkLrvObSQMbSiKWgEj7ykY9w9tlnA7kO1IJx8skn+1QJqiq1/fbbe2pYqQzUMTfccAPHHHMMkFNSSw2NjY1+UmjCqH57MVhr/eSR2UWTL6tKqgarJvhLL70EuPvVwA9rz2uQf+1rXwPgHe94BwB33XWXb+MXv/hFAN773vf6PKwXXXQRAOeffz7gzFG6loTWqlWr/MZGE1HCuaura1TGkSZuaHbWM25qavL9lhZ4VVVVflFNC6MQ4eZAz0Lzsq6urqAOcylAClcx02tacd2Qkqr5JvNlFqHxGwZqhPXo021Vv4QuHApoKDbO0lXNIJcjs6yszF83i9WW0um0qqur/SYs7Ddw5sswHyg4RV3zRkq7zmlra2OPPfYAckGV6bzFWceECRMKKqwV25RIUWtra/NVHvW81T8VFRV+k5RlqF3V1dW+zdqMhy6Bkv2SwZ2dnX5ehbl1w00OkJc6Ub8lxau6urrALWAkEAa+qg1K2SgltKKiwrdV59TU1PjnrPZpLHR3d/v2FSMxNIdCd4l0WjtjTN74CY8NJJVdNPdHRERERERERERkDsPCpBZjLqRpT5gwwWv1ouFPO+00AA499FC/aw1Tgsgso+Tu2v23tbV5c3H426WUzH+bbbbxDsly7tcOpxjKysr8riRMS5NlaEcqU8KRRx4JwM477+x3dTKf1NTUeKdvtUtm/z/84Q9+tyjW9IwzzvAMuyqRKTBryZIlnj2S6bKsrMzv3nbeeWcg58S9YsWKosEoww3twJ977jl/bwqgWrNmje8bMQJ67rW1tX5HH5prNP7T46K8vNzPQ13/ySef9A73skqUAtKpk9Jm2hDF0tKFFp7+LBdZg7W2gC0Pmb0wFR/ks0b6H7rBhDXLdX1BrjVhCq8sMqmyKMksuXTpUm8R0dwO0xiG6evAsYSSTTqmvurs7PTX0PUHErSXBaSr10HuOYdWGb3WutPa2lrANksuVVVVeUtYlhEWs5C8DF0C1R6NCz3TsWPHFhT8COVHeF1w8lbMcui6pnUqXVFxOBH+VroAhe67o6PDP++wIEGaKdbYgdw6EsqRvnSriooKL4P1m//5z39838uyKVZ2IIUxIpMaERERERERERGROQwLBVcsmb92aQceeCCf+9znADj11FOBnK8I5KcRAVemS7sW7Q60w993330LdvZZZxXTmDJlSl5JUMjtZpqbm/3OQ+js7CzYjWSpRGExXHHFFQB85StfAXIO2HV1dQXBYtZaz4yk/UNPPvnkguTEN9xwQ0GAUOjbK5ZJ42/q1KnesV27Rd3D+vXrvX/rSEL+c21tbd5fUBaFMF2Q2hCmNdEOOEzLlk4bI1RUVHjWUP6G69ev90xDus5zlpGutS4US9wfluaVfAmZ1FLwsdNz7unpKSi6sHLlygImNPQ91mvNhfr6+gIf1LA/0kERZWVlmfZXVnls+a4fddRRPPfcc0BufGhOhO0Ix0K6fLdk6ooVK3yJ7jPPPDPv97IOyYTy8vKCORGm2kuzqx0dHQV+6mGgkQJZs4wwcErjWXpDGBiWDq5saGgoCN4uKyvz8Qu6rixXO+20k1+PJW8rKio8QzuShR9CJjW0mvQFPdMw2LZYGtGBpFwLrdc6X/1SXl7ug5TVH+r3gTDNw6rRhQ3WzZWVlflobC3OWjinTZvma7LLBHrwwQd7JeI3v/kNAJ/61KeA/EoJMutaa0vCzC+MGTPGm3Nff/11IBd1PnfuXK+sCJ2dnXnmXuh/II42vv/973PzzTcDOZO7FIz29vY8h26ARYsW+U1LmPcN4JZbbvEKqNDV1eUXJEE1icNzlXv2pZde8uPo+9//PkBe7tXjjz9+kC0dPPQ8d9hhB/72t78BuUwF3d3dXimR8NRYX7VqlRc0GgOzZs0qmPgSIKFZU0Jp6tSpfoNQSjXJZXodaCaPtAkvRCkoqVI4rLV50bLglMp0gEZomtOioejj1tZWvwAXq8SlTaKCVxsbGwvMh1mCcv4qYBJykc5pZWHt2rUFOWLDSmySQ+qzVatW+fl21llnDWs7hhphMFA6F6pc6MJsKmEgXjrYVTJo7Nixedlnso7Ozs6CYMKxY8cyf/58ID9XKLi5VKyKpeRNmhR77bXXisrNtMl7JBCSOmFmgvT9pMd4d3d3n/lcw9z1IflYjBwQ0oGrXV1deRlpIKfzDaTKXzT3R0REREREREREZA7DyqSGrKa08eXLl/tKU+lUQMuWLfMavXYu//73vzn00EMBvCn2z3/+M+ACZRQs8/vf/x4ojWCpNMLUFZBjuV5++eUCJrWystLnFgsrXGQVb3vb27j33nuBwt1dVVWVD5LS84YcUyiT9Fvf+lYAbrvtNs8ivfOd7wSc6U15dD/wgQ8AuQC00CyqPps0aZI3C4px/dGPfgQ414HRgJi8np4ePxbUB+vWrfOMVpgSBdyOVqyp2jpu3Djfz+l0OhUVFX6uaT4eeOCBnlUQU1sKkMuITNh9BYuFxyDHnoXMaikEToVmsrQ1IWRr9OzDClJhtT9dK90PYQqrNBM/ZcoUL2PCgIqsIB3kU1VVVcBghSm50um6ws/EihVzCxGKpVbMIsJnpWcvM7dY1q222sqn/5N5dsKECXksbPj9JUuWlMQaq+dZXV3tLZPhM1OqS7UzzS5C4VwKIUa1tbW1YCz09PT4tbxYEPlwYfvtt/ev9fu692IuYiH6ck8IK9ZtCqTraa2Wy+fnP//5DX43MqkRERERERERERGZw5Ayqf2lfhJ72t7ezkknnQTk2E+xO7Nnz/b+mfPmzQPgvvvu47jjjgNyjr7yp5k1a9aI+nwMF9KJp4WQXRSMMQXpY/pLWTXa2Guvvfz9aWevZ/zKK694Pzn5h37mM58pSDit+r5bbLGF39WJUa2rq/PjR7tBXX/9+vV+JylG+sILL+QHP/gBkGPk08z0SEPM+YQJE7zVQLv/8Nlqt69+mTlzpmdhxYKMGTPGj6N0paqqqio/N8Vgjx8/3n+WDsjJMtI7/pAJUD8V89VO+9rV1NRksopSGvLV7+joKHhOzc3NPqAwzZKEifjFlIf9kk7639PTU8AcVVdXe5/YkUypM1AUSyumOVUsxZZkh45VVVX5dqVT9hTzmRtIIEkWIN/itWvXet9/9YtkXm9vr2cT1a7Ozs6CCkth8nsxr0qdKHYsS1A7W1pavHUqhIJ5ZYkp5rMuNjJcc0M5q/dpfaehocGPJ61vI4H9998fKM7ePv744wDsvffeflyITX7LW96SactAacy2iIiIiIiIiIiINxSGlElN7yhCn1SxhZdccolnuuTntGDBAsBFfMkPRMxXY2Oj96cQu6pozpqaGh8RX8pQe26//XYg54spX8EQixYt8rs0tV0l7LKKM844A8iloNIOdeutt+bOO+/MO/ftb3+7b5eetxjYMG2Kdv+Q6y+xg9oVNjY2+qT1s2fPBlwfaxd811135f32aPmanXDCCf61GMJvfOMbgGOuHnvsMSDXb2JX6+rq/P2GY6ZYwm5wDImYEe2ma2trufTSS4e+UcMMMXtiTSUjKioqPOtYDPLn1Bzq6uryjFCWIearubk5b+yDK3KiyHaxhBoD9fX1BSn8wnK6Yt3VH+3t7X4sCU1NTd6qc8cddwDwoQ99aAhbNzQIE5JrfIglD5lUrTuhdU/jR0yZrpXOpFBK2GeffQC3rur56nmLVTfGeDmrtvf29vo59M9//tNfA/LTFWlNzyJ0//fee2/RFI2yVOn/UOKZZ57x/StfzKOOOmrIf2djsPfee/vXyp4Tpv7MMkYsqahMso8//rgXHHpwWjBbWlr8IqKFedmyZV55k5lKAvPpp5/2QTIhSqniFMCxxx4L5JQx9VUxU9OOO+7o3R/22msvICeMsgrd7x//+EcAPvrRjwK5CmIh6urqvAN46Ag+VAirKmlDpAUsC6m8dA/f/OY3AbdYKjhQm5aw4lra/FpVVeWFskx2Mg/X1dX5NCXaGClAq9TwwQ9+EMgJWrX5bW97Gz/+8Y+BXKDllClTfNqx97znPQD8/Oc/B9x8OvDAA0fuxgeJb3/724CTlcr3KEyYMIEHHngAgMsuuwzIBeN1dHR4RV7KbWVlpTdna8Omufbe977XjxvhjDPO4O677wYoCOTMEsIN5mGHHQbk1gq58lRUVHjFQWRJmEtWSpwUWVWlC1Eq64qCC1944QU/HqR0KvfrKaec4tN1qXLfUUcd5dfoW2+9FchVqDvuuONGJU3fxkLVn3baaaei+Z/7CmgKPw+fczqtUoj0eDj22GP9xnfOnDkbeecRaURzf0REREREREREROZgRjJFQkRERERERERERMRAEJnUiIiIiIiIiIiIzCEqqREREREREREREZlDVFIjIiIiIiIiIiIyh6ikRkREREREREREZA5RSY2IiIiIiIiIiMgcopIaERERERERERGROUQlNSIiIiIiIiIiInOISmpERERERERERETmUHJKqjFmnjHmiNG+j4iIUkCcLxERb2wYY840xtwXvLfGmO1G855GGv3JQWPMwcaYl0b6niIGhk1SUo0xBxljHjDGrDXGrDbG3G+MedNQ3dzmhGSStBtjmo0xTUm/nWWMKbmNwlDCGHOaMeZRY0yLMWaJMeY2Y8xBw/RbecJ6pBHnSyGS566/3mSO6P3po31/WUGUH8XxRpIfyT3MC+bIMmPMlcaYsaN5T8OJkZAP1tp7rbU7buA+iiq5yfi7xhizdaL8VwzFPY0UUuNpjTHmVmPMzNG+rxCDFnDGmHrgL8BPgAnADOBCoGNobm34MIoD6QRr7ThgFvBd4Dzg8mInGmPKR/LGRgPGmM8BPwS+A0wFtgJ+Bpw4irc1LIjzpTistWP1ByzAzRF99ruRuIeBIgP3EOVHgDeS/EjhhGS+7A28CTh/lO+nX2zKvBmofBguDODejwP+Otz3MczQeNoCWIZbo7IDa+2g/oB9gaY+jp0J3Ad8D1gDvAYcGxxvwAnXJcDrwLeA8uTYtsAdwCpgJfA7oDH47jzgiOT1Tsm1T03eHw88CTQBDwC7p753HvA0TjGoGGzbB9lf/r6Dz/YDeoE5wK+BS3EDvhU4ApgO/BFYkbTz06nvPgqsww2sS5LPa4Crk/5rAh4Bpo5kWwfYHw1AC/DePo5X4xagxcnfD4Hq5Nh4nMK3IhlffwG2TI2/uUBz0m+nAzsD64Ge5HeLjt1hbG+cLxsxR4BDgUXJPSwFfruBMXEmcF/qehbYLnl9HPB8MiZeB/4nOC9T/bChvgk+i/LjDSI/+hoHwP8l92/DsQncBXyk2NxIzYsG4KqkL+bjFN6ypP+agDnB9yYD7cCU0Zg3xeZA6vikpC+agNXAvUBZ8N3/Se5nLfB7oCY5diiwqJ97vxY3z9qTZ/+F5Lwy3NyZhFOgbXK8BTgwOX5+0q/Lk35uSL67dXL+x5LxuQQ4NwPj6TjgP8nrtwNP4GTEQuDrqe9+IGnbKuCCDT2fQd/jJjSuPrm53wDHAuODY2cCXcBHgXLgE8mDMMnxm4DLgDHAFOBh4OPJse2AI5NJMhm4B/hhulNxu8gFwPHJ53snA2H/5Dc/mJxbHXzvSWAmUDvagyH4fEHSP79OJs9bksFdBzwGfBWoArbBCc6jk+89CLw/eT0WOCB5/XHgluT75cA+QP1It3cA/XEM0E0fwgv4BvBQMj4m44TgN5NjE4F3J20cB/wBuCk5NiaZVDsm77cAdg3G5X3D2a44X4ZmjuAWjm7gf5O21W5gTBQ8W/IX4yXAwcnr8cDeWe2HDfVN6vMoP4of36zkRx9zZCbwHG4DN1gl9Srgz0k/bA38B/iv5NgVwLeD730SuH205k1fcyA4fhHwc6Ay+TuYnAydh5Ob03GWrBeAs5Jjh1KopObde7HfBg4AHkxeb13kGXwYeAU398YCNwK/TZ1/bTLmdsNtFIZcyduI8VSHW5+uCvplN5w82R2nkJ+UHNsFp4wfhJMv38OtYdlRUpMb3RknHBfhBMbNOLPLmcArwXl1yQOZlhzvCAcu8D7gzj5+4yTgiVSnXpj85mHB55eSCKHgs5eAQ4LvfXgkB0BfgyH1+UPAV5J+vCr4fH9gQercLwFXJq/vSfphUuqcD5Pa1WbxD8dOLO3n+KvAccH7o4F5fZy7J7AmeT0Gt5N+NynhyCgvMnG+DHyO4ARkJwnbsaExUezZkr8YL8ApYPWpczLXDxvqm9TnUX4UP77ZyY9gHLQk9zgf596wM4NQUnHKZQewS3Ds48BdyesjgLnBsfuBDySvR3ze9DUHguPfwCnc2/Xx3TOC9xcDP09eH0qhkvrhDf028E3gguT11kWewb+As4P3O+IUuYrg/J1S93T5KI6nbhw5slsf5/4Q+EHy+qvAtcGxOpy8HnIldZOc7q21L1hrz7TWbokzOU1PGgLORKfz2pKXY3H+VJXAkiQAoAnHEk0BMMZMMcZcZ4x53RizDmd6mpT66bOAB6y1dwafzQLO1TWT685M7klYuCntHSbMwJkmIP/+ZgHTU+35Mk5pAfgvYAfgRWPMI8aY45PPfwv8DbjOGLPYGHOxMaZy2Fux8VgFTOrH52c6TggL85PPMMbUGWMuM8bMT8bIPUCjMabcWtsKnIIbI0sSR/Cdhq8ZA0ecLxuNFdba9cH7PsfEAPBunClrvjHmbmPMgcnnpdAP/SHKj+LY7ORHgJOstY3W2lnW2rNxZujBYBKOBUv304zk9R1ArTFmf2PMLJwy/6fk2KjOG2PMVmFQVfLx/+GYy78bY+YaY76Y+trS4HUbTr72hYHc+4b8UYuNwQpyczD9Oxsjz4YSJ1lrG3EWq08BdxtjpiXP/U5jzApjzFrcnNDaMp3g3pM1a9Vw3NyQRYZaa1/E7ebnbODUhbjd26RkojVaa+uttbsmxy/C7TB2t9bWA2cAJnWNs4CtjDE/SF3328E1G621ddbaa8PbHFzrhgfGRXbPwPkjQv79LQReS7VnnLX2OABr7cvW2vfhlJX/BW4wxoyx1nZZay+01u4CvBnnN/SBEWvUwPEgzsfrpD6OL8YJQmGr5DOAc3G70v2TMfLW5HMDYK39m7X2SJyp7kXgl8nxzDz/OF8GhPTv9zcmWnG7eQCMMdPyLmTtI9baE3Hz5Sbg+uRQKfRDUUT58caVHym0Jv/rgs+mFTsxhZU4Zi/dT68DWGt7cfPkfcBpwF+stc3JeaM6b6y1C2x+UBXW2mZr7bnW2m2AE4DPGWMOH+xP9Pc+kS9bAI/3cT4UH4PdOLO5MDN1fDGjBGttj7X2Rpzf9UHANThr30xrbQPOlUJryxJgS33XGFOLc6MZcmxKdP9OxphzjTFbJu9n4gbzQ/19z1q7BPg78H1jTL0xpswYs60x5pDklHEk9LMxZgbw+SKXacb5JL3VGPPd5LNfAmcl2r8xxowxxrzdGDNusG0cLiTtPh64DrjaWvtMkdMeBtYZY84zxtQaY8qNMXOShQljzBnGmMmJIGlKvtNjjDnMGLNbEt27DieEeoa/VRsHa+1anMng/xljTkrYjUpjzLHGmItxvjrnG2MmG2MmJedenXx9HI49aDLGTAC+pusaY6YaY95hjBmDU+5ayLV/GbClMaZqRBoZIM6XIUF/Y+IpYFdjzJ7GmBrg6/qSMabKGHO6MabBWtuFmxcaEyXXD1F+vPHkR3+w1q7AKZZnJM/5w7iAyg19rwenhH7bGDPOOLb0c+T6CZyicgrOveKa4PPMzRtjzPHGmO2MMYbcHB+qsbsM51sqHIfzz5VyugIXXBWecy1wjjFmtnFpwr4D/N5a2x2cc0EydncFPoQL6BoVJM/xRJzP/gu4ebLaWrveGLMfbqMi3ACcYIx5czIfLqSQHBkaDNZPALeDvx43OVqT/5fhAkTOpH//sAacT8sinLP/E+QijnfFOfy34JyXz6XQX0R+axNwi5Mc4o/BRaM24TT9PwDj0t8bjb/k99txCsNaHBPwSXJR2r8GvpX6znTcQF+Ki0J9KGj71TjH9Rac8/xJyefvw/kGteIm1o8ZpYjkAfbL6bgo49aknbfiGJya5N6XJH8/JheNOR3nc9WCc/T/eDK+KnC727uTPm5Kztsl+V5Vcv3VwMoRbmecLwObI3nR/anjfY6J5PhXcOzQQhyjLN+7KuD2ZA6tS9p8UPC9TPVDP30T5Udhv7wh5EexOZL6/FhcJoIm4PtJGwYSODU+GQsrknnzVZKI+OD8V5I2V6U+H9F5s6FrAuck57TiZOUFfX0Xt4m9Onl9KH3IzOCzE3F+7U24LAE3AO9JnfONpB+bcEFVZUl/Lkw+v5okYJbC6P6lJFkDRmE8KWtBM/AscHpy7D04F4RmXNaEn6rPgnG1gFx0/+skwalD+afIt4iIiIiIiIiIiH5gnB/0UmBb6xj9wVxja9ymotLmM6sliYQpbgK2t9a+NpTXfkNXK4mIiIiIiIiI2AhMwLG0g1JQNxcYY05IXBXG4FJQPYNjZocUUUmNiIiIiIiIiBgArLXLrbWXjvZ9ZAAnkiuWsT3OBW3ITfPR3B8REREREREREZE5RCY1IiIiIiIiIiIic+grEfJAUOoU7FCnS9jk/mhvdzmZb7jhBgDuuOMOZs+eDcDy5csBWLFiBVtssQUAO+64IwAnnngiANOnb1Ie4Mz1x8qVKwG4806Xg37u3LlUVbnsL/PnuxzJM2bM4MgjjwRg111d6tDKylzucVkKXFaSjULm+mOUMRzpRWKf5GPI++Pqq6/mmGOOAWDSJJeHu7W1lT/9yeVkP+QQl8ls5syZxS+wcchsf3R1dQFw+eWXeznR3OxSfh500EHU19f3fRNRhgwVRrw/enp6KCtzXFyx59fU1ATA5z/vMvftu+++nHaay7Sk8TF9+nR+/OMfA/DKK68A8IMfuJTT5eXlm3L/cXzko2h/RCY1IiIiIiIiIiIic9gUn9TNUmvfBAy6P7TL32effQA44ogjAOju7uaJJ54AYNUqV3GssbGR4493FQzFNL7++usAXHHFFYwZM2awtzEq/dHb2wvgd7sLFizg6KOPBuDFF18EoKGhAXAMqdo8YcIEANra2li/fn3eNU899VQArr02V/xkEGxIZsZHRpApJvXrX/86AN/5zncA2HZbl7u8qanJP+uWFlct8ZRTTuGXv3RFgzQ2br/9dgCWLl1KXV1YqGejkNkxctRRRwHw2muv0d3tMtzIClFWVuaZQzFBDzzwwFD8bOb646GHXK0Mte++++5jxYoVAFRUOEPiGWecwRlnnAE4lhly8gVyskMoNRly33338ec//xmAG2+8EYDtt98egDe96U1evtbU1ADOanfPPfcAOfn8nve8B4Bjjz3Wf3cQGLH+CJ+ZnpeY0WeeeYbVq10l4XHjxuUdu/zyy+npcfn/Z8xw1WEffPBBnnrqKQB+8YtfALD//vsDbr1qbGwEYK+99gLYmDU4E+MjQyjaH1FJHTpscn986lOfAvCL5imnnOLNcpo4S5cu5YMf/CAAt956K5BzBfjNb36zKT+fif6YPn06//Vf/wXg3RrOO+88AMaOzZValuBpb2/3Su0117iCKFp4Fy5cyJZbusptaWV4AMhEf2QImVJS3/zmNwPwwgsvALmNjDGGtrY2IKdoLFmyxCsfkydPBqCjowOARx55hG22CYvEbBQyN0YWLnTltKWkVldX+0U0HPtTp7ry4Vqc3/GOdwDwsY99bFN+PhP9MXfuXP71r38BcNtttwF4l4elS5d6k63649RTT/Vy4qWXXgJgv/32A5wbRKkpqb/97W8B+PWvfw3A6tWrfRuqq6uBnDzs7u72mxehvb3dnydFXkSAMYYDDjgAgJ/97Gcbe/+j0h+PP+4qlz733HMAjB8/3rdZ/SL5MWnSJB588EEgJ1t6e3v58Ic/DOTWoGeffRZw/SMCSTLl0EMP9eNpA8jEfMkQork/IiIiIiIiIiKiNLApgVMRQ4zOzk4AdthhB8CZ6rRr//e//w3ATjvt5HfB2v29/PLLI32rw4bddtuNf/zjH0CODdJu11rrd8BykWhvb/f9JpOdgkAeeOABTj75ZCDHmlhrBxMAUTLo7e0tYIs//elPA3jn/80BaqOYDZkqe3t7/RhRIOLYsWM9K6+x9J///AdwLjObwKRmDjJjKiDEWustM+qr7u5uzzavXevykW9i0GWmcMMNNzBr1iwADj74YCBnXXnrW9/K3XffDeTY0q233toz0GLaxahOmTLFs4qlkK7xscce43//93+BnCl7/PjxXl6m3Z7Kysr8eiLU1tYWyJDa2lr/vUceeQTIse4ygWcVV1xxBQB77rkn4OSCWE8F2S5YsABwrnNyHVKAXX19PUuWLAFyckPo7Oz0fSO2+aabbvJW0YhNR2RSIyIiIiIiIiIiMofIpGYI2u2LCXr++ed59dVXgdyuuKysjEcffRTA+5qFKZdKFbvvvjvg/Ae1IxV7LJasra2t6A5ffrvqNzFGp5xyCk8++SSQC7DZ3JnUkO157LHHgFywxE477cTZZ58N5HycNzGFyqhBAXQKChJT1NnZ6dumcVNWVsaaNWsACoKkFi5c6Bm1zQF77LEHgGd+jj32WO+Ll069BHDvvfeO8B0OHxYvXgy4MS0mWVYWjYnGxkbuuOMOAO/L3tXV5dkwydlly5YBjlkrJab9Rz/6kX+tud3a2urlpnxMNW+AAv/M3t5ez65KVupYRUWFT2X2zDPPAPDqq6969jFrePHFF/39qu1r1671r/W8Q0uM5o5kSnl5uR8f6iuNK31H54GzYig4T8x8xOARmdSIiIiIiIiIiIjMITKpGYL8oOQD9dJLL3lGYJdddgGcv4sYAO3cxKiWIpQmSn61kydP9sxw6EOn/2IEFK1dUVFR4G+o3f+UKVM8ayJsRHR/SSJkieWTqf785S9/6dObye+5VCFfSo0DMSFiSCDHnllrC46n01Rtrjj66KN9+hz5ndbU1Pg5szlBY6GxsdH7GCqNkOb96tWrffS7/FY7Ozt9Si6NDzHvr776qmdSS8ECM3fu3LzMJ+DYP30WMqjg2qt1RMxhCM2TMCG+5pWu9dxzz2WWSX3ggQf8va9btw5wY0JtTqcvrK6u9mNA3+vt7fVtTfdVTU2Nv678wcvLy3n++eeBXLGMkcTll1/uM+QUg1hg/Q/bPBRjXH01d+5coP+15l3vehcf//jHgZxlI41MKKn95bAsFggi/PGPf+Td7353wbVKQZgUg/LPSXGz1vq2y+xfXl7uzdtSTr/xjW+M8J0OHbRgyOxSWVlZICzDdCjqDykdVVVVXmjqezq/vLzcL1YyD8v0s7mhWFCH+kj9s2bNGg466CAA3ve+9wH55sFSQtrEpjkfVpgJ046lU5Dp/PQitbmhqanJzwe1fe3atXl5QGGTqiplBjLRl5WVefOs5OZxxx0HwLx58/yGX0pFTU1NQS5NBZZNnDjRX78U+mjVqlXepUVKanl5ecHmLAycCkkAyAVJQaFMHTt2rCdONKeyHLj76KOPsvfeewO5sfDAAw/4fMnFXOXUD2pfb2+vJ0wkb8LAK+XglaLe0NDAvHnzgNFRUj/ykY94mV8spdyZZ54J4NNk1dfXe0VbCN3B1NZ08F0I9cvatWv9a7kZHXnkkd7dTlC6zFtuucUHN/eFzZtWioiIiIiIiIiIKElkgkkttjNNmxnCz0Rlv/DCC3z3u98F8Gkx+tvlhqk2smj2/exnPwvkdhn19fUF6UF6enr8rlhJhLfeeusRu8ehxmuvvQbkdmK9vb2eAdSONty5FXtu+qzYMZlzVXlG1bo2N2jch+NfqWi0E54wYYJnDjTGrr/+es+EhMUSwD2LYtfNAtLzohjLpXNCi0QapZBWaFNw4403+uANmcMrKyt5+umngUEVucgsxPDV1tb612JXhWnTpvmAzAMPPNB/rnGjPlLgy/bbb+/ZdsmlLGPt2rXeKqV539PT44s3qC1hwKSevT7r7Oz0r3VMAUPGGM82ax3KMpO6evVq78oxZcoUAH71q1/5IEKxn2pne3t7QXGDzs5O35caA5KVZWVlnnUPAzZluRsNfOELX/Bz/l3vehcAb3vb2wC33mpuhGypmPK0nO/u7vZtl6zQ98LPQvZZ/Sf3oltvvdWPlfvuuw+Aww47DHDrz4ZkcOlLpoiIiIiIiIiIiM0OmWBSi6EYM/KBD3wAyJU73GqrrXw6pnPPPReAiy++uM+0OllnC3beeWcg52va0dHhmS/de7iL0Y5FfoalCCXRHj9+POB2ZGkn7tDvMM3slZWV+bGS3gGHgVYqIbu5MqnhfJHTvkr3adff3t7u+1K73DVr1njm5e9//zvgfIgg2/MlHQCi9oc+zZITy5Yt83526bGVvs7mhtdee82zRUuXLgWcfHn99dcBfIo2+e2VMuRXV15e7hkv+Wfq2NSpU718lY+ifFQhNz7ENB988MHer70Ugg3lhwo5lmvlypV+Lmj9COVoMWtdOkWdZOuqVat8wI0YSqX+yhL0/GbPnl2Qhqyurs4znVp3JAOttQW6R9gfupb6bPz48d5ap76vqanx4+jFF18EXPq/4YaKVJSXl/POd74TgG9+85sAXHXVVUB+0Q4929AfNa079fT0FPj9QyGDKra1urq6ICZg66239gy++lGFiI455hh+//vf99uuzCupkFNkJGRlklm+fLkXHDLhjBkzxit7p512GpBbrKZPn86xxx47Ane/aQgd39MP3BjjB0YpmJ/6Q29vb0FN9bVr1xbUGu/P1FzMVBBWqNIEk1vB5oqwjxQMJUGgeVNVVeUVOAmNcePG+cVGFb4uv/xyAF+vOsvQ4hkuvnrmqjy2YMECr6TqmMbI5qqkSmZOmDChICK5vLzcyxPlutwclNT58+cDblOmgCdlgdD8WLdunVdKpah3d3fnZQQB8sbLokWLgGwrqaF5Ob2RDwNRpTyl5wEUd5tKEwBhjl1dX3lFswRt0F955RX23XdfAP76178CLohObZQc1JobrqlhlcJ0dH/oHrDddtsBuWpUM2fO9G4mr7zyCjAySqryYe+5554+sl7PXkHZXV1dnvgKA2vTSmS4cdE1QnN/qMBDTkltaGjw645cCF577TWvCCuQ7PbbbwdcNUSd3xeyS5VERERERERERES8YZFZJjU0N4gllbavNEItLS1+hyPNfuedd/a58UT5S4tfv349s2fPBkZmZzNYaOdSVlbm+yHc3aZ3MaUKMXyQX9kkzQRsbHBLGAAgKBBic0OxwBelRJFDv3bMXV1dBec3Nzd7U5cYgW9/+9uAS212yimnALkgrKwg7egvpmf16tVMmzYNgP333x9wDIrSraTNWWG6nc0JYpLKysp80Iz6rK6uzluXxCZuDhAb1tzczD777AMUmqKttX5eiFEyxnhZIdcYBZysXbvWs0RZRihL0/KyWJBUyBIKobwV45o2c3d3d+flH4ZspnGTFfWoo47ybdX47+jo8O5fYs4lP3p7ewvypIYuROE1wLlLKQ2m0k7tv//+/jPJ1pGA5P6nP/1przOJYRcDvmLFCh9oLTcFa61vl/QpPdNw7Icm/mLuh+DWmvSxWbNmeUum5JLG0wsvvLDB8ROZ1IiIiIiIiIiIiMwhc0xqsSAY7QrE9Ahbb721DxLRbrGnp8fvjsS4ane8Zs2aTNdhlh+L/Khqa2v9zka7k+7ubs8A6Dz1j5ijUoH85mDjUxyFfqdpViC8lnbF4W9tTkj3m7XWpxjRuA+ZD80T7axra2t9H4lVlJ9wW1ub9+fKGrTzFysmFm3BggW+PfI/v+CCC/Lqkocodb/uviAGsaampqDueFVVVUGaolKGkvLrGb/5zW/240HQmOjt7fVzQHI0TLWm/pDf6t133+395cPgkKxBLF7oPymMHz/es1VjxozJO1aMOezt7S2YJ2K+dt99dx+srN+RvMgiwuItYdDs1VdfDeQqi8nCun79+ry1FvKLyKTTka1cudKz9vo/WtBz2Hbbbb01TJXl5Pc5ZcqUgqDR1tbWvEp9ITo7OwviYor58as/QiZVfVVZWenjTOQr/sILLwBu3EqP6QuRSY2IiIiIiIiIiMgcMsekppmhhx9+2Ecbq7SdWKCddtrJa/BiVFtaWnzEqrT20J8knaYoS1CCdfmKjBkzpoAlLCsr832knc1ll10GlB6T2pcflZiOYsn80+cUi0QVwsTCWUyTMhRIs8fPPPOM31GnS/l1d3f7uaNjY8aM8Z+JWRI7uddee/He9753JJqx0RATqDkgttAY4/0tw4h1scnpMogbiiwtVYhN7+zs9PJPTODYsWP96zBlUalCTIxiD+rq6gqyN0gOdHV1FcgVY4xnktQfWjustd6vT8eyyKTqeVdXV/t2iU2ePn269wlUyiW1JUxB1Z+clXzZcsst+ec//wnk5lwWM2SEzzYtI9va2nw/pEsGd3d35/nwgxs7aqPkRxgrIvkZprFKYySKoYRzWZkGNI7DOBY9tzBtn+SBGPONlQthtoN0vEBra6vvy3QMQHV1tU+D1xcGraQOVx1jdZIcm1999VW+8pWvAPCPf/wDyFVYWrhwoR8s+qyrq8sHyYhuTtctzip++tOfAjnqPLzf8LWEioTQlVdeCcAVV1wxIvc5VGhubvbjJxzY6dRToUN/2vE/NEul06qUl5cXDRAoJVhrC+rU94fbbrvNzyGNI216jDHe7KJrtra2FvSpBMpoVk3ZEDT2paBojnd3d3sTVzHZlP4sXcN+c4HMv62trd6kqXFRV1fnZaQ2JKWM9Aakvr7eKw5S3sKKOOnAn1CGaIMnpXbBggV+PIWBmFmD5ngYbKuN6KRJkwqqQoVrYrG1PJ2+Sv9XrVpVEFSVRfSnl1RWVvpxobRiUsrCIKnwWqG7CORXSCy2aRmNCn16josWLfJpweSCELZJ41j/i1XkC/OQh6+BPNeA9Bzq6OgoaHt5eblXgiW3RRqtXLnSy6e+EM39ERERERERERERmcOgmdSh3Clo93rdddd5BlW7tG233dbvWsSWalfY0dHhE5Frp7Dtttv6mvZhAFJ4TtYgxjdtpg3TMYXMoXYv2sGJHZs/fz6zZs0asfveVKxfv74oO5hmOsKxFgZMgdulpVnScFeYNkUtXrw4r+pGKaA/BjW9A77yyit9EnuxBWKYwgAAMQe9vb3+M7FqGk9KRJ1FaEeusaL2WGsL2FFjjJ/7mluCgiw3N4RpXdJzoKenx3+2OTCpMvMrwGns2LF+jVCgbMi0h5WB9H2tI4Jk68SJE32wUZZdI8SMh0xgGDxWrCgM9J2CKn2+zuvp6SkoCGCM8X1TCindQvZY9yvmvaGhIa/ohc4XJG/CYg/pILPRgvSdRYsW+XuWvJNOVFZW5lnNMGhS7SqWzL+YFTK97uiczs7OgjSHY8aM8WNRn2lePvnkkxtMDxmZ1IiIiIiIiIiIiMxhkwOnQp+5/nzAwmMKmLnhhhsAePzxxwGnxe+4445ALh3TE0884Xct0sKVgLqrqysvxQS4HYB21EpiLd+L1157rSCFRBYgX1v5PhVLsB3u/tN9qtRct956K2efffaw3+9QYc2aNXmpw8C1Kd2+cJeXThRcVlbmPxMTLad4KGQhn3322ZJiUsN5k66nHUK+R729vZ75CXe34FiWtF9deXm5Dx5Kpw7R/MkixBAUS4uixP1CQ0NDXjBiiPT7zQWSb7W1tT54Iqy5LrZ8c2i/GKQwxZbWD7UvLO0Y+hOCmwN9lXnceeedfRBKmj3KEuTXV1NT4+eC2ODW1taCQFO1L/QvDNmzUB5DTpY0NDQUfNda68dWKTCp3d3dBWWSZVGYMmWKb1+xuId0idC+UjeNBsJYHc15IQy21r2HsjMtR0NrZn9FdYpZetPBiuvXr/fXU7o43evTTz+9Qav8JvdwWCt7ILj00kt9FLuUq7D6icz94TVlztEA0flLlizxk1OdtHLlSt85MuGIWu7q6vK0t6pSZQH3338/kLvfgw46CIB77rnHt10VsubPn++Vhze96U2ACy4DePHFF0fupocA69atK1BIOzo68qqbQH51lNDMr+9JqQonhf6nzbvLly8ftvYMN9Lz7M9//jOnnnoqkDNbhwp4Ohq5u7s7L/cfOGEk5V6fSUhnaSOXRlpJDQVousZ6mCUjbfreHJS0YpDcqK6u9n0lRb2qqsov0ptDdgPJP2HSpEl5QWIhysvLCxSw0IVKrjGSIcYY5s+fD+SUYbnTZAkyV1dUVPjnLLKmqampoDKj0N3dXTRTShjZDrnxpBrwkK/IZtWVrhjWr1/v53164x/2TzEzftqUPRoBUn1h5513BuDBBx/0m9A0QlefYlkZ0opoT09PAZEUvtaaEeYcFnT98vJyL4M0jkRG3nLLLWy11Vb9tiu7W8OIiIiIiIiIiIg3LIaEq07vOLTjfP3111m0aBHgWEFwpv4999wTyDE2qucapsiRZt7R0eG1du1+xJ5uscUWbLvttgA89dRTgNtRqnqEzhdTVFtbm8m0GTInLViwAIC3ve1tgGNKxY6qHnlvby977LEHkDNrq3qD3AVKBevWrcurnw2OTdbOLc0EhkxiuItP51UVIx2aHgSxDKOJdNqOYrv3Ymakv/3tbwB86lOfApwlQRWhwjyP2q2qb0NTTNpkWVtbW1A7Wf0tdiaLEAOocRA+ZzFIQn19fUEqonRgweYG5cUM5adMmmVlZX5uZTmt0kBx+OGHAzk5X1ZWVjC3NMbDY+G6le4HjZddd93Vy+UsB9mpfdXV1T79kNxeurq6fFvTwV/W2qJrYjpAVTJlu+2283NP15w+fbrv+7T7XRbR1tbmZaMY4LACW1o+h1W5hJBRzUrg1JlnngnA97//fa8vyHIc5sJOy8BwXU2nHCuWsq0Yoxpa6NJ91d7eXpCXWbKotbWVt771rf22KzKpERERERERERERmcOgmVRp0F//+tfz6oBDbpcRVvfQZ42NjV5zT+9eKyoq/DFp8iHLJIZWO7k999zT7xrlKzNnzhyv8Wt3p/crV67MZBoR+RFqt6HKUd3d3Z75eu655wC3kxX785a3vAWAX/7yl0DO97YUobYX8zsNof4IfYjSVYRCx3ddS37MG0p3MVwoluqlv/YJra2tHHrooQC+Zrbe77jjjn5HGjJj8iNTP2he1tTU5FXe0ffStZy1ExablEWkg8P6Q0NDg29LqRZ12FgsXLgQcONHwQqSlRMnTvSBRVku2DBQyHIWIu1rGFagSrNoaeYdcnJi55135gMf+MDQ3/QQQ3M99NFXv8ybN6/P/ghZ1hBpNjG0VMhiJ//+ysrKTK6rfaG8vNyPAd13GGSdDhQKg43S8rmnpycz1tmjjjoKgP/5n/8pSEOm921tbT7WQGNh/fr1fsykA6fC88Jxki78EvokF0tRJkZXDK/kTl1dHR/72Mf6bVdkUiMiIiIiIiIiIjKHTU7m/8EPftCnkHrppZeA3A4r9HXTjmXdunV+lypNW9+bMWOGTyAuzby7u9snqJef1Zw5cwDnoyeGRNHvoc+hfOvEJFVUVGQymvWEE04A4KabbgLw0aQ77bQTd911F5BrS319vY9elr+vdjhZbFt/mD9/vmcx1Iampia/00v7tvT09BTd9YfpqCDH0Ie7QV3rgQceGMomDBgDiQJdtWoVjz32GAC33347ANdee63ffWrHOXfuXMCl80jv7Kurq3271Q+yKIQ+qeHcSLOr+n5LS4tPF6d7yAo01ouluEuXKRw3blwBg6rz1fbNDaH8TNexh1zBhs2VWU6nwRFC+SErRFdXVwF7Vmr9EpbS1rorX7/777+/QP6EsrG/1Frp4h/V1dWeodX6U1tbWzRSPKuoq6vzslH9ov5ra2srmqi+mB8z5Kdvygruvfde/+yLMcDpMR5m1Ck27ou1L20NDLNHhFkzdE66+Ir0vOnTp2/Qj3nQSqocqRsaGjj55JOLnrN27VpP8+r8lpYWP4nSZpauri4fDBQ66aYrQKhD1q5d6x2edayystILJpnF1UFhZYws4YADDgBgv/32A+CKK64A4JxzzvFmTZlz2travEP8BRdcAOSE0bnnnjtyNz0EaGtr8zV8w9ycams6NUoYCJWu163j4fnt7e0FJmvlZxst/OlPf+LSSy8FciYPzZFQGGjibr311j5g4/nnnwfwOfCam5sLcqEWE5o6JxRUoXCWAq95Fgoq9V/WlNT05iZEWkkdO3Zsgdkyy+m1hgJhvkop9Ao4HTt2rJ9vm2sKrrQLSxgsotdaC3p6eopWt4PiwYZZRLih12ul+YFCxaTY5k4olqta35s3b54nje644w7AyemNSUM52mhtbfWug0rbpPlQrIJhmIYpbfouKyvLXPBhQ0OD1yEUTKU1o1iu7K6urn7HeH/BUcVS+xVz15OckR4o+fuvf/1rg+3J/uyLiIiIiIiIiIh4w2HQTKrYyfnz53vztLRjpQAaP368Z65CTV1mewVcifksKyvzn4WJ3NOafJhINp3ov62trU9H5p6eHu/0feCBBw6y5UOPefPmAbnKW+9+97sBl6Ran73rXe8CnOlGu6LTTz8dgJtvvhmA2267jWOPPXbE7ntT8fe//53LLrsMgPe+970AfOQjH+HPf/4zgE/yW6wSitDb21tgeghreYs9kmkvndx7pCAz/te+9jXvpK/2yVUlZG3EhK1YscK7z+gzpSybMGFCXs16cIxqOqVUuAPWPNFuOky5kna2Lysry1Sy6hCaA+kADyhkUuvq6grOS5+zuUHt6+np8SyGxk9VVZWX1ZtrP8jCkmYQu7u78wpZ6FiaYSxm9u+Lbc0SwsAwQfKwGIwxBcUNjDEF80XXXLJkSdG0bVmqvLQhFHNNkMwMA4DC/lD70oVjwvOyBFUg/PKXvwzA+eefDzjdTM+vv7RRYdrHY445xn8X4OGHH/bmeq1XGh8VFRV+zISVIpXGTQHuf/zjHwfclsikRkREREREREREZA6D3v5oJ7H99tv7XZdYULFXK1eu9HXoQ61dKVH0Xzv82tragvJkYboIafvhbleMgD6bPHmyv0bIHOicLNZt32WXXYBcOUcxZdtvvz3ve9/7gBwTGKYdEsuq3V8p1E1O4+Mf/3je+/nz5xeUdAsDozQGwt2wXmucaEwo5Q6MHoMqPPjgg4BjRsUCyt9TgU2VlZV+py6mM9yZpksAL1u2rMBfu6yszO+CiyWg1rEwTVyaNVF/Z3k8KaVY2vcQClN7VVRUFLAdpRZkuLEI0++lmcPe3l4/zkotQGigSPvdhUEi6eIgPT09RYM0w+9lHaHPteI0hCVLlvhYj2IphoSw/HS6zKmOLV++3Ft+hM7OzkzGevSFrq4urxPouYfpM9Pyo7Ky0p+ntUXfz1IKqrCAheSd9AfpFhdeeKEvEBQGKKfXnTCO4ec//zmQ8ysN2Xr1leTNmDFjCooFVFZW+hLuV111Vd49h5aNvjAkHH1YLSj8HzEwpJUrKS/z5s3z1LwUlIkTJ/rNgJR8BdlsqHJD1lAsKCHMSRdWvYC+zSphdTLITRy939BvjgSOO+44AK677jqf8zadxaCqqsq/1kJaVVVVEJGf/g/kOfYXM2fqf9qMWVZW5q8vQaN+njlzJk888QSQH4SRBUhJ1WIRKhNpc2eYL1d9IpeLzRUiBxoaGnxOVP2X3IBcJpbNDTJx6zmHLi1pxbOzs7MgICYrisdAESpZxeq2pwOhigU6hedItugzXbOpqcln1xF6enp8AOzuu+++iS0ZfoQbeckKjZdw/ZEc7ezs9OdJRobfz0rQWH/BcDL/33zzzTzzzDNALvDthRde8Mpp2h3MWpuXBQNc29NkUeg+JBdPVcY85phj+qzsNxA3kWjuj4iIiIiIiIiIyBxKx9v5DQCxQ8r12t7ezpNPPgnAO9/5TgD+8Y9/+PQ7MuvIKbkUUqWEKHa/W265pQ8kC1NPQX6qmP4qOOlYsQpco2W+073cd999PjDsN7/5DZBzBZg3b96A7k/tDQOhhhKh24kc3bMGmS/TrOmWW25ZYO6srq4uyN3X185+c0GYekltLpamTCxJKSMd0NTS0lLgBhOaw9Npy8L3pc6kdnV1+RSFwmOPPeZdjOTmErZP/Raa+9Wn6fR099xzD5dffjmQM//29vb662cRaXew8vJy3w9iANWWkFUPK5OJOZXc0DgZN25cZoIPBxrAtdtuu+X9zzpKS6uJiIiIiIiIiIh4QyAyqRnCvvvuC+Adm7u6uthzzz2BnB/Zjjvu6AOCtBs++uijR/hOhw9hLfFiDGmYrkxIV5eRE/jSpUu9/67YtSwEQpx44ol5/0PIt0s+hUuWLOHVV18FivscyVcsrKomBkD9Ee7007vtMDAx7Qw/adIkZsyYMag2DjdkTRDTo8CNzs7OgqCZ8LN0Sp3NHWPGjPFjQ2xRXV1dQcWuUkaaSe3u7vZjOF2gIqzAJbS0tPg+Svu3l0r/hP7nssgJDz74oC8YongRzZfe3t6C/gitM2IONX/CoCnJ1La2tgLrRZaxYsUKb0HQcw4rUEle6pxx48Z566bOU3uXL19esMZEDC0ikxoREREREREREZE5RCY1Q1DkoHb1NTU1PpJSzGFZWZk/LyxqUKpIlzLdfvvtvU+qStdp99pXmpN0lLx2tIcffnjB7jYrkZh9QSnSspgqLUvQc1QhEVkcnnzyyQJ/0/r6el84QSyRyiFurgizN0heaH6sXr3aW2MOOuig0bnBIUSxUqbKkKJ2qg90PERNTU1eNg3IpVMMS2JmGWL9mpqaCuSkIruHC6tXr/Ylm9PpqbKAdOxDY2Oj98dUuWf1WWtrq4/01/fmzp3LdtttB+TkjiwRW2yxxWZfYnm0YTbB/Dn6dtNNw1DbcTa5P/7yl78AcPvttwPODCUhK0VtzJgx3pyjRedtb3sbAGecccam/Hzm+uPZZ58F4KGHHgKcQiJTfpgCRK/33ntvAI466qjCm9n4ajGZ649RxnDYPQcvfFJya5TMspkdI1LSvvSlL3kTr/IqH3HEEd5FSIvvEAWSZaY/7rvvPneB1LwP68xLplZVVRW4xuh/seDLjcCI9ccjjzwCwK233urdxo4//nj3JWsL0vgVCzwdCEKFT+Np6dKlvtLhBq6VmfHRH5SqTenLFi1aVBCMNkQoif4YQRTtj2juj4iIiIiIiIiIyBw2hUmNiIiIiIiIiIiIGBZEJjUiIiIiIiIiIiJziEpqRERERERERERE5hCV1IiIiIiIiIiIiMwhKqkRERERERERERGZQ1RSIyIiIiIiIiIiMoeopEZERERERERERGQOUUmNiIiIiIiIiIjIHEpSSTXGWGPMdgM4b+vk3NKtGzoAlGp/9HffA21Tke+daYy5b9PvLmJzRanOl4iRQymOkShP+4YxZp4x5og+jh1sjHlppO8pYmAYUiXVGHOQMeYBY8xaY8xqY8z9xpg3DeVvlBLeKP1hjLnLGLPGGFM92vcyXDDGHGqMWTQE12kJ/nqNMe3B+9OH4l5LFW+U+TIYJItsuzGm2RjTlPTTWcaYkiQaBos3whiJ8jTvvGGXl9bae621O27gPooqucaY04wx12Rps9IXSlWGDNnNGWPqgb8APwEmADOAC4GOofqNUsIbpT+MMVsDB+PqBr9jdO8m+7DWjtUfsAA4IfjsdzovC8JuJO/hjTJfNhEnWGvHAbOA7wLnAZcXO9EYUz6SNzYSeCOMkShP8zFQeTlcGIAMPA7463DfxxCi9GSItXZI/oB9gaY+jm0L3AGsAlYCvwMag+PzgP8BngbWAr8HaoLjnweWAIuBD+Mm8HbJsbcDTwDrgIXA14PvbZ2cWzFU7Yz9UdCWrwL3A5cAf0kd+zXw/4BbgWbg38C2wfHwvg9K7vewIseqge/hhNQy4OdAbR/3c2ZyPz9J+u5F4PDg+HTgZmA18Arw0eBYNfDDpF8XJ6+rgTFAO9ALtCR/04eg7+YBRySvDwUW4YTGUuC3fd1P0M77UtcL++w44Pmk318H/ic473jgSaAJeADYPXVP5yVjr2Mox0qcL0MzVoLP9kvG5BzcXLsUt2C2AkckY/2PwArgNeDTqe8+mrR7GXBJ8nkNcHXS103AI8DU0W7/G2WMEOXpRs2B1PFJuE1MU3I/9wJlG3r+JLI39TuhDLw2udf25F6/kJxXlvTfpKQvbdCeA5Pj5wPzgeXAVUBDatx8LOmbJcC5wzx/CvqPEpAhQ9kB9clN/QY4FhgfHNsOODIZoJOBe4Afpjrv4aRDJgAvAGclx45JOmBOMrivIX/CHQrslgyI3ZNzTxoOARL7o2g7XwHOBvYBusLBmAz61clgrsAtHNcFx23SF0fjBOp+6WPJ6x/iBOEEYBxwC3BRH/dzJtANnANUAqfghNKE5PjdwM9wE2lP3OQ7PDn2DeAhYEryXB4Avhn066Kh6LPUcw6V1G7gf5NxUbuB+zmT/pXUJcDByevxwN7J671xAnN/oBz4YHIf1cE9PQnMpI+FK86Xkf+jjwUatzh+AjfX1gJvSdpSBzyGU3qqgG2AucDRyfceBN6fvB4LHJC8/jhuftUl42MfoH602/9GGSNEebrRcyA4fhFO4a5M/g4GzACef969UEQGFvtt4ADgwb7GAW6z8wpu7o0FbgR+mzr/WtyY2y3puz7bNwRjq2j/kXEZMtSdsHPS0EXJwL6ZIho0cBLwRKrzzgjeXwz8PHl9BfDd4NgOBBOuyLV/CPygr4Ezkn+be3/gdutdwKTk/YvAOcHxXwO/Ct4fB7wYvLfAl3A7zd1S15bANbhdXcgYHAi81sc9nYnbmZrgs4eB9+OETg8wLjh2EfDr5PWrwHHBsaOBecnrQxl+JbWTfHanv/s5k/6V1AU4YVGfOudSkoUi+Owl4JDgnj4c58voy4++xkrq84eAryT9dlXw+f7AgtS5XwKuTF7fgzOVT0qd82FS7HqW/jbnMUKUp4OaA8HxbwB/LvbcNvD88+6FIjKw2G8D3wQu6GscAP8Czg7e75g834rg/J1S93T5MM6dov1HxmXIkDrMWmtfsNaeaa3dErcrnQ780BgzxRhznTHmdWPMOhwVPCn19aXB6zacZk5yjYXBsfnhl4wx+xtj7jTGrDDGrAXOKnLtUcEboD8+CPzdWrsyeX9N8lmIvtohfBa43lr7TB+/MZlkR5c4ezcBtyef94XXbTJbEszH9dt0YLW1tjl1bEbyejr5/anvjRRWWGvXB+835X7ejVvE5htj7jbGHJh8Pgs4V32Z9OfM1HUXMgp4A8yX4cAMHLsG+e2cBUxPPecvA1OT4/+FU8ZeNMY8Yow5Pvn8t8DfgOuMMYuNMRcbYyqHvRUDxGY+RqI8HSCMMVuFQVXJx/+HYy7/boyZa4z5YuprG+q7EAORgRvyRy3W/gpyczD9OyO93giZliHDFtVlrX0Rp5nPwe2uLE6zrgfOwO3oBoIluEVU2Cp1/BrcbnqmtbYBR/cP9Nojhs2tP4wxtcDJwCHGmKXGmKU4k9Aexpg9NuJS7wVOMsZ8to/jK3G+QLtaaxuTvwbrHOn7wgxjTNjmrcj5RU0wxoxLHXs9eb0YNzHT3wP3vIYb6d/o735acYsNAMaYaXkXsvYRa+2JOFPbTcD1yaGFwLeDvmy01tZZa6/t5z5GHJvbfBkOJFHtMwClCAqf20IcOxY+53HW2uMArLUvW2vfhxsf/wvcYIwZY63tstZeaK3dBXgzzn/5AyPWqI3A5jRGojzdOFhrF9j8oCqstc3W2nOttdsAJwCfM8YcPtif6O99Im+3AB7v43wo3v5unLuIkB53ixlBlIIMGcro/p2MMecaY7ZM3s8E3oejksfhnImbjDEzcE7qA8X1wJnGmF2MMXXA11LHx+F2c+uNMfsBp21qW4YCb4D+OAln6tkF54u0J84Udy8bNyAXA4cDnzbGnJ0+aK3tBX4J/MAYMwXAGDPDGHN0P9ecklyv0hjz3uS+/mqtXYgzQ1xkjKkxxuyO2w0qSvRa4HxjzGRjzCScL87VybFlwERjTMNGtG1T0d/9PAXsaozZ0xhTA3xdXzLGVBljTjfGNFhru3CO7T3J4V8CZyXskDHGjDHGvD210Iw43gDzZchgjKlPWIvrgKv7YM0eBtYZY84zxtQaY8qNMXOSRQljzBnGmMnJ/GpKvtNjjDnMGLObcZG963DmyZ4i1x9xbOZj5CSiPN0kGGOON8ZslyjUknlDNXaX4XwyheOA2wOGeQUuACk851rgHGPMbGPMWOA7wO+ttd3BORcYY+qMMbsCH8IFdA07SkmGDCWT2ozzYfi3MaYVJzieBc7F+S3sjXPKvRXnQDwgWGtvw/kA3YGj8u9InXI28A1jTDNuElxPNrC598cHcb4pC+z/b+/Mo6Mq7z7+zUwSEgJJgCCQiKAsLrig4lq1VuuCUutp8VhtFbVataiV44JHrcupb6tWtC5VK29bLZWK6KuI2ipIRRGsuIIIhE0IwSAkECbJJDOZmfePy/d3n3nmEpIwM7mhv885npHJnZl7n/vcZ/n+tkSihv8BeALAT3M6kL4okUhsgDOwTs7JybnS45DJcK71wxzHlDcXjn/PrvgPgBFwVIP/ATA+kUjU7vzbRXD8gTYBeAXA3YlEYs7Ov90HJ1pxCYClcHbJ9+08xxVwBp21OY7pIxtmmbbOpxKOD9ZcAKvg7oTJJQC+3tle18BRlpBIJD4GcBWc+7QNTrteluHraA97+/OSDmbvPM8qOD5kD8OZ2FJIJBIxOGrSaDhRuVsB/C8ALgrOBrAsxzGVPgrgJztdTQYCeAnO5LIcTmDM3+EP9uY+ouPpnjNi57U0wAnqeTKRSLybhu8FHKX+zp3nejMsU38ikWiC0zYf7DzmeDi+ztPg+G6uA9AM4Hrre+fDuRfvAHgokUi8nabz3RXdbgxh5JuiKIqiKIrSBjs3DDVwgs/qO/kdQ+Es/PIsZVWx8HWlAUVRFEVRFB/RF05Uf6cWqErHUCVVURRFURQlS6iS2n50kaooiqIoiqL4DjX3K4qiKIqiKL6j3RGDHnR3CTbduRC1PZLR9kimU+3xwgsvoKCgAACQn58PAIjH4ynHBQIBeaV1pEePHkl/a25uxtlnn92Z0wAykztU+0gyHWqPujon//aWLVuwcOFCAEBDg5PX/Prr7SDiZO666y4AwNixYwEA4XAYADB69Gj07du3I6dh4otnxkdoeyTTpe3BPt7U1IQFC5xkKOXlTlKBY445pl3fUVvrJDVYutTJ2DRs2DDk5jrLqEGDBnXkdAAf9Q9e16pVqwAAr7zyCgDgiiuuwIEHJid+mDlzJj7++GMAwNVXXw0AOOCAA5AGPNtDlVRFURRFURTFd+yJT6ru6pLR9khG2yOZDrXHhg0bAAD33HMPysqcCoymWkr4/zk7C8IkEgn5fyqpeXlORbqGhgbceOONAIB+/fp19PxVSU2lS/rIfffdBwCIxZz82BUVFQgGgwCAqVOnAgCOOMIpUjR27FhRRgsLCwEAkyZNwk9+8hMAwOmnOwV5PvvsM/n+gw46CICjqnYQHUOS0fZIJuvt0dDQgHXr1gGAPCN9+vRBNBoF4D4vVFRPPPFE/PGPfwQAhEJOtdeRI0eiosKp9ErlcOXKlQCAgQMHYtMmp0hUc7NT0bqiogL9+7dVZVbwRf+46aab8OWXXwJw552tW7fKK5XUnj2dAoeJRALV1U5RseOOOw4ARFmdP38+Ro4cCcC1+Jnz1W7wbI89MfcrStbhpionJ7U/f/TRRwCA+nonM0h+fj569XKq/Q0e7FSf22effdr8bq/v7Qo4WPTv31/OneZ+Lk7y8vJkYOSCFIAMwFyI8t/btm3Dli1bkv6m+Bvea06wK1eulAlhzJgxAIB9990Xra1OgPANN9wAwHETAYCFCxfikEMOAQA8/fTTAJyJ9cornRzvfGa4MI3FYqipcUqc83XgwKSKu4rSbdi0aROKiooAAL17O0X1YrGYjH+XX+7ksb///vsBOJu1NWvWAHDcAng8XWu+/fZbAJB5JRQKobS0FACwfft2AEBVVVV7F6ldCueO6dOno6TEyc/PhSWf+by8PFx8sVNgbf78+QCAdevWiXBSVVWV9J3XXXcd3n7bqUfQgcVpm6i5X1EURVEURfEdqqQq3YZYLCaKEpk3bx7+7/+cCog7duwA4Jo1hwwZIoEk3OUWFxfj4IMPBgBceqlTEpvqqV9UVMA10RcWFsr/U0U224BO+7bZH3AVVB6Tm5srKvN/O20p8gCwfPlyAMBrr70GAJg8eXJ2TszC7u/vv/++BGh89dVXAIADDzxQ+vm+++4LwA2IWr16tQSMHHXUUQCAX/7yl2Ku5PdHIhEAzjPG54fBIf3795fjbGVXUfwIx/umpiZRTdnHA4GABArxeXnmmWcAAGvWrJHPkhEjRqC4uBiA2/+pqMbjcRlnqdi2trbKd1Bl9SNz584F4KjBDM6155GtW7fi0EMPBeCa9GOxmFjuqMby89u2bUv7eaqSqiiKoiiKovgOVVKVboOp3sycOROAkyqDvpr77bcfANcJvqamRvyKuEPcsWMHXn/9dQDAW2+9BcBNPzJp0qRMX0K7ofpVVFQkvlR8j9cSiURkx8u2iUajory2tLQkfWcwGJRdf3dnd0ro7vAKNCPr1q0T304qlPRda8unOZ3YiiV94hYuXIhRo0YBAP72t78BAIYOHSp+pzz+5JNPBuCcP/s5g6nC4bB8H4Oq+HuRSET6GZWnDRs2YP/998/IdXZHbr31VrHCUGVShdlfNDY2AnCCfThWcOzr2bOnPPNUVHnfhg4dmnIPI5GI+PLb4455LP05CwsL5fnys5L6n//8B4DTLnZaQ44Bw4YNw8SJEwG4MRFFRUXS32mlo5JaXV2Nr7/+GoDTlulAlVRFURRFURTFd/hOSeUK3dyZdnR3+vjjjwNwo/kuu+wyAM5OJ10RZ0rXsmjRIgBOFCIjE6l6ffLJJwCcyENGYXJn3adPH4mWJytWrADgJEX3S1SmmaGAkdu2Qmqmj7PTTvGzgLvDz8/PFx+i7k66/IfN75k1axYAYMqUKTJOsO1YBOHTTz9Ny+/uDnvMYxR+7969JW3Ugw8+CACYM2cOjjzySABu1DEV0uOOOw4vv/wyAODaa69N+W72Eaqm+fn5oqKQ1atXi5K6tyuFXgr9u+++CwB4+OGHATh+jExJRPa2eaWjlgr7eEbIP/PMM3jggQcycIbtIxgMyjPM8dO0NhFTWeXag58LBoNyvL0+CQQC8jeOrbFYzFfxDbuC82QgEJA5hdfCOadnz56yjuJ74XBYfHS/+eYbAO5aKxqNytycLiXVd4tUDoJegyEbiX/z6ghr167FU089BcAdOM477zwAzsCtZpm9A06aoVBInNRpzmF+u9LSUllc0MRfX18vi9oBAwYAgPybju9+gP20sLBQFp7s7+agy8GF1x4MBsUEY74HOAtYLkYUl0suuQSAm/arb9++EoTH14suuqhrTm4nzz//PADgu9/9boqpvra2VgKhmEqK6aNKSkpw2223AYBswOrq6qTP2+4EJSUlsnAlLS0t0jZ0qdlbseeUWbNm4bHHHgPgPpNPPvmk/N2eT/yUxq69eC1I+f8cX9if9ttvP8/rs98bNmwYAOCNN97Aj370IwBuTs1sYI59XhX67ByeZhtw/CSJREKO41zDdikvLxcBhG1QWFgo/cLPrF69GoBz3pwX7Lz5ubm5cu3mAp3HUxiiuT8ej+P9998HkL4xc+/a/imKoiiKoih7Bb5QUqmQ5ubmSnWI9957DwAwYcIEOc7e4XgxYcIEUQUY7EDFIR6Pq4LajTFVCqaReu+996Q6Bs0tDJY69dRTcfzxxwOApKkqLCwUpZWvVIdYUcMPcKefm5ub4qRuKl18dkxFxzRFAUhKF9Iddvh7iq0GtKVs3XfffTLmMMhhyJAhYtaniZyBVNnATLVGBYtpofbZZx9s3LgRgKve8fwBpAQ9xWIxScxtBkfx/zlWsk+9/vrr4jpAtai0tFR+Y29SUqm22aZfwE1B9sYbb0jFnd///vcpx9nzSXdTUYFUi4t5TaeccgoAN2l7WVmZ9EnWa6+oqJDAPaql48aNAwA88cQTWL9+fdLfMgn7tRkkxXmB88SgQYPk7/ZYmZOTkzJ+mMdxjuBrLBbD5s2bAbgm7759+0pAlp8tt7ynwWAwyU0MQFKRGI49HA969uwpCqo9Z8bjcQmcSheqpCqKoiiKoii+o0uVVK/k5L/+9a8BuE69M2bMEJ+Wk046CYDrd2XCNEIbN26UpNW/+93vMnTmXYcZ/EW1jTuXpqYm8Znh39asWSP+m4cddhgA19mZ6VO6M+FwWJI1czfIkm19+vRBZWUlADdx/7Rp08QXlcFJfgmWMqFCGggE5H5TJTMTq/MZ4v02/a/4OfpMBYPBbqn0dBQz4GFXvPPOOwAcn0M+F/zcs88+i5tvvhlAdhVUYp43A6ao5lVUVEigHxWOcePGYe3atQAgqhWTlNfV1cn32b6mgNunqLaOHDlSlFqqrGPHjsXChQsBOD6x3Qnb39K0xrSloLJ++/nnn49zzjlnj37T77BfUCkLBoOSnohJ69k/IpGIFJPguLJ27VrpH7NnzwbgluCtrq7GX//612xcBgDXh5xzgRnQtmzZMgCOBY0+s+z/tFLt6p7ZwVQMUFy5cqUoywzIpeWWvwX4MxUV0+lVVVVJ3A7bYdq0aQCc4je2shwIBGSdwbUW59KGhgasWrUqrefpi0UqO9K3334rgysH4LVr1+Khhx4CAPzjH/8A4HaQ22+/XSJXTcmdju6EE75Jd4vGZFu1trbKoPLmm28CgERPDh06VGR6mvvC4bAsSrkYq66uBuA8YLYDuZ8x3TU4gcZiMfzsZz8D4F4DTRGVlZUy4bJfTZw4ESNHjgTgVhnxMu90NbzHDQ0NMnDy+jiQBINBmVh4vwsKCuQ4u+LUfwv24tRcmHz22WcAgB/+8IcAgFGjRklf4t9+/vOfy2aZdJXZjosETnzV1dUyKf773/8G4IyH3HhxQ8+8jiUlJSlVpQD3evi9X3zxBQDIswS40bmHH364PFN+Nl96YS86zH8zCjmRSODVV18FAIlapvmar4C7oCkoKEha9Nrf310Wp4RzgGnyvfLKKwE4G33zmFgsJvMvA01LSkpECBk9ejQAt223bdsm72UDjoec84uKirBp0yYArsgVDAbFJcxro2KLQCa831x8lpeXy+KUFZkOPvhgWb+wz/hpkcrxjmuFRCKBM844A4BbxY4EAgE5jqb95uZmGUu+853vAHCFr5UrV6Z9vvH/ykRRFEVRFEX5r6NLJRZbvdtnn31w//33J71XXV0tK3/uYlg5JRQKSa1YqkannXYahg8fnvQd/JzXrsnPmAoQX02THeV6ml1aWlpSHJmPP/54URGpgsybN0/+3h0UVC+oeGzfvl0q73BXR/Lz8+Wa6RJx0EEH4dlnnwUALF68GABwxx13ZOGMOwZTBG3atEn+31ZoevfuLUoxA1uOOuoouWb2Fe5s4/G4p8l3bycnJweff/45AOCEE04A4ATVAY46QJP6scceC8DNh2lC5bC5uVlURbqVZAIqoVRiGMy0YsUKcV2honXbbbeJ4sfnn+baww47LEl553fyszTTUTU1g7buvPNOAM5zQhMvU1F11wpUS5cuxYsvvgjAvYahQ4fKWHr44YcDAJYsWQIASTljmWbHpLuppl7Yc8ALL7wg6Yk419K8a84vXnlE2Z9orct2oKYdLBoIBKTP8hmJRqMyZ7Z3TWCnYeK19+vXTwKmaOGor6+XZ87OOewHaDUxLSvsx5xPSHNzs6jSVIMDgYCst6hIH3300QCA5557Tt5je9BVpLN0zxWKoiiKoiiKslfjO2c12+m8oqIipboHjznjjDOSUiUAwG9+85uU7+RuKRQKiSo7ZMiQDJy9N16O9HzP3GmaaTDs4+nvMnXqVPzpT38CkKr8eAXGBINBUX4YVOHla+MX2psMmzv60tJS2eXPnTsXgFu3fMuWLaKWUF1fvny5pN6gP57pQ+MXnzsqxWYiacL2CYVCOO200wAA//znPwE4CoadUou7+ZycHE81yI+kM/hk6dKlGDt2LAC3chSVxA8++EB865imzIRtd++99wJw/L8ZFHL11Vfv8bntCo5TfDVTUNk+fsOHDxerAINDeH3hcDjFkhIMBkVR53t8hsx+T7/EK664QvoSxxK+8nf8iv08v/nmm6IgMYXStm3bJIjUHhurqqok3Z1XX6QPJC01jzzyiATl3HLLLem8lIxgBuIyWObSSy+VuYWKGdslHA6LcsjXcDgszxDVNj43VKizBZVczvmbN2+W8Z7vse+2l0QiIf2H7cB2iUQi8jeqs+vXr8eIESMAeMfDdDVUlvlslJSUiMp81113AXDnn4KCAmkv+uEWFRWJde+tt94C4AZUFhcXy/PFIERVUhVFURRFUZS9Dl8pqWakpJ1aB0hVtxYsWCAqAVXCl19+GTfeeCMAR0EB3CjuOXPm4MILLwTgKiPZwIwEtdPjtBUJF4/HJXsBFbC8vDyp2c02eu655wA4igB3t9zhDxo0SHY2VD3477q6uqR0GX6hPcnYzeh+pqDi7o7+df379xcFiq9m6dPy8nIA2VXV24tdKxpI9TGtq6uT1ESMAn/ttdfE8mCnV/FbTWk+C17lGO0SsEByCUO7wIFXloo5c+YAAC688EIp6sDnjj6fGzduFKWFrFy5EnfffTcA12+ZJf5mzJjR4ZREnYG/y3vJ/u6VTP/KK6+UlH1UPehzC7jXzGfALPjAPjJq1Khdnks8Hpfv5fH0N7P9//2GPWdMnjzZ8zgmeqfFiuPihAkTMGPGDABuYvva2lopNkNfZypno0eP9kyR2JW0ZZ0ynxf6Oh988MEyTjIynnNGSUlJSmaDPn36yHjF/kELJ+fnbME5wCznynPjWNHc3JyScorXYo4tJnyP8zAVW/NYzkOrVq2SOYXzsZ9g+j0qo2VlZZK6i2Mhx514PC7tR1/TnJwc8cdnOWZ+VzAYlLmIa5fvfe97e3S+vlqkej1I5oBKvvzySwCOKYFmKFaz2LFjh6QYYr4uNnhubi6uueaazJx8G5iTsW02WLZsGdasWQPA7SA0HeXm5spDwU5/0EEHJaWXAZyqOYCT35EPJ78/FArJ99F8xwfyk08+kdQT6aaz5tr2Hs/BMxqNygRKkx0H1Mcff1wWdpdddhkAYMCAASlO8BxQ/QQHt9bW1qRKH4A7sbS2tkp/YmALkLwQB9zFbSgU8tWg6eXWQjjZcWFlY7YBkLzZY/Al3WBOPvlkudccQPm58ePHS/twA/vRRx/hiiuuAOBWGeIgvHjxYjm3TJq6H330UQDAr371KwCuOd5MiUTq6+tl0uS10Oy/7777yoLLPsaEgTJeAVHXX389Hn/8cQBu+7E9/bpIteeM3bnvcNJlYB1dh3784x9LoCXz6c6bN08Cds866ywAbuBIJBLxXcq33Y2p3OhyAzR06FBZtHBsNRd4NOnTlN+zZ08xD/PV3EhnE9u9YPDgwUkbNtLRtIO2uMRXmv0Bd45paWnxtVuVvXEYP368bLoIF6aNjY0yBvOaOB8Brpvlv/71LwCOCxTTuY0ZMyYt56vmfkVRFEVRFMV3ZH3L197AGBP7eKa3iEajooJx5/fggw+KksT0GSQQCEjS62xgFysAIOmSuHM/6aSTRN2iyY07lrKyMjFD/fa3vwXgqKcsakBljebH1tZWUUuozl5wwQV44YUXALhmcNYjnzp1asaU1I7e47aUV69gJjMtGZUtJjdn9ZOCggIxwVx++eUAHNWAqgd3hFRF7N/oSthPo9GoXB93t2Zydu5kaS1oaWkR9YP9iG3a2toqSocfME36dnAYd+8NDQ1yHaaJzi5wQKXzqquukvRLTN7d3NwsQQ1UEamkLlq0SI5jgv/JkyenpGShktm7d++sqCRU7ThOsBCHVxDCHXfcIdfvRVuJ+NkedJdaunSp/Dbh82J+F58rP2GOIbt6jhcvXiwmSo6RZu1yqolUiM877zwJCrn99tsBOK4BHEs5F9G6dcIJJ+xS/e8q4vG4KIF8hmht6tu3r7QVzbLLly+X54MqPO97JBKR92i9+fDDD2XuYuVHWvmY7shPRKPRTo/zXG94jaMcM/ysogLuWoKvgGsR4FqE44KZco/92iz6QFWdbjLTp09P+t50oEqqoiiKoiiK4jv2WEltSxmNx+Pis8HVd2cCN2yVhf5RLS0tsnPjznbatGmyq6utrQXg+o2Ew+GMJ69PJBKeCirgqDynn346ADcVzvTp0yXdB31pTVhyjcphY2Oj+N/SaZ8BIkuXLhXFkD4mpv/Zk08+CcAt/zhixAgpZ2b6NHYFbfULc9e7YMECAK4qNGzYMLz77rsA3NQavN85OTmisLNPRCIRUVCouFdWVgKABNf4Afo7Dh48WHyI2J/ZHoMHD5Y+xh1taWmpqKpU17izLyoq8pWSSryeSar/t9xyi/hj894Drs/q9OnTAbjBkQMHDpQxgQpAbW2tjD/83IoVKwA4/qosK0yloLKyUpQntisVqKKiopSCGemG1hDAVaT4/EcikRSlLhKJSB/h806lbOPGjfI3qmP5+fkppVL5/NfU1KQoqYB7jxgwxXtSU1Ozxylm0oVXij8WaqC6XlBQIL7HbFsveA/i8bj41lFlmj9/fkoSePaho48+WlTvTGL73LalIgcCgZRSnwz0GjVqlAR6sY169eqV5McOIOn5oeWJxVPWrVuH+fPnA3DnFiqOBxxwgFghslka1PQ59Sphy+uhNc1sH6/xyC5JTdqyYHQnOAeyf3DeBFzVmGNnNBpNCeJlMRmTdKURTOsi1c79mZubmyQNtwd+lt+Vm5srpjqa5fjvW2+9VaI1uSh77LHHkiLNADcqLRuDR1u1mz///HMxDzFIqqamRm4w64V73VzmeXz++eclKp0mN17nhg0bZFFrQsd/5nfkoBuPx2XB1tWLVBKLxWQytfvOl19+KbksuVBYsmSJPDCbN28G4C4+m5qaUoJbhg4dKvWoueiz6xX7iblz58pC4oYbbgDgZq34xS9+IQtyTprffPMN7rnnHgCQIEH2+5kzZ/pqIW4GFNrPDKPrzzjjDOmjM2fOBOBMhFywMjiSz0BdXZ0MnFw8DRw4UMzZDCL8wx/+AMAJGmDQECcsM1jTrvw0cODAjLuELFq0SO4rTatsA6+gp2AwKH2a521WHeN4yHYJhULybNmR/19//bVcszleMiCFx/M86urqunSR6rUYWbp0qQRvcEPPjUu/fv3w5z//GQDw0ksvAXACShkoR1chCgDz58+Xyltc3HrBMTkUCmUsg0Z73Bm82LhxI6ZMmQIAkmObwVIDBgwQlxYzbyzHUm70eI/D4TDWrl0LwF2g5eXlyUKGwat8llauXCnuFbwX2aCtexAIBOR5trMImf3JXKza+XPN/LG7C/LsTtibr3A4nFI1Ky8vT8YXHs+NsJkxyA426yxq7lcURVEURVF8xx4rqeZug7sXM10FHWqvu+46AE7gEOsj2yoQkKwAAM7Ong7dVIGef/75lN82zZhUnvhd/Hcm0+9wN/rpp5+K+YQqHl/79u2bUmt9+PDhYqam4kuV0DQ9MIfd7Nmzpd2YjmbcuHEAHPOvHRwRCoVEHWAwgFmBKxvY6T68cl+SYDCYsvOaPXs2AEcp5bVTkTbrD7MdTNODba4dPHiwpDCjSsB29yNbt26V82NVKZp+R40alWJ+2rp1q5jXqB6yP82ePVtSGmXDqrA7vMxqHCd43wYPHiwmZqqrgwYNkutmyiT2cS8aGxslAIl122lNWLZsmaiK/M4ePXqI0sRnk0pmNqioqEgJWOH5e6kSX3zxBc4///yk9zh+eh3vVXGO40aPHj2SninC6lpU4ojXsdnESzF76aWXMHHiRADeFY/oEjV16lQAwD333CNVpZ566ikA7hgya9aslCAxLxc3uhz16tVLvivdmL/JvLiffvopANeVo6SkREzvrCA1aNAguedUlKm8V1ZWpvSRpqYm6T+0zPHzAwYMEMWQrjatra3yvNCdjXPtokWLMu5i1154DbFYLCUVJMnJyWnzfG3VNC8vT1Tj7qKkellqGaBtuzKZ12TnlgXcMZxtkInxwB+9R1EURVEURVEMOq2kcufd0tIiq2/u5ribKioqEidrrr4/++wzUVJt/wfAVQCoYBxzzDH46U9/CsD1I/OC9ZIBdzVvq0yZTD9FZaqwsBAffPABALc9+LtnnXWWqGKsKFNVVSU+tkzKz1Q4dGYGkncqVEAZaMXE3YsXLxaVjTuc/Px8uQdUm3letbW1WamtbKsObfmoJBIJ8RF8//33kz5fW1sr7WcGA7H/MQUV1eoxY8ZI0n8qI/X19aK+Ua1jn2tpaemwD3WmmTJlCm666SYAbvoy9oUzzzwz5fiLL75Y1EKmKuM1jRkzxnfVcMhtt90GAPLsUI1atWqV3Guee25urjwDvL+0NLCNTMrLy/HGG28AcK0PfP6Ki4vlWWHQRygUkmeEVhAqT9lQherr66VPUrnxqjTF4J7CwkLp03z+vZRUXpNXgRSzAp5XbXN+P4ONbD81P8BzGj58eJvPMe8la4sDkGeM/uzsR/369UuxTnmpt7w/9OXMJFdddRX+8pe/AHDHMM6lkUhExnumYBw2bJjce1pZeJ4DBgyQ+001rbW1VeZOfi/H2HA4LEFR7H95eXni682k7qafsl+KG7B/xOPxdvlJeimG/BzbE+h6a0JH8VJSGZPCvmCm6rMrA5oB8ewX/M76+vq0+6irkqooiqIoiqL4jk5vcbijMH0YuGOiP9X69etT/HmuvfZaTJgwYZffy50e01tccMEFbSqohD4wpj+Q7UORyShU7tzN5Pi8Fr7uv//+opqeeOKJAJzoYe7wTD9SwIlEp08I2/mSSy5pUyVgJCV3Orm5ubLT4+e469myZYtn2qt0w+vjbpT/rqurE9WTynlNTY2oA1TOPvzwQwBO1CnTKzGF0JYtW+SaqWazrei3Bbh9sry8PCWhO30eGxoafKek1tbWSsQsM0Lw+swk62THjh2ijrON2BfSVaYu3Xz00UfiW8f+yGemrq5O0ssxLUoikRBFj1HVzO4xYsQI8TWcNGkSACfjB33x6DtIhchMz8PnYr/99pPUbHaN+2yUlV22bFlSNDXgFmsw4dgwZMgQOS/7WTNVLDvy38SM1jVTYNlwbGKEd7ZrsxNTDaJ6x/O++OKLU9TgttLhjBs3TtTVRx55BIDrAwwgpX94fQcV2EyWyuU1vfrqq+Iryoh8jgXl5eXyvNAyUFlZKfOOrfpFo1G5p2ZWHlqs+MzRmkE/d8C7tCjHavqoAqllmrsKMxsBn39es+mvaqqkQLL1xE7lBfizrHZH2LFjR0pcBq11+fn5KQUgYrFYyvjCdqyqqkq69+mg04vUl19+GYDjSE9TEHOvccBMJBJyYewURxxxhGeKE8B5qGjqZrop5vYEUgOtvBzYCwsL5YFkRzJrzWYTM3dltuhIB8mG6Xft2rWSJogmJqZ8+vbbb2Ww5WKjtLRUcjC+/fbbANyKUGVlZRIwxcmgrKxMTJx0+eC/t23bJq4WTDcVCoVkUOHExP6xadMm31XSCYVCsrhkYBihec4+nhulH/zgBwDclG3Z2JB0BG5MrrnmGpnIzFyefOX94TGNjY3SJ3i/+Lft27dLoOX1118PwMmFOmPGDABu8CAH2YaGBlm4mimr7PQpS5YsSfp8JvFKM+UVlMFzMze6HGvMPIfETDvFcZm/ZS7GvRaxhJVpOOF7LZ7TTSKRSLkfXkEf5557rry3aNEiAG4+aq+F5b333gvAmXxvvfVWAMmLU+KVZ9OrehfgjGmZgm5v0WhUNt08J46LLS0tcm+4+Y5EIrLI5OaC59/Y2Chty/k1kUhIX+FzwjHz0EMPxVFHHQXAbRevtuXvlZeXiwtPV48/Ztop223H3MDZ1+OVlso0//NZsp+p7kJzc7OMsRRA+G9z7cR26NGjR5Lp3/wb5950ouZ+RVEURVEUxXd0WkmlYldWVpaU+BhwFZIhQ4Z4ysjc3bImMgM8SktLMX78eADAww8/LJ/hat0r0Mqmuro6JSCGAUOZDJxSvPn4449FbWAybAZ6bdy4UcyqVDDy8vLEZMSdPftaU1OTKEo07W3ZskXUC7p6UGX76quv5Di+FwgE5Dts1aSystKz2k5X0rt3b0nebwZ4Ae41mZSWlopSzQBGtn8mVZ7OwOs59dRTZZygiwKV73A4LOfN/lBWVibH0ypD8/+6detwxx13AHCDYWbOnCkqKRV4KrbxeFwUHqopDQ0NolRRlWI/yoZVhCmSdgeVodra2pQk+3ZaLSBZ9eO12oUzYrFYm0oqx+xssrvk9QyY/P73vw8gWd2i4sMUWk8//TQeeughAJBUdBMnTmzXc99Wgni2cyarkZ166qkAnHHNLu5At6mysjJRtUyTNvsK+z/nVNM9gepnMBhMSc3EMaehoUHmd1riwuGw9CP+Nu9XQUFBm+4j2YDnZBYbspXwttIi7q44Az9Ltbm7Kanbt2+X+8drtV3izPcSiUSSO6EJ+6H5XXuKKqmKoiiKoiiK7+i0kspgDib0NaH6UFVVJSoI0ylVV1fLLpCr7ptvvhmA43PjFdy0q7QvXiv1t99+W3Z9dOrnLpK+s0rmYTBTQUGB3O8nnngCgLsja2pqkl0o1U2zrCUDgKiWrVq1Svxb6fsSjUbls1RP2F969OiRsotnQA6QqiJ5+aN1NaYywn7NHbuXZaGoqEj+zjbiNXdVkMuuoIpz7rnnih8yU0pxbKivr5f0NhxXtm/fLtfEvsR7PmXKlKT0VUBykQuzJjvgqCr0Z6XfaW5urvjRHXvssQBcn9eWlpYuDQQxE5FzXKupqZH2YiokPkNmuikvP0oex2doV4GDu/LBzCS836tXr5Y2Ny0igHMfqegxdV08Hhfr2V133QXAVdpfeeUVCbI755xzALgp/zqCPSdRQc1kYB0DASdNmoR33nkHAPDoo48CcAMC2QYmubm5KYqX6WfbnsAfM3iM/q/0eTV9FO3URJs3b5aUk12Fbc0tLCyU8yRmv7ZLpbaVdi4QCMjx9nd2F7yUVDP+h9dvWl94nJ2Sy+x/6VJSM5LAjA/qIYccInkJaarINF39QCgOjNY2K/Ww43PxVFtbK5MHg6pqa2tlocKAHzMbARdm5qDLv9s12BkYBbgPUZ8+fWQC4+d4XLYqcHWE4uLipKhbIDknoo05YZCOuMtkE57PsGHDJHKZ92b06NEAnHvCCGYz0JKLTdOsyPfZD3h8jx49UqJQ2Rd79eol/YAZAHr16iV5A/nb3DD5KfsDF/mNjY0pi28SCARSFpjme3wuuOBtbW31XIjuanFqLprTDV0+XnvtNVmk8jo5NhQUFMgihAu2iooKiUJnYCYD7BYvXowHHngAANJaGYpizJw5c3DRRRel7Xt3BV15+Eqam5slaIxCwfr16+X5ss3bLS0tEvDMTXpxcbHM4dy88PkJBAIy7vA7Vq9eLf/Pz9FdprCwUDLZdBX2Itxr0WkGQtlZC8wqVF6uMPbCzu/Y17du3bqULAektbU1Kb8y4PQFvme3h1cw756i5n5FURRFURTFd/ijFISy10E1YfXq1WKaoomWO/GWlhZRPbkjy8nJEXWVcBdfUlIi/8/dsVkhx05R1q9fP1HYuPMrLi4WtY67Rv7773//O4477jgA/qhtDySbpqgetUVRUVFKWhC+si38gpm313bUN9NO2ffCDGyyVeOePXvK8dz55+XliZmYyhD7WDweT6p7DjjmQfZR5ppk/8xUXfb2YqqWvJb8/HxRQtlubFszpRTfMyurmQFTPL6z55NumO/z7rvvzthvdBa73915551ddCbJFBQUSAo2vmaaU045JSu/01k4fprqsRk4Cbj9OBqNeiqpxO7v8Xi8TZW1O7B9+/aU/K+mOd++5pycHBmvbZcI5vI2v2NPUSVVURRFURRF8R2qpCoZZfjw4ZIA3fQtBZx0Q0xnwh1YKBRKcfLnrq2goEDUQKqfgwYNSkr8DiTXoKbaZNabpu8pVSRTqfOb2lhSUiLKMHf/dr1kE6+a2dzR+u3aiBnQSAWTqnFDQ4P4HJqFQewUKKZ/Lq+T7VNYWJiSOor3vrW1NSUtz9y5c1N88djmmUwxlA54nVRUTSWe/SgYDKaoP1RLIpGIqNQkk36nipJp2P/NPm/PMcRrTDUVQa+KU93NJ9Vm1qxZMsbSd9uOgzDfSyQSKanGOC7uql33BFVSFUVRFEVRFN+hSqqSUcw6v9xxMlKar5n4TWIrRuFwWFRVc2fI88tGbfaOQlWR7UfV0Ct1TCQSkfe5s+erX2pot4WtipsZGrJFZ1ISdSXBYFBSZjGbCtMEmTXszRKmfJ+qKRVYfk5R9hbs4gZmqU97DojH46KSmv6WnEdopTHLo9plZf2OrRbfeOONUlSH6f7aGwfBcYPtuGDBgjSeqYMuUpWM0hVmQq/fpBmid+/evlyItgUX83Zde9ssCzjpnGiy4aBM14hMbQqUruXqq6/Giy++CMBdZJpBVdzUcGFaV1eXlCIIcIMajzzySMm1qih7A/bi1HR7otsPj4nH47LgMsUVW+wwF6R2rmG/Y5vkzzzzTJx55pkAINUN582bB8Bxj2PQKYMqA4GAtBs3vhwzWE00nai5X1EURVEURfEdOV6OwoqiKIqiKIrSlaiSqiiKoiiKovgOXaQqiqIoiqIovkMXqYqiKIqiKIrv0EWqoiiKoiiK4jt0kaooiqIoiqL4Dl2kKoqiKIqiKL7j/wF1GDBf6+qSqwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x345.6 with 40 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_rows = 4\n",
    "n_cols = 10\n",
    "plt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))\n",
    "for row in range(n_rows):\n",
    "    for col in range(n_cols):\n",
    "        index = n_cols * row + col\n",
    "        plt.subplot(n_rows, n_cols, index + 1)\n",
    "        plt.imshow(X_train[index], cmap=\"binary\", interpolation=\"nearest\")\n",
    "        plt.axis('off')\n",
    "        plt.title(class_names[y_train[index]], fontsize=12)\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0], cmap=\"binary\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creación de un modelo Sequential. Es el tipo más sencillo de modelo de Keras para redes neuronales que consta solo de una única pila de capas conectadas secuencialmente. Es lo que se llama API secuencial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A continuación, construimos la primera capa y añadimos al modelo. Es una capa Flatten cuya función es convertir cada entrada de imagen en una matriz 1D: si recive datos de la entrada X, computa X.reshape(-1,1). Esta capa no tiene parámetros, está ahí solo para el preprocesamiento. Dado que se trata de la primera capa del modelo, deberías especificar input_shape, que no incluye el tamaño del lote, solo la forma de las instancias. Como alternativa, puedes añadir una capa keras.layers.InputLayer como primera cpa, configurando input_shape=[28,28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Flatten(input_shape=[28,28]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Desués añadimos una capa Dense oculta con 300 neuronas, que utilizará la función de activación ReLU. Cada capa Dense administra su propia matriz de pesos, que contiene todos los pesos de conexión entre las neuronas y sus salidas. También gestiona un vector de términos de sesgo (uno por neurona)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(300, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Luego añadimos una segunda capa Dense oculta con 100 neuronas, también con la función de activación ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(100, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Por último, añadimos una capa de salida Dense con 10 neuronas (una por clase), con la función de activación softmax(porque las clases son exclusivas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- En lugar de añadir las capas de una en una podemos pasar una lista de capas al crear el modelo Sequential:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.Sequential([\n",
    "#     keras.layers.Flatten(input_shape=[28,28]),\n",
    "#     keras.layers.Dense(300, activation=\"relu\"),\n",
    "#     keras.layers.Dense(100, activation=\"relu\"),\n",
    "#     keras.layers.Dense(10, activation=\"softmax\")\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- El método summary muestra un resumen de las capas y los parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Se puede conseguir una lista de capas del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.core.Flatten at 0x228117dfbe0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x22852b09ac8>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x22852b25cc0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x22852c5ee48>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = model.layers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer('dense') is hidden1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Se puede acceder a todos los parámetros de una capa utilizando sus métodos get_wights() y set_weightd().\n",
    "- Para una capa Dense, esto incluye tanto los pesos de conexión como los términos de sesgo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight, biases = hidden1.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.05202346,  0.04776664,  0.05505526, ..., -0.05502917,\n",
       "         0.02929763,  0.05636753],\n",
       "       [-0.0395397 , -0.00291999, -0.02307609, ..., -0.01734661,\n",
       "         0.02243754, -0.06655454],\n",
       "       [-0.06023254, -0.0474097 , -0.0654242 , ..., -0.00779164,\n",
       "        -0.0031507 , -0.06257842],\n",
       "       ...,\n",
       "       [ 0.04901219, -0.06292232,  0.06910484, ..., -0.03629965,\n",
       "        -0.05887005, -0.03881078],\n",
       "       [ 0.0322689 ,  0.04961727, -0.05702887, ..., -0.06480993,\n",
       "         0.04597393,  0.01671734],\n",
       "       [ 0.00805768,  0.02907991,  0.06341749, ..., -0.03094993,\n",
       "         0.06393726, -0.05329397]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La forma de la matriz de pesos depende del número de entradas, por ello es recomendable especificar input_shpae para crear la pirmera capa de un modelo sequential, aunque sino se especifica Keras esperará hasta saber la forma de la entrada antes de crear el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilación del modelo\n",
    "- Después de crear el modelo hay que compilarlo con compile() para especificar la función de pérdida y el optimizador que se utilizará. De forma opcional, se puede especificar una lista de métricas adicionales que computar durante el entrenamiento y la evaluación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Usamos la pérdida sparse_categorical_crossentropy porque tenemos etiquetas en las que para cada instancia hay un solo índice de clase objetivo, de 0 a 9 en este caso y las clases son exclusivas, lo que denominamos etiquetas dispersas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento y evaluación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/30\n",
      "55000/55000 [==============================] - 3s 60us/sample - loss: 0.7338 - accuracy: 0.7573 - val_loss: 0.4996 - val_accuracy: 0.8352\n",
      "Epoch 2/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.4915 - accuracy: 0.8288 - val_loss: 0.4671 - val_accuracy: 0.8330\n",
      "Epoch 3/30\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.4443 - accuracy: 0.8443 - val_loss: 0.4398 - val_accuracy: 0.8516\n",
      "Epoch 4/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.4171 - accuracy: 0.8539 - val_loss: 0.3968 - val_accuracy: 0.8646\n",
      "Epoch 5/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.3950 - accuracy: 0.8602 - val_loss: 0.3789 - val_accuracy: 0.8706\n",
      "Epoch 6/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.3792 - accuracy: 0.8667 - val_loss: 0.3833 - val_accuracy: 0.8654\n",
      "Epoch 7/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.3652 - accuracy: 0.8703 - val_loss: 0.3740 - val_accuracy: 0.8662\n",
      "Epoch 8/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.3536 - accuracy: 0.8757 - val_loss: 0.3726 - val_accuracy: 0.8686\n",
      "Epoch 9/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.3428 - accuracy: 0.8789 - val_loss: 0.3399 - val_accuracy: 0.8790\n",
      "Epoch 10/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.3327 - accuracy: 0.8814 - val_loss: 0.3425 - val_accuracy: 0.8776\n",
      "Epoch 11/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.3245 - accuracy: 0.8841 - val_loss: 0.3401 - val_accuracy: 0.8812\n",
      "Epoch 12/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.3175 - accuracy: 0.8874 - val_loss: 0.3321 - val_accuracy: 0.8836\n",
      "Epoch 13/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.3096 - accuracy: 0.8886 - val_loss: 0.3293 - val_accuracy: 0.8800\n",
      "Epoch 14/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.3031 - accuracy: 0.8902 - val_loss: 0.3504 - val_accuracy: 0.8706\n",
      "Epoch 15/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.2962 - accuracy: 0.8937 - val_loss: 0.3296 - val_accuracy: 0.8834\n",
      "Epoch 16/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.2900 - accuracy: 0.8948 - val_loss: 0.3188 - val_accuracy: 0.8840\n",
      "Epoch 17/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.2840 - accuracy: 0.8972 - val_loss: 0.3134 - val_accuracy: 0.8868\n",
      "Epoch 18/30\n",
      "55000/55000 [==============================] - 3s 49us/sample - loss: 0.2782 - accuracy: 0.8990 - val_loss: 0.3195 - val_accuracy: 0.8862\n",
      "Epoch 19/30\n",
      "55000/55000 [==============================] - 3s 49us/sample - loss: 0.2740 - accuracy: 0.9016 - val_loss: 0.3025 - val_accuracy: 0.8926\n",
      "Epoch 20/30\n",
      "55000/55000 [==============================] - 3s 49us/sample - loss: 0.2691 - accuracy: 0.9022 - val_loss: 0.3195 - val_accuracy: 0.8852\n",
      "Epoch 21/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.2638 - accuracy: 0.9043 - val_loss: 0.3080 - val_accuracy: 0.8898\n",
      "Epoch 22/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.2592 - accuracy: 0.9068 - val_loss: 0.3160 - val_accuracy: 0.8852\n",
      "Epoch 23/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.2546 - accuracy: 0.9081 - val_loss: 0.3124 - val_accuracy: 0.8874\n",
      "Epoch 24/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.2500 - accuracy: 0.9097 - val_loss: 0.3652 - val_accuracy: 0.8654\n",
      "Epoch 25/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.2455 - accuracy: 0.9119 - val_loss: 0.3010 - val_accuracy: 0.8920\n",
      "Epoch 26/30\n",
      "55000/55000 [==============================] - 3s 49us/sample - loss: 0.2416 - accuracy: 0.9127 - val_loss: 0.3203 - val_accuracy: 0.8852\n",
      "Epoch 27/30\n",
      "55000/55000 [==============================] - 3s 49us/sample - loss: 0.2380 - accuracy: 0.9136 - val_loss: 0.3233 - val_accuracy: 0.8846\n",
      "Epoch 28/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.2347 - accuracy: 0.9155 - val_loss: 0.3074 - val_accuracy: 0.8864\n",
      "Epoch 29/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.2299 - accuracy: 0.9177 - val_loss: 0.2994 - val_accuracy: 0.8952\n",
      "Epoch 30/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.2266 - accuracy: 0.9173 - val_loss: 0.3325 - val_accuracy: 0.8800\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pasamos las características de entrada (X_train) y las clases objetivo (y_train), además del número de repeticiones para entrenar (si no lo hacemos, se establecería por defecto en 1, lo cual no sería lo bastante bueno para converger una buena solución).\n",
    "- También pasamos un conjunto de validación.\n",
    "- Keras medirá la pérdida y las métricas adicionales de este conjunto al final de cada repetición, lo cual es muy útil para ver si el modelo está rindiendo bien. Si el rendimiento del conjunto de entrenamiento es mucho mejor que el del conjunto de validación, es muy probable que el modelo esté sobreajustando el conjunto de entrenamiento.\n",
    "- En cada repetición durante el entrenamiento, keras muestra el número de instancias  procesadas hasta el momento, el tiempo de entrenamiento medio por muestra y la pérdida y la exactitud (o cualquier otra métrica solicitada) para el conjunto de entrenamiento y para el de la validación. Veremos que la pérdida de entrenamiento ha bajado, lo cual es buena señal, y la exactitud de validación ha llegado al 88.18% tras 30 repeticiones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Para obtener la curva de aprendizaje podemos crear un Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid\n",
    "plt.gca().set_ylim(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cuando estemos satisfechos con la exactitud de validación del modelo, debemos evaluarlo con el conjutno de prueba para estimar el error de generalización antes de implementar el modelo en producción. Puedes hacerlo fácilmente con el método evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 43us/sample - loss: 66.9625 - accuracy: 0.8436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[66.96252846283913, 0.8436]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Es habitual obtener un rendimiento logeramente inferior con el conjunto de pruebas que con el de validación, ya que los hiperparams están ajustados con el conjunto de validación.\n",
    "- No hay que ajustar los hiperparams con el de prueba, porque al hacerlo la estimación del error de generalización será demasiado optimista."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso del modelo para hacer predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Para cada instancia, el modelo calcula una probabilidad por clase, desde la clase 0 hasta la 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Como nos sale que pueden ser varias opciones, como alternativa podemos utilizar predict_classes para obtener la más elevada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(class_names)[y_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- El clasificador ha clasificado correctamente las trés imágenes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new = y_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1], dtype=uint8)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PMC de regresión con la API secuencial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vamos a cambiar al problema de la vivienda en California y usaremos redes neuronales de regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = scaler.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Utilizar la API secuencial para construir, entrenar,evaluar y usar una PMC de regresión para hacer predicciones es bastante similar a lo que hemos hecho para clasificación.\n",
    "- Las diferencias principales son que la capa de salida tiene una sola neurona (porque solo queremos predecir un valor), no usa función de activación y la función de pérdida es el error cuadrático medio.\n",
    "- Puesto que el conjunto de datos tiene bastante ruido, vamos a usar solo una capa oculta con menos neuronas que antes para evitar el sobreajuste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu', input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.8805 - val_loss: 0.8859\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.6654 - val_loss: 0.4789\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 1s 49us/sample - loss: 0.5087 - val_loss: 0.4667\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4670 - val_loss: 0.4528\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 1s 49us/sample - loss: 0.4528 - val_loss: 0.4418\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4528 - val_loss: 0.4385\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4377 - val_loss: 0.4274\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 1s 49us/sample - loss: 0.4281 - val_loss: 0.4327\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4233 - val_loss: 0.4142\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.4181 - val_loss: 0.4078\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4127 - val_loss: 0.4042\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4197 - val_loss: 0.4048\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.4126 - val_loss: 0.4009\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 1s 49us/sample - loss: 0.4039 - val_loss: 0.3981\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 1s 49us/sample - loss: 0.4020 - val_loss: 0.3964\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4076 - val_loss: 0.4015\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3972 - val_loss: 0.3890\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3926 - val_loss: 0.3924\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 1s 49us/sample - loss: 0.5037 - val_loss: 0.3996\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 1s 49us/sample - loss: 0.4033 - val_loss: 0.3918\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mean_squared_error\", optimizer='sgd')\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160 [==============================] - 0s 27us/sample - loss: 0.3916\n"
     ]
    }
   ],
   "source": [
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.5362494],\n",
       "       [3.0633569],\n",
       "       [5.4332366]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- El API sequential es bastante sencilla pero a veces hay que construir redes neuronales con tipologías complejas o con múltiples entradas o salidas, para ello keras ofrece el API funcional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de modelos complejos con la API funcional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Un ejemplo de red neuronal no secuencial, es la red neuronal \"ancha y profunda\".\n",
    "- Se introdujo en 2016 en un trabajo de Heng-Tze Cheng.\n",
    "- Conecta todas o parte de las entradas directamente a la capa de salida.\n",
    "- Esta arquitectura posibilita que la red neuronal aprenda dos patrones profundos (usando la ruta profunda) y las reglas sencillas (a través de la ruta corta)\n",
    "    - PMC Normal: obliga a todos los daos a fluir por toda la pila de capas, de modo que patrones sencillos de datos pueden acabar distorsionados por esta secuencia de transformaciones.\n",
    "    - PMC de ruta ancha y profunda permite que la red neuronal aprenda dos patrones profundos y reglas sencillas\n",
    "- Construyamos una red neuronal así para el problema de la vivienda de California:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/pmc_1.png\" width=\"30%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se trata de una especificación del tipo de entrada que tendrá el modelo,\n",
    "# incluidos su shape y dtype.\n",
    "input_ = keras.layers.Input(shape=X_train.shape[1:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una capa Dense con 30 neuronas, con la función de activación ReLU\n",
    "# En cuanto estñe creada, la llamamos como funciòn, pasándole la entrada, de ahi el nombre API funcional\n",
    "# Solo le decimos a Keras como debería conectar las capas, todavía no se están procesando datos\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una segunda capa oculta y volvemos a usarla como una función. \n",
    "# Le pasamos la salida de la primera capa oculta\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una capa Concatenate y la usamos de inmediato como un función,\n",
    "# para concatenar la entrada y la salida de la segunda capa oculta.\n",
    "concat = keras.layers.Concatenate()([input_,hidden2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos la capa de salida\n",
    "output = keras.layers.Dense(1)(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un model de keras, especificando las entradas y salidas para usar\n",
    "model = keras.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 1s 72us/sample - loss: 2.6372 - val_loss: 0.9529\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 5.6902 - val_loss: 1.1728\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 5.7019 - val_loss: 1.4285\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 1.2512 - val_loss: 2.8884\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 8.9670 - val_loss: 49.7174\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 491.8866 - val_loss: 53.6292\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 511.5148 - val_loss: 70.6615\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 843.5039 - val_loss: 97.4606\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 732.3794 - val_loss: 132.5866\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 81.6165 - val_loss: 186.8371\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 156.1104 - val_loss: 261.3202\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 213.5601 - val_loss: 349.9347\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 273.6788 - val_loss: 452.8335\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 271.3900 - val_loss: 586.3035\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 5378.7813 - val_loss: 941.4048\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 7037.9428 - val_loss: 1248.7403\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 508.3824 - val_loss: 1387.8611\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 10249.0460 - val_loss: 1912.2285\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 987.5781 - val_loss: 2724.0531\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 31809.2085 - val_loss: 3569.9621\n"
     ]
    }
   ],
   "source": [
    "# Compilamos el modelo\n",
    "model.compile(loss=\"mean_squared_error\", optimizer='sgd')\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.057346 ],\n",
       "       [4.27587  ],\n",
       "       [1.8328173]], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160 [==============================] - 0s 28us/sample - loss: 69.7860\n"
     ]
    }
   ],
   "source": [
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enviar conjunto de características por ruta ancha y ruta profunda\n",
    "- Suponiendo que queremos enviar un conjunto de características por la ruta ancha y otro por ruta profunda. En este caso queremos enviar cinco características por ruta ancha (de 0 a 4) y seis por ruta profunda (de 2 a 7):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/pmc_2.png\" width=\"30%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name='wide_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_B = keras.layers.Input(shape=[6], name='deep_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat = keras.layers.concatenate([input_A, hidden2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = keras.layers.Dense(1, name='output')(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer=keras.optimizers.SGD(lr=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 1s 74us/sample - loss: 1.7988 - val_loss: 0.9191\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.7835 - val_loss: 0.6891\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.6563 - val_loss: 0.6171\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.6053 - val_loss: 0.5766\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.5727 - val_loss: 0.5495\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.5491 - val_loss: 0.5275\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.5312 - val_loss: 0.5113\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.5179 - val_loss: 0.4995\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.5075 - val_loss: 0.4909\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.5003 - val_loss: 0.4867\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 1s 61us/sample - loss: 0.4945 - val_loss: 0.4820\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.4897 - val_loss: 0.4756\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.4853 - val_loss: 0.4735\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.4820 - val_loss: 0.4692\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.4789 - val_loss: 0.4668\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.4756 - val_loss: 0.4681\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.4735 - val_loss: 0.4639\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.4705 - val_loss: 0.4607\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.4688 - val_loss: 0.4584\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.4665 - val_loss: 0.4576\n"
     ]
    }
   ],
   "source": [
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20,\n",
    "                    validation_data=((X_valid_A, X_valid_B), y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160 [==============================] - 0s 31us/sample - loss: 0.4516\n"
     ]
    }
   ],
   "source": [
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hay varios casos en los que interesa tener múltiples salidas:\n",
    "    - La tarea puede requerirlo. Por ejemplo, puede que quereamos clasificar el objeto principal de una foto. Esto es una tarea de regresión (encontrar las coordenadas del centro del objeto además de su anchura y altura) y también de clasificación.\n",
    "- Del mismo modo, puede que tengamos múltiples tareas independientes basadas en los mismos datos. Desde luego, podríamos entrenar una red neuronal por tarea, pero un muchos casos conseguiremos mejores resultados entrenando una sola red neuronal con una salida por tarea. Esto se debe a que la red neuronal aprende características de los datos que son útiles entre tareas. Por ejemplo, podríamos hacer clasificación multitarea con fotos de caras, usando una salida  para clasificar la expresión facial de la persona (sonrisa, sorpresa, etc) y otra para identificar si lleva gafas o no.\n",
    "- Otro caso de uso es como técnica de regularización (es decir, una limitación de entrenamiento cuyo objetivo es reducir el sobreajuste y mejorar así la capacidad del modelo para generalizar). Por ejemplo, puede que nos interese añadir alguna salida auxiliar a la arquitectura de red neuronal para garantizar que la parte subyacente de la red aprende algo útil por su cuenta, sin depender del resto de la red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/pmc_3.png\" width=\"30%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Añadir salidas adicionales es bastante facil: solo hay que conectarlas a las capas apropiadas y añadirlas a la lista de salidas del modelo. Por ejemplo, basada en la imagen anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = keras.layers.Dense(1, name='main_output')(concat)\n",
    "aux_output = keras.layers.Dense(1, name='aux_output')(hidden2)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cada salida necesitará su función de pérdida. Así pues, al compilar el modelo, deberíamos pasaruna lista de pérdidas (si pasamos una sola pérdida, Keras asumirá que debe usar la misma pérdida para todas las salidas). Por defecto, Keras asumirá todas las pérdidas y simplemente las sumará para obtener la pérdida definitiva utilizada para el entrenamiento.\n",
    "- Nos preocupamos mucho más por la salida principal que por la auxiliar ( ya que esta solo la usamos para la regularización), así que vamos a dar a la pérdida de la salida principal mucho más peso. Por suerte, es posible configurar todos los pesos de pérdida al compilar el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=['mse','mse'], loss_weights=[0.9,0.1],optimizer='sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cuando entrenemos el modelo, tenemos que suministrar etiquetas para cada salida. En este ejemplo, la salida principal y la auxiliar deberían intentar predecir lo mismo, así que usaremos las mismas etiqueta. Entonces, en vez de pasar y_train, tenemos que pasar (y_train, y_train) (y lo mismo se aplica a y_valid e y_test):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 1s 107us/sample - loss: 0.7855 - main_output_loss: 0.6789 - aux_output_loss: 1.7432 - val_loss: 0.5878 - val_main_output_loss: 0.5391 - val_aux_output_loss: 1.0250\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 1s 74us/sample - loss: 0.6709 - main_output_loss: 0.6384 - aux_output_loss: 0.9632 - val_loss: 0.5198 - val_main_output_loss: 0.4737 - val_aux_output_loss: 0.9340\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 1s 77us/sample - loss: 0.5080 - main_output_loss: 0.4662 - aux_output_loss: 0.8826 - val_loss: 0.4944 - val_main_output_loss: 0.4543 - val_aux_output_loss: 0.8553\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 1s 76us/sample - loss: 0.4865 - main_output_loss: 0.4505 - aux_output_loss: 0.8102 - val_loss: 0.4899 - val_main_output_loss: 0.4559 - val_aux_output_loss: 0.7957\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 1s 77us/sample - loss: 0.4755 - main_output_loss: 0.4450 - aux_output_loss: 0.7488 - val_loss: 0.4683 - val_main_output_loss: 0.4367 - val_aux_output_loss: 0.7520\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 1s 74us/sample - loss: 0.4564 - main_output_loss: 0.4291 - aux_output_loss: 0.7026 - val_loss: 0.4589 - val_main_output_loss: 0.4311 - val_aux_output_loss: 0.7090\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 1s 73us/sample - loss: 0.4467 - main_output_loss: 0.4215 - aux_output_loss: 0.6733 - val_loss: 0.4482 - val_main_output_loss: 0.4228 - val_aux_output_loss: 0.6764\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 1s 74us/sample - loss: 0.4393 - main_output_loss: 0.4162 - aux_output_loss: 0.6513 - val_loss: 0.4575 - val_main_output_loss: 0.4333 - val_aux_output_loss: 0.6743\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 1s 73us/sample - loss: 0.4323 - main_output_loss: 0.4102 - aux_output_loss: 0.6308 - val_loss: 0.4474 - val_main_output_loss: 0.4253 - val_aux_output_loss: 0.6459\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 1s 73us/sample - loss: 0.4232 - main_output_loss: 0.4016 - aux_output_loss: 0.6176 - val_loss: 0.4215 - val_main_output_loss: 0.3985 - val_aux_output_loss: 0.6276\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 1s 73us/sample - loss: 0.4221 - main_output_loss: 0.4014 - aux_output_loss: 0.6093 - val_loss: 0.4269 - val_main_output_loss: 0.4054 - val_aux_output_loss: 0.6204\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 1s 73us/sample - loss: 0.4232 - main_output_loss: 0.4044 - aux_output_loss: 0.5928 - val_loss: 0.4250 - val_main_output_loss: 0.4045 - val_aux_output_loss: 0.6092\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 1s 74us/sample - loss: 0.4056 - main_output_loss: 0.3867 - aux_output_loss: 0.5745 - val_loss: 0.4046 - val_main_output_loss: 0.3838 - val_aux_output_loss: 0.5912\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 1s 76us/sample - loss: 0.3994 - main_output_loss: 0.3811 - aux_output_loss: 0.5642 - val_loss: 0.3982 - val_main_output_loss: 0.3777 - val_aux_output_loss: 0.5818\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 1s 73us/sample - loss: 0.3967 - main_output_loss: 0.3791 - aux_output_loss: 0.5545 - val_loss: 0.4024 - val_main_output_loss: 0.3827 - val_aux_output_loss: 0.5799\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 1s 77us/sample - loss: 0.3907 - main_output_loss: 0.3734 - aux_output_loss: 0.5467 - val_loss: 0.4099 - val_main_output_loss: 0.3919 - val_aux_output_loss: 0.5721\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 1s 77us/sample - loss: 0.3949 - main_output_loss: 0.3777 - aux_output_loss: 0.5511 - val_loss: 0.4180 - val_main_output_loss: 0.4012 - val_aux_output_loss: 0.5687\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 1s 77us/sample - loss: 0.3883 - main_output_loss: 0.3717 - aux_output_loss: 0.5371 - val_loss: 0.3815 - val_main_output_loss: 0.3626 - val_aux_output_loss: 0.5515\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 1s 78us/sample - loss: 0.3869 - main_output_loss: 0.3714 - aux_output_loss: 0.5271 - val_loss: 0.3753 - val_main_output_loss: 0.3569 - val_aux_output_loss: 0.5411\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 1s 79us/sample - loss: 0.3818 - main_output_loss: 0.3664 - aux_output_loss: 0.5200 - val_loss: 0.3797 - val_main_output_loss: 0.3618 - val_aux_output_loss: 0.5408\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20,\n",
    "                    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Al evaluar el modelo, Keras devolverá la pérdida total, además de las pérdidas individuales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160 [==============================] - 0s 43us/sample - loss: 0.3721 - main_output_loss: 0.3565 - aux_output_loss: 0.5119\n"
     ]
    }
   ],
   "source": [
    "total_loss, main_loss, aux_loss = model.evaluate(\n",
    "    [X_test_A, X_test_B], [y_test, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Del mismo modo, el método predict() devolverá predicciones para cada salida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilización de la API de subclasificación para crear modelos dinámicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Las API secuencial y funcional son declarativas\n",
    "- El API de subclasificacion es buena para los casos en el que los modelos impliquen búcles, formas cambiantes, ramificación condicional y otros comportamientos dinámicos.\n",
    "    - También si preferimos un modelo de programación más imperativo.\n",
    "        - Subclasificamos la clase Model\n",
    "        - Creamos las capas en el constructor\n",
    "        - Usamos las capas para realizar cálculos que queramos en el método call()\n",
    "        \n",
    "- Ejemplo: \n",
    "    - Crear una instancia de la siguiente clase WideAndDeepModel da un modelo equivalente al que acavamos de crear con la API Funcional\n",
    "        - Podemos compilarlo, evaluarlo y usarlo para hacer predicciones como hicimos anteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.models.Model):\n",
    "    # Creamos las capas en el constructor\n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "    \n",
    "    # Usamos las capas en el constructor\n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "\n",
    "model = WideAndDeepModel(30, activation=\"relu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardar y restaurar un modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Para guardar un modelo se realiza así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Keras utiliza el formato HDF5 para guardar la arquitectura del modelo(incluidos los hiperparams de tosas lass capas) y los valores de todos los params del modelo para cada capa (por ejemplo, pesos de conexión y sesgos).\n",
    "- También guarda el optimizador (incluidos sus hiperparams) y cualquier estado que tenga)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Para Cargar el modelo se realiza así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Si el entrenamiento durara varias horas se debería guardar el modelo al final del entrenamiento y también los puntos de control en intervalos regulares durante el entrenamiento para evitar perder todo si el ordenador falla.\n",
    "- La pregunta a: ¿cómo puedo decir al método fit() que guarde puntos de control? => usando retrollamadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilización de retrollamadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- El método fit() acepta un argumento callbacks que permite especificar una lista de objetos a los que keras llamará al principio y al final del entrenamiento, al principio  y al final de cada repetición, incluso antes y después de procesar cada lote.\n",
    "- Por ejemplo, la retrollamada ModelCheckpoint guarda puntos de control del modelo en interalos regulares durante el entrenamiento, por defecto al final de cada repeteción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crea y compila el modelo\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('my_model.h5')\n",
    "history = model.fit(X_train, y_train, epochs=10, callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Si usamos un conjunto de validación durante el entrenamiento, podemos configurar `save_best_only=True` al crear el punto de control ModelCheckpoint. En este caso, solo se guardará el modelo cuando su rendimiento con el conjunto de validación sea el mejor hasta el momento.\n",
    "    - De esta forma, no tenemos que preocuparnos por entrenar demsiado tiempo ni sobreajustar el conjunto de entrenamiento: simplemente restauramos el último modelo guardado después del entrenamiento, que será el mejor modelo con el conjunto de validación.\n",
    "- El siguiente código es una forma sencilla de implementar detección temprana:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/10\n",
      "11610/11610 [==============================] - 1s 72us/sample - loss: 1.9063 - val_loss: 0.7688\n",
      "Epoch 2/10\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.6865 - val_loss: 0.6254\n",
      "Epoch 3/10\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.6191 - val_loss: 0.5811\n",
      "Epoch 4/10\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.5852 - val_loss: 0.5522\n",
      "Epoch 5/10\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.5589 - val_loss: 0.5326\n",
      "Epoch 6/10\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.5394 - val_loss: 0.5156\n",
      "Epoch 7/10\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.5239 - val_loss: 0.5034\n",
      "Epoch 8/10\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.5113 - val_loss: 0.4933\n",
      "Epoch 9/10\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.5015 - val_loss: 0.4843\n",
      "Epoch 10/10\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.4934 - val_loss: 0.4783\n",
      "5160/5160 [==============================] - 0s 35us/sample - loss: 0.4744\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])    \n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\", save_best_only=True)\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb])\n",
    "model = keras.models.load_model(\"my_keras_model.h5\") # rollback to best model\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Otra forma de implementar la detección temprana es usar la retrollamada EarlyStopping.\n",
    "    - Interrumpirá el entrenamiento cuando no mida progreso en el conjunto de validación durante un número de repeticiones determinado (definido por el argumento patience), y volvera opcionalmente, al mejor modelo.\n",
    "- Podemos combinar ambas retrollamadas para guardar puntos de control del modelo (por su falla el ordenador) e interrumpir el entrenamiento pronto cuando no haya más progreso (para evitar malgastar tiempo y recursos):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "11610/11610 [==============================] - 1s 70us/sample - loss: 0.4866 - val_loss: 0.4737\n",
      "Epoch 2/100\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.4813 - val_loss: 0.4681\n",
      "Epoch 3/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.4760 - val_loss: 0.4650\n",
      "Epoch 4/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.4724 - val_loss: 0.4599\n",
      "Epoch 5/100\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.4682 - val_loss: 0.4591\n",
      "Epoch 6/100\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.4648 - val_loss: 0.4553\n",
      "Epoch 7/100\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.4619 - val_loss: 0.4532\n",
      "Epoch 8/100\n",
      "11610/11610 [==============================] - 1s 60us/sample - loss: 0.4589 - val_loss: 0.4504\n",
      "Epoch 9/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.4564 - val_loss: 0.4476\n",
      "Epoch 10/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.4538 - val_loss: 0.4459\n",
      "Epoch 11/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.4512 - val_loss: 0.4435\n",
      "Epoch 12/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.4492 - val_loss: 0.4420\n",
      "Epoch 13/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.4472 - val_loss: 0.4404\n",
      "Epoch 14/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.4450 - val_loss: 0.4389\n",
      "Epoch 15/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.4431 - val_loss: 0.4371\n",
      "Epoch 16/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.4413 - val_loss: 0.4355\n",
      "Epoch 17/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.4396 - val_loss: 0.4345\n",
      "Epoch 18/100\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.4377 - val_loss: 0.4345\n",
      "Epoch 19/100\n",
      "11610/11610 [==============================] - 1s 61us/sample - loss: 0.4366 - val_loss: 0.4320\n",
      "Epoch 20/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.4347 - val_loss: 0.4308\n",
      "Epoch 21/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.4333 - val_loss: 0.4293\n",
      "Epoch 22/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.4320 - val_loss: 0.4286\n",
      "Epoch 23/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.4309 - val_loss: 0.4261\n",
      "Epoch 24/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.4292 - val_loss: 0.4262\n",
      "Epoch 25/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.4280 - val_loss: 0.4242\n",
      "Epoch 26/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.4267 - val_loss: 0.4245\n",
      "Epoch 27/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.4258 - val_loss: 0.4227\n",
      "Epoch 28/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.4242 - val_loss: 0.4221\n",
      "Epoch 29/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.4236 - val_loss: 0.4209\n",
      "Epoch 30/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.4221 - val_loss: 0.4191\n",
      "Epoch 31/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.4210 - val_loss: 0.4186\n",
      "Epoch 32/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.4197 - val_loss: 0.4172\n",
      "Epoch 33/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.4193 - val_loss: 0.4163\n",
      "Epoch 34/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.4175 - val_loss: 0.4150\n",
      "Epoch 35/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.4168 - val_loss: 0.4150\n",
      "Epoch 36/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.4168 - val_loss: 0.4131\n",
      "Epoch 37/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.4154 - val_loss: 0.4136\n",
      "Epoch 38/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.4136 - val_loss: 0.4112\n",
      "Epoch 39/100\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.4124 - val_loss: 0.4116\n",
      "Epoch 40/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.4120 - val_loss: 0.4093\n",
      "Epoch 41/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.4108 - val_loss: 0.4100\n",
      "Epoch 42/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.4098 - val_loss: 0.4085\n",
      "Epoch 43/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.4084 - val_loss: 0.4066\n",
      "Epoch 44/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.4074 - val_loss: 0.4080\n",
      "Epoch 45/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.4078 - val_loss: 0.4039\n",
      "Epoch 46/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.4056 - val_loss: 0.4060\n",
      "Epoch 47/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.4062 - val_loss: 0.4035\n",
      "Epoch 48/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.4034 - val_loss: 0.4031\n",
      "Epoch 49/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.4028 - val_loss: 0.4012\n",
      "Epoch 50/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.4016 - val_loss: 0.4007\n",
      "Epoch 51/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.4004 - val_loss: 0.3989\n",
      "Epoch 52/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3994 - val_loss: 0.3987\n",
      "Epoch 53/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3985 - val_loss: 0.3975\n",
      "Epoch 54/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3973 - val_loss: 0.3980\n",
      "Epoch 55/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3970 - val_loss: 0.3967\n",
      "Epoch 56/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3957 - val_loss: 0.3961\n",
      "Epoch 57/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3949 - val_loss: 0.3949\n",
      "Epoch 58/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3936 - val_loss: 0.3940\n",
      "Epoch 59/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3930 - val_loss: 0.3924\n",
      "Epoch 60/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3925 - val_loss: 0.3944\n",
      "Epoch 61/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3911 - val_loss: 0.3906\n",
      "Epoch 62/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3903 - val_loss: 0.3907\n",
      "Epoch 63/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3895 - val_loss: 0.3902\n",
      "Epoch 64/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3891 - val_loss: 0.3887\n",
      "Epoch 65/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3877 - val_loss: 0.3873\n",
      "Epoch 66/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3872 - val_loss: 0.3880\n",
      "Epoch 67/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3864 - val_loss: 0.3864\n",
      "Epoch 68/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3853 - val_loss: 0.3868\n",
      "Epoch 69/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3846 - val_loss: 0.3849\n",
      "Epoch 70/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3841 - val_loss: 0.3848\n",
      "Epoch 71/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3826 - val_loss: 0.3830\n",
      "Epoch 72/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3820 - val_loss: 0.3841\n",
      "Epoch 73/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3821 - val_loss: 0.3827\n",
      "Epoch 74/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3810 - val_loss: 0.3800\n",
      "Epoch 75/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3812 - val_loss: 0.3805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3792 - val_loss: 0.3816\n",
      "Epoch 77/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3787 - val_loss: 0.3782\n",
      "Epoch 78/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3779 - val_loss: 0.3797\n",
      "Epoch 79/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3777 - val_loss: 0.3771\n",
      "Epoch 80/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3766 - val_loss: 0.3793\n",
      "Epoch 81/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3764 - val_loss: 0.3783\n",
      "Epoch 82/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3746 - val_loss: 0.3765\n",
      "Epoch 83/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3737 - val_loss: 0.3766\n",
      "Epoch 84/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3737 - val_loss: 0.3732\n",
      "Epoch 85/100\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.3724 - val_loss: 0.3755\n",
      "Epoch 86/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3724 - val_loss: 0.3744\n",
      "Epoch 87/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3711 - val_loss: 0.3740\n",
      "Epoch 88/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3704 - val_loss: 0.3731\n",
      "Epoch 89/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3695 - val_loss: 0.3708\n",
      "Epoch 90/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3695 - val_loss: 0.3736\n",
      "Epoch 91/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3695 - val_loss: 0.3692\n",
      "Epoch 92/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3675 - val_loss: 0.3681\n",
      "Epoch 93/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3677 - val_loss: 0.3694\n",
      "Epoch 94/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3661 - val_loss: 0.3696\n",
      "Epoch 95/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3661 - val_loss: 0.3663\n",
      "Epoch 96/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3651 - val_loss: 0.3707\n",
      "Epoch 97/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3670 - val_loss: 0.3650\n",
      "Epoch 98/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3639 - val_loss: 0.3722\n",
      "Epoch 99/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3676 - val_loss: 0.3636\n",
      "Epoch 100/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3632 - val_loss: 0.3704\n",
      "5160/5160 [==============================] - 0s 27us/sample - loss: 0.3620\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                  restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- El número de repteciones se puede configurar en un valor más alto, ya que el entrenamiento se detendrá automáticamente cuando ya no haya progreso.\n",
    "    - En este caso, no haynecesdiad de restaurar el mejor modelo guardado porque la retrollamada EarlyStopping llevará un registro de los mejores pesos y los restaurará por nosotros al final del entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Si necesitasemos control adicional, podemos escribir nuestras retrollamdas personalizadas.\n",
    "    - Por ejemplo, la siguiente retrollamada personalizada mostrará la proporción entre la pérdida de validación y la de entrenamiento durante el entrenamiento(por ejemplo al detectar un sobreajuste):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "10848/11610 [===========================>..] - ETA: 0s - loss: 0.3595\n",
      "val/train: 1.01\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3627 - val_loss: 0.3655\n"
     ]
    }
   ],
   "source": [
    "val_train_ratio_cb = PrintValTrainRatioCallback()\n",
    "history = model.fit(X_train, y_train, epochs=1,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[val_train_ratio_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tensorboard es una herramienta de visualización interactiva para:\n",
    "    - ver curvas de aprendizaje durante el entrenamiento\n",
    "    - comparar curvas de aprendizaje entre ejecuciones\n",
    "    - visualizar el gráfico de computación\n",
    "    - analizar estadísticas de entrenamiento\n",
    "    - ver imágenes generadas por el modelo\n",
    "    - visualizar datos multidimensionales complejos proyectados en 3D y agrupados automáticamente por nosotros\n",
    "- Se instala automáticamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hay que modificar el programa para que saque los datos que queremos visualizar como archivos de registro binarios especiales denominados \"archivos de evento\"\n",
    "- Cada registro binario se llama \"resumen\"\n",
    "- El servidor Tensorboard monitorizará el directorio de registros y cogera automáticamente los cambios para actualizar las visaulizaciones:\n",
    "    - Esto nos permite visualizar datos en vivo (con un breve retardo) como las curvas de aprendijaze durante el entrenamiento\n",
    "- En general conviene apuntar el servidor TensorBoard a un directorio de registros raíz y configurar el programa para que escriba en un subdirectori diferente cada vez que se ejecute.\n",
    "    - De este modo, la misma instancia del servidor TensorBoard nos permitirá visualizar y comparar datos de distintas ejecuciones del programa, sin que se mezcle todo.\n",
    "- Empezaremos definiendo el directorio de registro raíz que usaremos para los registros de TensorBoard\n",
    "    - Además una definiremos una pequeña función que generarña una ruta de subdirectorios basada en fecha y hora actuales, así será diferente en cada ejecución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root_logdir = os.path.join(os.curdir, \"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\\\logs\\\\run_2020_11_24-17_06_51'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()\n",
    "run_logdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Keras ofrece una retrollamada a tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/30\n",
      "11610/11610 [==============================] - 1s 83us/sample - loss: 1.9063 - val_loss: 0.7688\n",
      "Epoch 2/30\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 0.6865 - val_loss: 0.6254\n",
      "Epoch 3/30\n",
      "11610/11610 [==============================] - 1s 65us/sample - loss: 0.6191 - val_loss: 0.5811\n",
      "Epoch 4/30\n",
      "11610/11610 [==============================] - 1s 76us/sample - loss: 0.5852 - val_loss: 0.5522\n",
      "Epoch 5/30\n",
      "11610/11610 [==============================] - 1s 82us/sample - loss: 0.5589 - val_loss: 0.5326\n",
      "Epoch 6/30\n",
      "11610/11610 [==============================] - 1s 70us/sample - loss: 0.5394 - val_loss: 0.5156\n",
      "Epoch 7/30\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 0.5239 - val_loss: 0.5034\n",
      "Epoch 8/30\n",
      "11610/11610 [==============================] - 1s 68us/sample - loss: 0.5113 - val_loss: 0.4933\n",
      "Epoch 9/30\n",
      "11610/11610 [==============================] - 1s 65us/sample - loss: 0.5015 - val_loss: 0.4843\n",
      "Epoch 10/30\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.4934 - val_loss: 0.4783\n",
      "Epoch 11/30\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.4865 - val_loss: 0.4727\n",
      "Epoch 12/30\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.4812 - val_loss: 0.4680\n",
      "Epoch 13/30\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.4764 - val_loss: 0.4646\n",
      "Epoch 14/30\n",
      "11610/11610 [==============================] - 1s 65us/sample - loss: 0.4721 - val_loss: 0.4613\n",
      "Epoch 15/30\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.4683 - val_loss: 0.4581\n",
      "Epoch 16/30\n",
      "11610/11610 [==============================] - 1s 69us/sample - loss: 0.4649 - val_loss: 0.4549\n",
      "Epoch 17/30\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.4619 - val_loss: 0.4527\n",
      "Epoch 18/30\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.4587 - val_loss: 0.4513\n",
      "Epoch 19/30\n",
      "11610/11610 [==============================] - 1s 69us/sample - loss: 0.4565 - val_loss: 0.4484\n",
      "Epoch 20/30\n",
      "11610/11610 [==============================] - 1s 68us/sample - loss: 0.4538 - val_loss: 0.4467\n",
      "Epoch 21/30\n",
      "11610/11610 [==============================] - 1s 71us/sample - loss: 0.4511 - val_loss: 0.4440\n",
      "Epoch 22/30\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.4492 - val_loss: 0.4425\n",
      "Epoch 23/30\n",
      "11610/11610 [==============================] - 1s 64us/sample - loss: 0.4472 - val_loss: 0.4399\n",
      "Epoch 24/30\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.4448 - val_loss: 0.4393\n",
      "Epoch 25/30\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.4431 - val_loss: 0.4369\n",
      "Epoch 26/30\n",
      "11610/11610 [==============================] - 1s 64us/sample - loss: 0.4412 - val_loss: 0.4367\n",
      "Epoch 27/30\n",
      "11610/11610 [==============================] - 1s 65us/sample - loss: 0.4397 - val_loss: 0.4346\n",
      "Epoch 28/30\n",
      "11610/11610 [==============================] - 1s 65us/sample - loss: 0.4377 - val_loss: 0.4337\n",
      "Epoch 29/30\n",
      "11610/11610 [==============================] - 1s 64us/sample - loss: 0.4366 - val_loss: 0.4322\n",
      "Epoch 30/30\n",
      "11610/11610 [==============================] - 1s 64us/sample - loss: 0.4348 - val_loss: 0.4301\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])    \n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- El siguiente código es otra ejecución para verlo en TensorBoard y poder comparar resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\\\logs\\\\run_2020_11_24-17_15_43'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_logdir2 = get_run_logdir()\n",
    "run_logdir2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/30\n",
      "11610/11610 [==============================] - 1s 82us/sample - loss: 0.5792 - val_loss: 0.4464\n",
      "Epoch 2/30\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.4434 - val_loss: 0.4200\n",
      "Epoch 3/30\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.4155 - val_loss: 0.3856\n",
      "Epoch 4/30\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 0.3869 - val_loss: 0.3592\n",
      "Epoch 5/30\n",
      "11610/11610 [==============================] - 1s 64us/sample - loss: 0.3740 - val_loss: 0.3958\n",
      "Epoch 6/30\n",
      "11610/11610 [==============================] - 1s 65us/sample - loss: 0.3646 - val_loss: 0.3444\n",
      "Epoch 7/30\n",
      "11610/11610 [==============================] - 1s 64us/sample - loss: 0.3559 - val_loss: 0.3705\n",
      "Epoch 8/30\n",
      "11610/11610 [==============================] - 1s 72us/sample - loss: 0.3484 - val_loss: 0.3444\n",
      "Epoch 9/30\n",
      "11610/11610 [==============================] - 1s 69us/sample - loss: 0.3479 - val_loss: 0.3191\n",
      "Epoch 10/30\n",
      "11610/11610 [==============================] - 1s 68us/sample - loss: 0.3422 - val_loss: 0.3314\n",
      "Epoch 11/30\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 0.3444 - val_loss: 0.3213\n",
      "Epoch 12/30\n",
      "11610/11610 [==============================] - 1s 65us/sample - loss: 0.3321 - val_loss: 0.3927\n",
      "Epoch 13/30\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 0.3306 - val_loss: 0.3189\n",
      "Epoch 14/30\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.3238 - val_loss: 0.4143\n",
      "Epoch 15/30\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.3247 - val_loss: 0.3130\n",
      "Epoch 16/30\n",
      "11610/11610 [==============================] - 1s 64us/sample - loss: 0.3178 - val_loss: 0.3257\n",
      "Epoch 17/30\n",
      "11610/11610 [==============================] - 1s 64us/sample - loss: 0.3153 - val_loss: 0.3649\n",
      "Epoch 18/30\n",
      "11610/11610 [==============================] - 1s 70us/sample - loss: 0.3171 - val_loss: 0.3039\n",
      "Epoch 19/30\n",
      "11610/11610 [==============================] - 1s 65us/sample - loss: 0.3120 - val_loss: 0.3154\n",
      "Epoch 20/30\n",
      "11610/11610 [==============================] - 1s 65us/sample - loss: 0.3112 - val_loss: 0.3009\n",
      "Epoch 21/30\n",
      "11610/11610 [==============================] - 1s 65us/sample - loss: 0.3065 - val_loss: 0.3100\n",
      "Epoch 22/30\n",
      "11610/11610 [==============================] - 1s 65us/sample - loss: 0.3045 - val_loss: 0.4309\n",
      "Epoch 23/30\n",
      "11610/11610 [==============================] - 1s 65us/sample - loss: 0.3081 - val_loss: 0.3576\n",
      "Epoch 24/30\n",
      "11610/11610 [==============================] - 1s 70us/sample - loss: 0.3064 - val_loss: 0.3847\n",
      "Epoch 25/30\n",
      "11610/11610 [==============================] - 1s 69us/sample - loss: 0.2994 - val_loss: 0.3055\n",
      "Epoch 26/30\n",
      "11610/11610 [==============================] - 1s 65us/sample - loss: 0.2998 - val_loss: 0.3022\n",
      "Epoch 27/30\n",
      "11610/11610 [==============================] - 1s 64us/sample - loss: 0.2958 - val_loss: 0.3124\n",
      "Epoch 28/30\n",
      "11610/11610 [==============================] - 1s 64us/sample - loss: 0.2940 - val_loss: 0.3205\n",
      "Epoch 29/30\n",
      "11610/11610 [==============================] - 1s 71us/sample - loss: 0.2973 - val_loss: 0.3080\n",
      "Epoch 30/30\n",
      "11610/11610 [==============================] - 1s 71us/sample - loss: 0.2939 - val_loss: 0.2903\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])    \n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=0.05))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir2)\n",
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function __init__ in module tensorflow.python.keras.callbacks:\n",
      "\n",
      "__init__(self, log_dir='logs', histogram_freq=0, write_graph=True, write_images=False, update_freq='epoch', profile_batch=2, embeddings_freq=0, embeddings_metadata=None, **kwargs)\n",
      "    Initialize self.  See help(type(self)) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(keras.callbacks.TensorBoard.__init__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Para iniciar el servidor TensorBoard se realiza así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=./logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tensorflow ofrece un API de bajo nivel en el paquete tf.summary\n",
    "- El siguiente código crea un SummaryWriter con la función create_file_writer() y lo usa como contexto para registar escalares, histogramas, imágenes, sonido y texto, todo lo cual se puede visulizar cn TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logdir = get_run_logdir()\n",
    "writer = tf.summary.create_file_writer(test_logdir)\n",
    "with writer.as_default():\n",
    "    for step in range(1, 1000+1):\n",
    "        tf.summary.scalar('my_scalar', np.sin(step / 10), step=step)\n",
    "        data = (np.random.randn(100) + 2) * step / 100 # datos aleatorios\n",
    "        tf.summary.histogram('my_hist', data, buckets=50, step=step)\n",
    "        images = np.random.rand(2, 32, 32, 3) # imágenes RGB de 32x 32 aleatorias\n",
    "        tf.summary.image('my_images', images * step / 1000, step=step)\n",
    "        texts = ['The step is ' + str(step), 'Its square is ' + str(step**2)]\n",
    "        tf.summary.text('my_text',texts,step=step)\n",
    "        sine_wave = tf.math.sin(tf.range(12000)/48000 *2 * np.pi * step)\n",
    "        audio = tf.reshape(tf.cast(sine_wave, tf.float32), [1,-1,1])\n",
    "        tf.summary.audio('my_audio', audio, sample_rate=48000, step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajuste de los hiperparams de una red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Una red neuronal puede variar en:\n",
    "    - Arquitectura\n",
    "        - Número de capas, de neuronas \n",
    "        - Tipo de función de activación para cada capa\n",
    "        - La capa lógica de inicilización de pesos,\n",
    "        - Etc\n",
    "- ¿Cómo sabemos que combinación de hiperparams es la mejor para nuestra tarea?\n",
    "    - Una opción es probar distintas combinacion es de hiperparams y ver cuál funciona mejor con el conjunto de validación (o usar validación cruzada de K iteraciones)\n",
    "        - Por ejemplo, podemos usar GridSearchCV o RandomizedSearchCV para explorar el espacio de hiperparams\n",
    "            - Para eso necesitamos envolver nuestros modelos de Keras en objetos que imiten regresores normales de SKLearn.\n",
    "\n",
    "1. Crear una funciónque construya un modelo Keras con un conjunto de hiperparams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Esta función crea un modelo sequential sencillo para regresión univariante (solo una neurona de salida), con la forma de entrada dada y el número de capas ocultas y lo compila usando un optimizador SGD configurado con la tasa de aprendizje especificada.\n",
    "- Es buena práctica proporcionar opciones predeterminadas razonables para tantos hiperparams como sea posible, como hae sklearn.\n",
    "- A continuación, crearemos un KerasRegressor basad en esta función build_model():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- El objeto de KerasRegressor es una envoltura fina para el modelo Keras construido con build_model()\n",
    "- Puesto que no hemos especificado ningún hiperparam al crearlo, usará los predeterminados definidos n build_model().\n",
    "- Ahora podemos usar este objeto como un regresor de SKLearn normal\n",
    "    - Podemos entrenarlo con su método fit() y luego evaluarlo con score(), y usarlo para hacer prediccciones con el método predict(), como se aprecia en el siguiente código:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 1.0652 - val_loss: 0.6805\n",
      "Epoch 2/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.6493 - val_loss: 0.5668\n",
      "Epoch 3/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.5638 - val_loss: 0.5255\n",
      "Epoch 4/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.5414 - val_loss: 0.4985\n",
      "Epoch 5/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.5114 - val_loss: 0.4842\n",
      "Epoch 6/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.4905 - val_loss: 0.4698\n",
      "Epoch 7/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.4797 - val_loss: 0.4640\n",
      "Epoch 8/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4728 - val_loss: 0.4577\n",
      "Epoch 9/100\n",
      "11610/11610 [==============================] - 1s 49us/sample - loss: 0.4665 - val_loss: 0.4520\n",
      "Epoch 10/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4620 - val_loss: 0.4487\n",
      "Epoch 11/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4579 - val_loss: 0.4457\n",
      "Epoch 12/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4544 - val_loss: 0.4430\n",
      "Epoch 13/100\n",
      "11610/11610 [==============================] - 1s 49us/sample - loss: 0.4515 - val_loss: 0.4400\n",
      "Epoch 14/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.4487 - val_loss: 0.4396\n",
      "Epoch 15/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.4459 - val_loss: 0.4353\n",
      "Epoch 16/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4437 - val_loss: 0.4338\n",
      "Epoch 17/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4414 - val_loss: 0.4327\n",
      "Epoch 18/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.4385 - val_loss: 0.4368\n",
      "Epoch 19/100\n",
      "11610/11610 [==============================] - 1s 49us/sample - loss: 0.4390 - val_loss: 0.4295\n",
      "Epoch 20/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4350 - val_loss: 0.4297\n",
      "Epoch 21/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.4334 - val_loss: 0.4257\n",
      "Epoch 22/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4316 - val_loss: 0.4290\n",
      "Epoch 23/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.4368 - val_loss: 0.4219\n",
      "Epoch 24/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4302 - val_loss: 0.4225\n",
      "Epoch 25/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.4260 - val_loss: 0.4200\n",
      "Epoch 26/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4246 - val_loss: 0.4229\n",
      "Epoch 27/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4244 - val_loss: 0.4191\n",
      "Epoch 28/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.4210 - val_loss: 0.4191\n",
      "Epoch 29/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.4214 - val_loss: 0.4166\n",
      "Epoch 30/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4203 - val_loss: 0.4128\n",
      "Epoch 31/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4166 - val_loss: 0.4159\n",
      "Epoch 32/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4235 - val_loss: 0.4104\n",
      "Epoch 33/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4196 - val_loss: 0.4088\n",
      "Epoch 34/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4119 - val_loss: 0.4069\n",
      "Epoch 35/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4112 - val_loss: 0.4119\n",
      "Epoch 36/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4300 - val_loss: 0.4049\n",
      "Epoch 37/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4148 - val_loss: 0.4067\n",
      "Epoch 38/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4068 - val_loss: 0.4019\n",
      "Epoch 39/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.4049 - val_loss: 0.4048\n",
      "Epoch 40/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4138 - val_loss: 0.4011\n",
      "Epoch 41/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4036 - val_loss: 0.4043\n",
      "Epoch 42/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.4030 - val_loss: 0.4026\n",
      "Epoch 43/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.4032 - val_loss: 0.3980\n",
      "Epoch 44/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3990 - val_loss: 0.4016\n",
      "Epoch 45/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.4119 - val_loss: 0.3956\n",
      "Epoch 46/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3970 - val_loss: 0.4023\n",
      "Epoch 47/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.4193 - val_loss: 0.3946\n",
      "Epoch 48/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3950 - val_loss: 0.3968\n",
      "Epoch 49/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3947 - val_loss: 0.3927\n",
      "Epoch 50/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3926 - val_loss: 0.3905\n",
      "Epoch 51/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3909 - val_loss: 0.3882\n",
      "Epoch 52/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3903 - val_loss: 0.3885\n",
      "Epoch 53/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3894 - val_loss: 0.3861\n",
      "Epoch 54/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3879 - val_loss: 0.3890\n",
      "Epoch 55/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3884 - val_loss: 0.3884\n",
      "Epoch 56/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3866 - val_loss: 0.3866\n",
      "Epoch 57/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3881 - val_loss: 0.3845\n",
      "Epoch 58/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3832 - val_loss: 0.3862\n",
      "Epoch 59/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3877 - val_loss: 0.3808\n",
      "Epoch 60/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3860 - val_loss: 0.3840\n",
      "Epoch 61/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3824 - val_loss: 0.3801\n",
      "Epoch 62/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3810 - val_loss: 0.3798\n",
      "Epoch 63/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3805 - val_loss: 0.3812\n",
      "Epoch 64/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3810 - val_loss: 0.3819\n",
      "Epoch 65/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3882 - val_loss: 0.3756\n",
      "Epoch 66/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3776 - val_loss: 0.3792\n",
      "Epoch 67/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3779 - val_loss: 0.3766\n",
      "Epoch 68/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3768 - val_loss: 0.3778\n",
      "Epoch 69/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3809 - val_loss: 0.3738\n",
      "Epoch 70/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3765 - val_loss: 0.3739\n",
      "Epoch 71/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3736 - val_loss: 0.3720\n",
      "Epoch 72/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3738 - val_loss: 0.3745\n",
      "Epoch 73/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3741 - val_loss: 0.3725\n",
      "Epoch 74/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3750 - val_loss: 0.3698\n",
      "Epoch 75/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3742 - val_loss: 0.3692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3716 - val_loss: 0.3734\n",
      "Epoch 77/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3757 - val_loss: 0.3677\n",
      "Epoch 78/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3698 - val_loss: 0.3725\n",
      "Epoch 79/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3780 - val_loss: 0.3696\n",
      "Epoch 80/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3683 - val_loss: 0.3699\n",
      "Epoch 81/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3691 - val_loss: 0.3697\n",
      "Epoch 82/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3669 - val_loss: 0.3688\n",
      "Epoch 83/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3657 - val_loss: 0.3665\n",
      "Epoch 84/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3660 - val_loss: 0.3645\n",
      "Epoch 85/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3645 - val_loss: 0.3652\n",
      "Epoch 86/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3648 - val_loss: 0.3663\n",
      "Epoch 87/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3637 - val_loss: 0.3663\n",
      "Epoch 88/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3634 - val_loss: 0.3663\n",
      "Epoch 89/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3650 - val_loss: 0.3621\n",
      "Epoch 90/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3642 - val_loss: 0.3667\n",
      "Epoch 91/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3688 - val_loss: 0.3601\n",
      "Epoch 92/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3616 - val_loss: 0.3605\n",
      "Epoch 93/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3614 - val_loss: 0.3620\n",
      "Epoch 94/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3625 - val_loss: 0.3617\n",
      "Epoch 95/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3617 - val_loss: 0.3585\n",
      "Epoch 96/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3596 - val_loss: 0.3607\n",
      "Epoch 97/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3605 - val_loss: 0.3583\n",
      "Epoch 98/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3582 - val_loss: 0.3629\n",
      "Epoch 99/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3611 - val_loss: 0.3564\n",
      "Epoch 100/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3576 - val_loss: 0.3622\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22a187c1f28>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_reg.fit(X_train, y_train, epochs=100,\n",
    "              validation_data=(X_valid, y_valid),\n",
    "              callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Queremos cientos de variantes a ver cual rinde mejor con el conjunto de validación. \n",
    "- Dado que hay muchos hiperparams, es preferible usar una búsqueda aleatorizada mejor que una exhaustiva (como vimos en el capítulo 2)\n",
    "- Vamos a explorar el número de capas ocultas, el número de neuronas y la tasa de aprendizaje:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] learning_rate=0.001683454924600351, n_hidden=0, n_neurons=15 ....\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 1s 76us/sample - loss: 3.1926 - val_loss: 1.3865\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.9501 - val_loss: 0.6899\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.6059 - val_loss: 0.5504\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5412 - val_loss: 0.5273\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5270 - val_loss: 0.5180\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5258 - val_loss: 0.5203\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5256 - val_loss: 0.5170\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5256 - val_loss: 0.5165\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5251 - val_loss: 0.5205\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5248 - val_loss: 0.5214\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5236 - val_loss: 0.5162\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5250 - val_loss: 0.5191\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5228 - val_loss: 0.5159\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5254 - val_loss: 0.5164\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5240 - val_loss: 0.5210\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5237 - val_loss: 0.5158\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.5254 - val_loss: 0.5181\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5238 - val_loss: 0.5163\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.5250 - val_loss: 0.5165\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5237 - val_loss: 0.5157\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5253 - val_loss: 0.5164\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.5240 - val_loss: 0.5161\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.5228 - val_loss: 0.5216\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5224 - val_loss: 0.5164\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5222 - val_loss: 0.5219\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5250 - val_loss: 0.5193\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5237 - val_loss: 0.5164\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.5241 - val_loss: 0.5163\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5247 - val_loss: 0.5169\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5233 - val_loss: 0.5207\n",
      "3870/3870 [==============================] - 0s 26us/sample - loss: 0.5700\n",
      "[CV]  learning_rate=0.001683454924600351, n_hidden=0, n_neurons=15, total=  13.3s\n",
      "[CV] learning_rate=0.001683454924600351, n_hidden=0, n_neurons=15 ....\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   13.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 1s 74us/sample - loss: 3.0773 - val_loss: 1.6819\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 1.1969 - val_loss: 0.8918\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - ETA: 0s - loss: 0.795 - 0s 56us/sample - loss: 0.7973 - val_loss: 0.7085\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6994 - val_loss: 0.6548\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.6675 - val_loss: 0.6323\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6499 - val_loss: 0.6158\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6372 - val_loss: 0.6049\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.6276 - val_loss: 0.5945\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.6184 - val_loss: 0.5857\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.6109 - val_loss: 0.5779\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6037 - val_loss: 0.5712\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5977 - val_loss: 0.5660\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5918 - val_loss: 0.5604\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5880 - val_loss: 0.5554\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5829 - val_loss: 0.5521\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5794 - val_loss: 0.5475\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5769 - val_loss: 0.5451\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5734 - val_loss: 0.5412\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5715 - val_loss: 0.5387\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5685 - val_loss: 0.5363\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5679 - val_loss: 0.5343\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5646 - val_loss: 0.5323\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5619 - val_loss: 0.5339\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5611 - val_loss: 0.5290\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5604 - val_loss: 0.5303\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5606 - val_loss: 0.5284\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5586 - val_loss: 0.5257\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5581 - val_loss: 0.5250\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5573 - val_loss: 0.5243\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5568 - val_loss: 0.5257\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5564 - val_loss: 0.5226\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5540 - val_loss: 0.5260\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5558 - val_loss: 0.5244\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5536 - val_loss: 0.5210\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5542 - val_loss: 0.5206\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5535 - val_loss: 0.5245\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5540 - val_loss: 0.5233\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5506 - val_loss: 0.5198\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5535 - val_loss: 0.5194\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5521 - val_loss: 0.5225\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5496 - val_loss: 0.5188\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5528 - val_loss: 0.5187\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5513 - val_loss: 0.5219\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5508 - val_loss: 0.5189\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5507 - val_loss: 0.5231\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5521 - val_loss: 0.5226\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5510 - val_loss: 0.5189\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5515 - val_loss: 0.5190\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5499 - val_loss: 0.5236\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5522 - val_loss: 0.5193\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5510 - val_loss: 0.5203\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5505 - val_loss: 0.5235\n",
      "3870/3870 [==============================] - 0s 25us/sample - loss: 0.5178\n",
      "[CV]  learning_rate=0.001683454924600351, n_hidden=0, n_neurons=15, total=  22.0s\n",
      "[CV] learning_rate=0.001683454924600351, n_hidden=0, n_neurons=15 ....\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 75us/sample - loss: 3.3303 - val_loss: 1.4798\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 1.0543 - val_loss: 0.7355\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.6818 - val_loss: 0.5902\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6058 - val_loss: 0.5599\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5873 - val_loss: 0.5503\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5801 - val_loss: 0.5457\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5760 - val_loss: 0.5420\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5724 - val_loss: 0.5389\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5692 - val_loss: 0.5364\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5663 - val_loss: 0.5337\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5638 - val_loss: 0.5321\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5616 - val_loss: 0.5297\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5593 - val_loss: 0.5283\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5575 - val_loss: 0.5269\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5558 - val_loss: 0.5246\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5544 - val_loss: 0.5235\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5529 - val_loss: 0.5229\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5516 - val_loss: 0.5212\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5505 - val_loss: 0.5203\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5494 - val_loss: 0.5198\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5485 - val_loss: 0.5191\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5477 - val_loss: 0.5178\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5466 - val_loss: 0.5183\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5463 - val_loss: 0.5172\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5455 - val_loss: 0.5160\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5449 - val_loss: 0.5155\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5442 - val_loss: 0.5156\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5440 - val_loss: 0.5148\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5434 - val_loss: 0.5143\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5430 - val_loss: 0.5137\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5425 - val_loss: 0.5139\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5424 - val_loss: 0.5136\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5419 - val_loss: 0.5130\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5416 - val_loss: 0.5130\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5413 - val_loss: 0.5132\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5412 - val_loss: 0.5132\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5408 - val_loss: 0.5126\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5408 - val_loss: 0.5129\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5404 - val_loss: 0.5132\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5403 - val_loss: 0.5122\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5402 - val_loss: 0.5125\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5398 - val_loss: 0.5123\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5399 - val_loss: 0.5127\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5399 - val_loss: 0.5128\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5396 - val_loss: 0.5126\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5395 - val_loss: 0.5125\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5395 - val_loss: 0.5121\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5392 - val_loss: 0.5128\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5393 - val_loss: 0.5126\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5390 - val_loss: 0.5122\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5392 - val_loss: 0.5119\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5390 - val_loss: 0.5120\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5389 - val_loss: 0.5126\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5391 - val_loss: 0.5122\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5389 - val_loss: 0.5119\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5387 - val_loss: 0.5125\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5388 - val_loss: 0.5126\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5387 - val_loss: 0.5122\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5387 - val_loss: 0.5121\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5385 - val_loss: 0.5122\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5387 - val_loss: 0.5123\n",
      "3870/3870 [==============================] - 0s 27us/sample - loss: 0.5364\n",
      "[CV]  learning_rate=0.001683454924600351, n_hidden=0, n_neurons=15, total=  25.6s\n",
      "[CV] learning_rate=0.008731907739399206, n_hidden=0, n_neurons=21 ....\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 76us/sample - loss: 1.4943 - val_loss: 0.5672\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5838 - val_loss: 0.6076\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5382 - val_loss: 0.5228\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5275 - val_loss: 0.5216\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5339 - val_loss: 0.5311\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5329 - val_loss: 0.5211\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5947 - val_loss: 0.8823\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.7361 - val_loss: 0.5440\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5649 - val_loss: 0.5990\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5765 - val_loss: 0.5424\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5302 - val_loss: 0.5207\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5472 - val_loss: 0.5250\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5241 - val_loss: 0.5206\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5847 - val_loss: 0.5202\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5393 - val_loss: 0.6029\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5444 - val_loss: 0.6168\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5713 - val_loss: 0.5354\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5881 - val_loss: 0.5217\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.6348 - val_loss: 0.5229\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5411 - val_loss: 0.5257\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.6720 - val_loss: 0.6309\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.6223 - val_loss: 0.5424\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6445 - val_loss: 0.5435\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5221 - val_loss: 0.5186\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5261 - val_loss: 0.5608\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.6461 - val_loss: 0.5269\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5937 - val_loss: 0.5315\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6293 - val_loss: 0.5407\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5811 - val_loss: 0.5288\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5487 - val_loss: 0.5381\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5717 - val_loss: 0.8490\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.7636 - val_loss: 0.5459\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.7761 - val_loss: 0.6165\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5308 - val_loss: 0.5192\n",
      "3870/3870 [==============================] - 0s 25us/sample - loss: 0.5676\n",
      "[CV]  learning_rate=0.008731907739399206, n_hidden=0, n_neurons=21, total=  14.6s\n",
      "[CV] learning_rate=0.008731907739399206, n_hidden=0, n_neurons=21 ....\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 1s 76us/sample - loss: 1.4676 - val_loss: 0.6396\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.6566 - val_loss: 0.5939\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5988 - val_loss: 0.5511\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5732 - val_loss: 0.5399\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5699 - val_loss: 0.5450\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5645 - val_loss: 0.5231\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6085 - val_loss: 0.8169\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6435 - val_loss: 0.5322\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5768 - val_loss: 0.5613\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5855 - val_loss: 0.5488\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5651 - val_loss: 0.5251\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5650 - val_loss: 0.5248\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5614 - val_loss: 0.5272\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5827 - val_loss: 0.5256\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5584 - val_loss: 0.5346\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5675 - val_loss: 0.5760\n",
      "3870/3870 [==============================] - 0s 26us/sample - loss: 0.5814\n",
      "[CV]  learning_rate=0.008731907739399206, n_hidden=0, n_neurons=21, total=   6.9s\n",
      "[CV] learning_rate=0.008731907739399206, n_hidden=0, n_neurons=21 ....\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 96us/sample - loss: 1.8707 - val_loss: 1.9038\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 4.2801 - val_loss: 19.6375\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 53.6245 - val_loss: 294.6571\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 785.3880 - val_loss: 4347.7417\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 12186.3292 - val_loss: 63924.8381\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 194200.1174 - val_loss: 920489.7220\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 2926150.2198 - val_loss: 13639373.5953\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 43219182.4455 - val_loss: 202109468.1584\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 608367431.2152 - val_loss: 3007052414.4656\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 9196692527.4791 - val_loss: 44392790133.6723\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 134632795553.1411 - val_loss: 657941671001.5670\n",
      "3870/3870 [==============================] - 0s 26us/sample - loss: 1029303172981.0852\n",
      "[CV]  learning_rate=0.008731907739399206, n_hidden=0, n_neurons=21, total=   5.1s\n",
      "[CV] learning_rate=0.0006154014789262348, n_hidden=2, n_neurons=87 ...\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 85us/sample - loss: 3.0693 - val_loss: 1.2942\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 1.0667 - val_loss: 0.8678\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.8151 - val_loss: 0.7710\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.7463 - val_loss: 0.7261\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.7108 - val_loss: 0.6957\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.6843 - val_loss: 0.6729\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.6635 - val_loss: 0.6537\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.6455 - val_loss: 0.6369\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.6291 - val_loss: 0.6224\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.6148 - val_loss: 0.6085\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.6014 - val_loss: 0.5957\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.5892 - val_loss: 0.5852\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.5780 - val_loss: 0.5737\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.5675 - val_loss: 0.5640\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5577 - val_loss: 0.5555\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5489 - val_loss: 0.5466\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.5403 - val_loss: 0.5388\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.5323 - val_loss: 0.5313\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.5253 - val_loss: 0.5247\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.5182 - val_loss: 0.5186\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.5121 - val_loss: 0.5138\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.5063 - val_loss: 0.5081\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.5011 - val_loss: 0.5037\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4960 - val_loss: 0.4985\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4914 - val_loss: 0.4954\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4872 - val_loss: 0.4914\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4830 - val_loss: 0.4866\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4793 - val_loss: 0.4834\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4759 - val_loss: 0.4806\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4725 - val_loss: 0.4776\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4691 - val_loss: 0.4750\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: 0.4663 - val_loss: 0.4728\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4634 - val_loss: 0.4705\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4607 - val_loss: 0.4674\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4585 - val_loss: 0.4651\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4560 - val_loss: 0.4639\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4537 - val_loss: 0.4619\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4514 - val_loss: 0.4591\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4496 - val_loss: 0.4574\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4474 - val_loss: 0.4561\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4455 - val_loss: 0.4544\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4440 - val_loss: 0.4534\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4422 - val_loss: 0.4515\n",
      "Epoch 44/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4406 - val_loss: 0.4500\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4390 - val_loss: 0.4487\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4375 - val_loss: 0.4474\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4360 - val_loss: 0.4466\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4346 - val_loss: 0.4451\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4333 - val_loss: 0.4441\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4317 - val_loss: 0.4423\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4305 - val_loss: 0.4415\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4291 - val_loss: 0.4409\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4279 - val_loss: 0.4396\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4267 - val_loss: 0.4380\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4254 - val_loss: 0.4369\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4243 - val_loss: 0.4363\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4230 - val_loss: 0.4355\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4219 - val_loss: 0.4339\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4209 - val_loss: 0.4333\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4197 - val_loss: 0.4319\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4188 - val_loss: 0.4317\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4176 - val_loss: 0.4302\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4166 - val_loss: 0.4294\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4156 - val_loss: 0.4290\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4141 - val_loss: 0.4281\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4136 - val_loss: 0.4270\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4125 - val_loss: 0.4265\n",
      "Epoch 68/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4113 - val_loss: 0.4249\n",
      "Epoch 69/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4105 - val_loss: 0.4254\n",
      "Epoch 70/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4095 - val_loss: 0.4241\n",
      "Epoch 71/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4085 - val_loss: 0.4235\n",
      "Epoch 72/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4078 - val_loss: 0.4210\n",
      "Epoch 73/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4067 - val_loss: 0.4221\n",
      "Epoch 74/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4061 - val_loss: 0.4209\n",
      "Epoch 75/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4049 - val_loss: 0.4205\n",
      "Epoch 76/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4043 - val_loss: 0.4188\n",
      "Epoch 77/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4034 - val_loss: 0.4175\n",
      "Epoch 78/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4026 - val_loss: 0.4167\n",
      "Epoch 79/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4016 - val_loss: 0.4166\n",
      "Epoch 80/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: 0.4006 - val_loss: 0.4162\n",
      "Epoch 81/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3999 - val_loss: 0.4152\n",
      "Epoch 82/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.3990 - val_loss: 0.4142\n",
      "Epoch 83/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.3982 - val_loss: 0.4134\n",
      "Epoch 84/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.3974 - val_loss: 0.4128\n",
      "Epoch 85/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.3966 - val_loss: 0.4118\n",
      "Epoch 86/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3957 - val_loss: 0.4113\n",
      "Epoch 87/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3951 - val_loss: 0.4105\n",
      "Epoch 88/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3941 - val_loss: 0.4100\n",
      "Epoch 89/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.3934 - val_loss: 0.4086\n",
      "Epoch 90/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.3928 - val_loss: 0.4086\n",
      "Epoch 91/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.3922 - val_loss: 0.4082\n",
      "Epoch 92/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.3915 - val_loss: 0.4074\n",
      "Epoch 93/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3905 - val_loss: 0.4066\n",
      "Epoch 94/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3902 - val_loss: 0.4063\n",
      "Epoch 95/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3893 - val_loss: 0.4060\n",
      "Epoch 96/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.3885 - val_loss: 0.4060\n",
      "Epoch 97/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.3883 - val_loss: 0.4045\n",
      "Epoch 98/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3874 - val_loss: 0.4042\n",
      "Epoch 99/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3866 - val_loss: 0.4043\n",
      "Epoch 100/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.3862 - val_loss: 0.4029\n",
      "3870/3870 [==============================] - 0s 28us/sample - loss: 0.4348\n",
      "[CV]  learning_rate=0.0006154014789262348, n_hidden=2, n_neurons=87, total=  46.1s\n",
      "[CV] learning_rate=0.0006154014789262348, n_hidden=2, n_neurons=87 ...\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 88us/sample - loss: 2.5657 - val_loss: 1.1545\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.9982 - val_loss: 0.8556\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 1s 66us/sample - loss: 0.8368 - val_loss: 0.7715\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.7758 - val_loss: 0.7258\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: 0.7380 - val_loss: 0.6915\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.7077 - val_loss: 0.6637\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.6826 - val_loss: 0.6416\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 1s 65us/sample - loss: 0.6605 - val_loss: 0.6223\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.6414 - val_loss: 0.6057\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: 0.6242 - val_loss: 0.5905\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 1s 66us/sample - loss: 0.6087 - val_loss: 0.5779\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.5954 - val_loss: 0.5656\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.5831 - val_loss: 0.5546\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 1s 67us/sample - loss: 0.5723 - val_loss: 0.5440\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.5623 - val_loss: 0.5357\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.5533 - val_loss: 0.5276\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.5453 - val_loss: 0.5210\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.5380 - val_loss: 0.5136\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5316 - val_loss: 0.5081\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5254 - val_loss: 0.5027\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5200 - val_loss: 0.4980\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.5152 - val_loss: 0.4936\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.5106 - val_loss: 0.4900\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.5061 - val_loss: 0.4855\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.5023 - val_loss: 0.4827\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4986 - val_loss: 0.4797\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4951 - val_loss: 0.4759\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4919 - val_loss: 0.4735\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4889 - val_loss: 0.4710\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4860 - val_loss: 0.4684\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4831 - val_loss: 0.4658\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4803 - val_loss: 0.4637\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4780 - val_loss: 0.4614\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4754 - val_loss: 0.4590\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: 0.4730 - val_loss: 0.4574\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4708 - val_loss: 0.4556\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.4687 - val_loss: 0.4542\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4665 - val_loss: 0.4523\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4644 - val_loss: 0.4499\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4624 - val_loss: 0.4486\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4606 - val_loss: 0.4470\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4588 - val_loss: 0.4454\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4569 - val_loss: 0.4448\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4553 - val_loss: 0.4427\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4534 - val_loss: 0.4414\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4519 - val_loss: 0.4399\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4500 - val_loss: 0.4394\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4486 - val_loss: 0.4381\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4472 - val_loss: 0.4365\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4457 - val_loss: 0.4353\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4446 - val_loss: 0.4341\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4431 - val_loss: 0.4331\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4419 - val_loss: 0.4322\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4405 - val_loss: 0.4310\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4392 - val_loss: 0.4298\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4380 - val_loss: 0.4295\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 1s 66us/sample - loss: 0.4369 - val_loss: 0.4281\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: 0.4358 - val_loss: 0.4271\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4346 - val_loss: 0.4266\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: 0.4335 - val_loss: 0.4254\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4325 - val_loss: 0.4244\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4314 - val_loss: 0.4237\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4303 - val_loss: 0.4224\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: 0.4292 - val_loss: 0.4216\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4281 - val_loss: 0.4214\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4273 - val_loss: 0.4201\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4262 - val_loss: 0.4193\n",
      "Epoch 68/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4253 - val_loss: 0.4193\n",
      "Epoch 69/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: 0.4242 - val_loss: 0.4186\n",
      "Epoch 70/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: 0.4235 - val_loss: 0.4182\n",
      "Epoch 71/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.4226 - val_loss: 0.4176\n",
      "Epoch 72/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4217 - val_loss: 0.4158\n",
      "Epoch 73/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4209 - val_loss: 0.4152\n",
      "Epoch 74/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4202 - val_loss: 0.4143\n",
      "Epoch 75/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.4194 - val_loss: 0.4136\n",
      "Epoch 76/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4185 - val_loss: 0.4135\n",
      "Epoch 77/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4177 - val_loss: 0.4128\n",
      "Epoch 78/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4169 - val_loss: 0.4120\n",
      "Epoch 79/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4161 - val_loss: 0.4116\n",
      "Epoch 80/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4155 - val_loss: 0.4110\n",
      "Epoch 81/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4147 - val_loss: 0.4108\n",
      "Epoch 82/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4140 - val_loss: 0.4097\n",
      "Epoch 83/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4132 - val_loss: 0.4088\n",
      "Epoch 84/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4126 - val_loss: 0.4085\n",
      "Epoch 85/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4120 - val_loss: 0.4084\n",
      "Epoch 86/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4111 - val_loss: 0.4070\n",
      "Epoch 87/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4105 - val_loss: 0.4065\n",
      "Epoch 88/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4098 - val_loss: 0.4066\n",
      "Epoch 89/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4093 - val_loss: 0.4057\n",
      "Epoch 90/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4087 - val_loss: 0.4053\n",
      "Epoch 91/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4080 - val_loss: 0.4049\n",
      "Epoch 92/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4073 - val_loss: 0.4044\n",
      "Epoch 93/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4067 - val_loss: 0.4034\n",
      "Epoch 94/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4060 - val_loss: 0.4037\n",
      "Epoch 95/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4053 - val_loss: 0.4036\n",
      "Epoch 96/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4050 - val_loss: 0.4023\n",
      "Epoch 97/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4042 - val_loss: 0.4021\n",
      "Epoch 98/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4037 - val_loss: 0.4012\n",
      "Epoch 99/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4032 - val_loss: 0.4010\n",
      "Epoch 100/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4025 - val_loss: 0.4005\n",
      "3870/3870 [==============================] - 0s 28us/sample - loss: 0.3948\n",
      "[CV]  learning_rate=0.0006154014789262348, n_hidden=2, n_neurons=87, total=  46.8s\n",
      "[CV] learning_rate=0.0006154014789262348, n_hidden=2, n_neurons=87 ...\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 84us/sample - loss: 2.8178 - val_loss: 1.3219\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 1.1650 - val_loss: 0.8879\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.8416 - val_loss: 0.7473\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.7428 - val_loss: 0.6870\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.6979 - val_loss: 0.6523\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.6701 - val_loss: 0.6292\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.6495 - val_loss: 0.6111\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.6323 - val_loss: 0.5954\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.6169 - val_loss: 0.5819\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.6031 - val_loss: 0.5693\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5902 - val_loss: 0.5589\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.5791 - val_loss: 0.5475\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.5684 - val_loss: 0.5375\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.5585 - val_loss: 0.5290\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.5497 - val_loss: 0.5205\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5412 - val_loss: 0.5129\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.5333 - val_loss: 0.5062\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.5260 - val_loss: 0.4998\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5194 - val_loss: 0.4936\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5132 - val_loss: 0.4885\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.5074 - val_loss: 0.4840\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5022 - val_loss: 0.4791\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4972 - val_loss: 0.4754\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4929 - val_loss: 0.4714\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4886 - val_loss: 0.4677\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4847 - val_loss: 0.4645\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4808 - val_loss: 0.4619\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4779 - val_loss: 0.4590\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4746 - val_loss: 0.4566\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4717 - val_loss: 0.4543\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4688 - val_loss: 0.4524\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4662 - val_loss: 0.4504\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4637 - val_loss: 0.4483\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4614 - val_loss: 0.4467\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4591 - val_loss: 0.4446\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4570 - val_loss: 0.4428\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4548 - val_loss: 0.4414\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4529 - val_loss: 0.4398\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4511 - val_loss: 0.4388\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4493 - val_loss: 0.4370\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4476 - val_loss: 0.4358\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4459 - val_loss: 0.4349\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4444 - val_loss: 0.4337\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4430 - val_loss: 0.4324\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4414 - val_loss: 0.4315\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4399 - val_loss: 0.4306\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4386 - val_loss: 0.4293\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4371 - val_loss: 0.4284\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4357 - val_loss: 0.4269\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4346 - val_loss: 0.4258\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4332 - val_loss: 0.4254\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4321 - val_loss: 0.4237\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4308 - val_loss: 0.4235\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4299 - val_loss: 0.4224\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4285 - val_loss: 0.4216\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4276 - val_loss: 0.4213\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4266 - val_loss: 0.4203\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4253 - val_loss: 0.4189\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4246 - val_loss: 0.4180\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4234 - val_loss: 0.4177\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4225 - val_loss: 0.4166\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4216 - val_loss: 0.4159\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4207 - val_loss: 0.4151\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4196 - val_loss: 0.4147\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4188 - val_loss: 0.4141\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4179 - val_loss: 0.4131\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4171 - val_loss: 0.4120\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4162 - val_loss: 0.4119\n",
      "Epoch 69/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4154 - val_loss: 0.4114\n",
      "Epoch 70/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4146 - val_loss: 0.4105\n",
      "Epoch 71/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4138 - val_loss: 0.4097\n",
      "Epoch 72/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4130 - val_loss: 0.4090\n",
      "Epoch 73/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4122 - val_loss: 0.4089\n",
      "Epoch 74/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4115 - val_loss: 0.4081\n",
      "Epoch 75/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4108 - val_loss: 0.4075\n",
      "Epoch 76/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4100 - val_loss: 0.4066\n",
      "Epoch 77/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4094 - val_loss: 0.4060\n",
      "Epoch 78/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4086 - val_loss: 0.4053\n",
      "Epoch 79/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4079 - val_loss: 0.4050\n",
      "Epoch 80/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4071 - val_loss: 0.4053\n",
      "Epoch 81/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4066 - val_loss: 0.4044\n",
      "Epoch 82/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4059 - val_loss: 0.4037\n",
      "Epoch 83/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4052 - val_loss: 0.4035\n",
      "Epoch 84/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4047 - val_loss: 0.4027\n",
      "Epoch 85/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4041 - val_loss: 0.4019\n",
      "Epoch 86/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4033 - val_loss: 0.4014\n",
      "Epoch 87/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4026 - val_loss: 0.4005\n",
      "Epoch 88/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4020 - val_loss: 0.4004\n",
      "Epoch 89/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4013 - val_loss: 0.3993\n",
      "Epoch 90/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4008 - val_loss: 0.3993\n",
      "Epoch 91/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4003 - val_loss: 0.3989\n",
      "Epoch 92/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3997 - val_loss: 0.3988\n",
      "Epoch 93/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3989 - val_loss: 0.3979\n",
      "Epoch 94/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3985 - val_loss: 0.3971\n",
      "Epoch 95/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3977 - val_loss: 0.3979\n",
      "Epoch 96/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3972 - val_loss: 0.3963\n",
      "Epoch 97/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3967 - val_loss: 0.3962\n",
      "Epoch 98/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3962 - val_loss: 0.3956\n",
      "Epoch 99/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3955 - val_loss: 0.3954\n",
      "Epoch 100/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3950 - val_loss: 0.3947\n",
      "3870/3870 [==============================] - 0s 29us/sample - loss: 0.3956\n",
      "[CV]  learning_rate=0.0006154014789262348, n_hidden=2, n_neurons=87, total=  45.4s\n",
      "[CV] learning_rate=0.0003920021771415983, n_hidden=3, n_neurons=24 ...\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 90us/sample - loss: 3.6045 - val_loss: 2.2656\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 1.9578 - val_loss: 1.6413\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 1.4806 - val_loss: 1.3663\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 1.2363 - val_loss: 1.1811\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 1.0691 - val_loss: 1.0372\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.9446 - val_loss: 0.9253\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.8522 - val_loss: 0.8394\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.7840 - val_loss: 0.7759\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 65us/sample - loss: 0.7350 - val_loss: 0.7301\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.7002 - val_loss: 0.6972\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6752 - val_loss: 0.6735\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6570 - val_loss: 0.6562\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6431 - val_loss: 0.6420\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.6317 - val_loss: 0.6308\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6220 - val_loss: 0.6216\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6139 - val_loss: 0.6133\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6062 - val_loss: 0.6059\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5992 - val_loss: 0.5989\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5929 - val_loss: 0.5928\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5865 - val_loss: 0.5868\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5808 - val_loss: 0.5825\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5756 - val_loss: 0.5767\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5707 - val_loss: 0.5718\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5658 - val_loss: 0.5674\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5613 - val_loss: 0.5631\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5569 - val_loss: 0.5596\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5529 - val_loss: 0.5549\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5489 - val_loss: 0.5512\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5453 - val_loss: 0.5479\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.5417 - val_loss: 0.5443\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5380 - val_loss: 0.5414\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5347 - val_loss: 0.5379\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5316 - val_loss: 0.5353\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5285 - val_loss: 0.5319\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5257 - val_loss: 0.5288\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5230 - val_loss: 0.5267\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5202 - val_loss: 0.5241\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5176 - val_loss: 0.5214\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5152 - val_loss: 0.5186\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5126 - val_loss: 0.5164\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5101 - val_loss: 0.5142\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.5083 - val_loss: 0.5122\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.5059 - val_loss: 0.5103\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5038 - val_loss: 0.5079\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5018 - val_loss: 0.5062\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4998 - val_loss: 0.5043\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4978 - val_loss: 0.5031\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4959 - val_loss: 0.5006\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4942 - val_loss: 0.4993\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4924 - val_loss: 0.4977\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4908 - val_loss: 0.4957\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4890 - val_loss: 0.4947\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4876 - val_loss: 0.4929\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4859 - val_loss: 0.4915\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4844 - val_loss: 0.4898\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4829 - val_loss: 0.4882\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4814 - val_loss: 0.4869\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4800 - val_loss: 0.4854\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4786 - val_loss: 0.4843\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4772 - val_loss: 0.4828\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4758 - val_loss: 0.4819\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4745 - val_loss: 0.4805\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4733 - val_loss: 0.4792\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4720 - val_loss: 0.4783\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4705 - val_loss: 0.4773\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4695 - val_loss: 0.4760\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4682 - val_loss: 0.4749\n",
      "Epoch 68/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4668 - val_loss: 0.4734\n",
      "Epoch 69/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4656 - val_loss: 0.4735\n",
      "Epoch 70/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4645 - val_loss: 0.4716\n",
      "Epoch 71/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4632 - val_loss: 0.4709\n",
      "Epoch 72/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4623 - val_loss: 0.4688\n",
      "Epoch 73/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4611 - val_loss: 0.4684\n",
      "Epoch 74/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4601 - val_loss: 0.4673\n",
      "Epoch 75/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.4587 - val_loss: 0.4681\n",
      "Epoch 76/100\n",
      "7740/7740 [==============================] - ETA: 0s - loss: 0.460 - 0s 62us/sample - loss: 0.4580 - val_loss: 0.4653\n",
      "Epoch 77/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.4569 - val_loss: 0.4637\n",
      "Epoch 78/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4559 - val_loss: 0.4628\n",
      "Epoch 79/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4548 - val_loss: 0.4619\n",
      "Epoch 80/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4537 - val_loss: 0.4619\n",
      "Epoch 81/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4528 - val_loss: 0.4606\n",
      "Epoch 82/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4517 - val_loss: 0.4592\n",
      "Epoch 83/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4508 - val_loss: 0.4583\n",
      "Epoch 84/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4499 - val_loss: 0.4574\n",
      "Epoch 85/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4488 - val_loss: 0.4568\n",
      "Epoch 86/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4479 - val_loss: 0.4559\n",
      "Epoch 87/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4470 - val_loss: 0.4552\n",
      "Epoch 88/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4461 - val_loss: 0.4540\n",
      "Epoch 89/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4450 - val_loss: 0.4525\n",
      "Epoch 90/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4443 - val_loss: 0.4524\n",
      "Epoch 91/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4435 - val_loss: 0.4517\n",
      "Epoch 92/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4425 - val_loss: 0.4503\n",
      "Epoch 93/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4416 - val_loss: 0.4497\n",
      "Epoch 94/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4411 - val_loss: 0.4494\n",
      "Epoch 95/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4401 - val_loss: 0.4486\n",
      "Epoch 96/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4391 - val_loss: 0.4485\n",
      "Epoch 97/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: 0.4388 - val_loss: 0.4468\n",
      "Epoch 98/100\n",
      "7740/7740 [==============================] - 1s 71us/sample - loss: 0.4375 - val_loss: 0.4456\n",
      "Epoch 99/100\n",
      "7740/7740 [==============================] - 1s 67us/sample - loss: 0.4365 - val_loss: 0.4469\n",
      "Epoch 100/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4362 - val_loss: 0.4446\n",
      "3870/3870 [==============================] - 0s 31us/sample - loss: 0.4836\n",
      "[CV]  learning_rate=0.0003920021771415983, n_hidden=3, n_neurons=24, total=  47.8s\n",
      "[CV] learning_rate=0.0003920021771415983, n_hidden=3, n_neurons=24 ...\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 92us/sample - loss: 3.6986 - val_loss: 2.4436\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 1.8526 - val_loss: 1.4342\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 1.2757 - val_loss: 1.1117\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 1.0325 - val_loss: 0.9303\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.8837 - val_loss: 0.8119\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.7890 - val_loss: 0.7337\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.7284 - val_loss: 0.6828\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.6886 - val_loss: 0.6482\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 1s 67us/sample - loss: 0.6612 - val_loss: 0.6236\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.6407 - val_loss: 0.6046\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.6242 - val_loss: 0.5897\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6103 - val_loss: 0.5766\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5979 - val_loss: 0.5648\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.5870 - val_loss: 0.5538\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.5767 - val_loss: 0.5441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5673 - val_loss: 0.5352\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.5587 - val_loss: 0.5271\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.5506 - val_loss: 0.5193\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5433 - val_loss: 0.5124\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5365 - val_loss: 0.5062\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5303 - val_loss: 0.5009\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5248 - val_loss: 0.4956\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5199 - val_loss: 0.4912\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5152 - val_loss: 0.4870\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5112 - val_loss: 0.4834\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5074 - val_loss: 0.4803\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5041 - val_loss: 0.4771\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5012 - val_loss: 0.4747\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4985 - val_loss: 0.4723\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4960 - val_loss: 0.4700\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4936 - val_loss: 0.4682\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4914 - val_loss: 0.4662\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4896 - val_loss: 0.4645\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4876 - val_loss: 0.4628\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4858 - val_loss: 0.4613\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4843 - val_loss: 0.4600\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4827 - val_loss: 0.4589\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4812 - val_loss: 0.4576\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4797 - val_loss: 0.4560\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4785 - val_loss: 0.4550\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4772 - val_loss: 0.4539\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4761 - val_loss: 0.4528\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: 0.4749 - val_loss: 0.4521\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4738 - val_loss: 0.4509\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4727 - val_loss: 0.4500\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4717 - val_loss: 0.4491\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4704 - val_loss: 0.4487\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4695 - val_loss: 0.4477\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4686 - val_loss: 0.4467\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4675 - val_loss: 0.4457\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4667 - val_loss: 0.4448\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4657 - val_loss: 0.4442\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4649 - val_loss: 0.4435\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4640 - val_loss: 0.4425\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4630 - val_loss: 0.4418\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4623 - val_loss: 0.4412\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4615 - val_loss: 0.4405\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4607 - val_loss: 0.4398\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4599 - val_loss: 0.4394\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4591 - val_loss: 0.4386\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4584 - val_loss: 0.4381\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4577 - val_loss: 0.4375\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4569 - val_loss: 0.4368\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4561 - val_loss: 0.4362\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4553 - val_loss: 0.4357\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4546 - val_loss: 0.4353\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4538 - val_loss: 0.4347\n",
      "Epoch 68/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4531 - val_loss: 0.4343\n",
      "Epoch 69/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4523 - val_loss: 0.4339\n",
      "Epoch 70/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4517 - val_loss: 0.4334\n",
      "Epoch 71/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4509 - val_loss: 0.4331\n",
      "Epoch 72/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4503 - val_loss: 0.4320\n",
      "Epoch 73/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4496 - val_loss: 0.4314\n",
      "Epoch 74/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4490 - val_loss: 0.4308\n",
      "Epoch 75/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4483 - val_loss: 0.4304\n",
      "Epoch 76/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4477 - val_loss: 0.4302\n",
      "Epoch 77/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: 0.4470 - val_loss: 0.4296\n",
      "Epoch 78/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4463 - val_loss: 0.4289\n",
      "Epoch 79/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4457 - val_loss: 0.4286\n",
      "Epoch 80/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4451 - val_loss: 0.4282\n",
      "Epoch 81/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4444 - val_loss: 0.4279\n",
      "Epoch 82/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4438 - val_loss: 0.4273\n",
      "Epoch 83/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4431 - val_loss: 0.4266\n",
      "Epoch 84/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4427 - val_loss: 0.4263\n",
      "Epoch 85/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4421 - val_loss: 0.4261\n",
      "Epoch 86/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4413 - val_loss: 0.4254\n",
      "Epoch 87/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4408 - val_loss: 0.4250\n",
      "Epoch 88/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4402 - val_loss: 0.4248\n",
      "Epoch 89/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4397 - val_loss: 0.4242\n",
      "Epoch 90/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4392 - val_loss: 0.4239\n",
      "Epoch 91/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4386 - val_loss: 0.4236\n",
      "Epoch 92/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4380 - val_loss: 0.4229\n",
      "Epoch 93/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4375 - val_loss: 0.4224\n",
      "Epoch 94/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4369 - val_loss: 0.4227\n",
      "Epoch 95/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4362 - val_loss: 0.4223\n",
      "Epoch 96/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4359 - val_loss: 0.4217\n",
      "Epoch 97/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4352 - val_loss: 0.4213\n",
      "Epoch 98/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4347 - val_loss: 0.4208\n",
      "Epoch 99/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4343 - val_loss: 0.4205\n",
      "Epoch 100/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4337 - val_loss: 0.4202\n",
      "3870/3870 [==============================] - 0s 30us/sample - loss: 0.4159\n",
      "[CV]  learning_rate=0.0003920021771415983, n_hidden=3, n_neurons=24, total=  47.8s\n",
      "[CV] learning_rate=0.0003920021771415983, n_hidden=3, n_neurons=24 ...\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 111us/sample - loss: 3.1764 - val_loss: 1.7021\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 1.2937 - val_loss: 0.9729\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.9376 - val_loss: 0.8150\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.8230 - val_loss: 0.7433\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.7621 - val_loss: 0.6966\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.7219 - val_loss: 0.6677\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6948 - val_loss: 0.6473\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6741 - val_loss: 0.6313\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.6575 - val_loss: 0.6182\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.6434 - val_loss: 0.6073\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6316 - val_loss: 0.5985\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6218 - val_loss: 0.5891\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6126 - val_loss: 0.5805\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6043 - val_loss: 0.5732\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5967 - val_loss: 0.5659\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5894 - val_loss: 0.5593\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5827 - val_loss: 0.5533\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5763 - val_loss: 0.5473\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5704 - val_loss: 0.5416\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5646 - val_loss: 0.5365\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5591 - val_loss: 0.5316\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5540 - val_loss: 0.5268\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5489 - val_loss: 0.5225\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5442 - val_loss: 0.5181\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.5395 - val_loss: 0.5138\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5351 - val_loss: 0.5098\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5306 - val_loss: 0.5064\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5268 - val_loss: 0.5028\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5230 - val_loss: 0.4992\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5193 - val_loss: 0.4960\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5158 - val_loss: 0.4930\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5123 - val_loss: 0.4903\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5092 - val_loss: 0.4872\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5061 - val_loss: 0.4846\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5031 - val_loss: 0.4818\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5003 - val_loss: 0.4793\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4974 - val_loss: 0.4770\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4946 - val_loss: 0.4747\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4921 - val_loss: 0.4724\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4896 - val_loss: 0.4701\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4872 - val_loss: 0.4682\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4849 - val_loss: 0.4662\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: 0.4826 - val_loss: 0.4643\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: 0.4805 - val_loss: 0.4625\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4783 - val_loss: 0.4609\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4763 - val_loss: 0.4594\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4743 - val_loss: 0.4575\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4723 - val_loss: 0.4560\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.4704 - val_loss: 0.4543\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4686 - val_loss: 0.4527\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4668 - val_loss: 0.4515\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4651 - val_loss: 0.4498\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4635 - val_loss: 0.4488\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4619 - val_loss: 0.4475\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4603 - val_loss: 0.4463\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4588 - val_loss: 0.4455\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4574 - val_loss: 0.4443\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4558 - val_loss: 0.4427\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4544 - val_loss: 0.4418\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4530 - val_loss: 0.4408\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4516 - val_loss: 0.4396\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4503 - val_loss: 0.4383\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4489 - val_loss: 0.4372\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4476 - val_loss: 0.4363\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4463 - val_loss: 0.4354\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4452 - val_loss: 0.4343\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4439 - val_loss: 0.4331\n",
      "Epoch 68/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4427 - val_loss: 0.4325\n",
      "Epoch 69/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4416 - val_loss: 0.4316\n",
      "Epoch 70/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4405 - val_loss: 0.4306\n",
      "Epoch 71/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4393 - val_loss: 0.4296\n",
      "Epoch 72/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4383 - val_loss: 0.4287\n",
      "Epoch 73/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4372 - val_loss: 0.4280\n",
      "Epoch 74/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4363 - val_loss: 0.4270\n",
      "Epoch 75/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4353 - val_loss: 0.4264\n",
      "Epoch 76/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4343 - val_loss: 0.4253\n",
      "Epoch 77/100\n",
      "7740/7740 [==============================] - 1s 66us/sample - loss: 0.4334 - val_loss: 0.4245\n",
      "Epoch 78/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4325 - val_loss: 0.4236\n",
      "Epoch 79/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4315 - val_loss: 0.4230\n",
      "Epoch 80/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4306 - val_loss: 0.4228\n",
      "Epoch 81/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4298 - val_loss: 0.4219\n",
      "Epoch 82/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4289 - val_loss: 0.4209\n",
      "Epoch 83/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4281 - val_loss: 0.4208\n",
      "Epoch 84/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4274 - val_loss: 0.4198\n",
      "Epoch 85/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4266 - val_loss: 0.4189\n",
      "Epoch 86/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4256 - val_loss: 0.4185\n",
      "Epoch 87/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4247 - val_loss: 0.4173\n",
      "Epoch 88/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4240 - val_loss: 0.4169\n",
      "Epoch 89/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4232 - val_loss: 0.4161\n",
      "Epoch 90/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4226 - val_loss: 0.4158\n",
      "Epoch 91/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4218 - val_loss: 0.4152\n",
      "Epoch 92/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4211 - val_loss: 0.4151\n",
      "Epoch 93/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4204 - val_loss: 0.4141\n",
      "Epoch 94/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4197 - val_loss: 0.4133\n",
      "Epoch 95/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4190 - val_loss: 0.4135\n",
      "Epoch 96/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4183 - val_loss: 0.4123\n",
      "Epoch 97/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4177 - val_loss: 0.4119\n",
      "Epoch 98/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4171 - val_loss: 0.4113\n",
      "Epoch 99/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4164 - val_loss: 0.4110\n",
      "Epoch 100/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4157 - val_loss: 0.4105\n",
      "3870/3870 [==============================] - 0s 30us/sample - loss: 0.4081\n",
      "[CV]  learning_rate=0.0003920021771415983, n_hidden=3, n_neurons=24, total=  47.8s\n",
      "[CV] learning_rate=0.006010328378268217, n_hidden=0, n_neurons=2 .....\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 75us/sample - loss: 1.9497 - val_loss: 0.5877\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5882 - val_loss: 0.5895\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5579 - val_loss: 0.5371\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5394 - val_loss: 0.5342\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5378 - val_loss: 0.5337\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5384 - val_loss: 0.5266\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5561 - val_loss: 0.5701\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.6024 - val_loss: 0.5302\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5496 - val_loss: 0.5640\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5599 - val_loss: 0.5451\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5277 - val_loss: 0.5180\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5414 - val_loss: 0.5258\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5235 - val_loss: 0.5193\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5649 - val_loss: 0.5173\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5325 - val_loss: 0.5665\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5348 - val_loss: 0.5428\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5498 - val_loss: 0.5276\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5317 - val_loss: 0.5164\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5713 - val_loss: 0.5187\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5371 - val_loss: 0.5222\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5870 - val_loss: 0.5480\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5715 - val_loss: 0.5269\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5425 - val_loss: 0.5424\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5206 - val_loss: 0.5175\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5227 - val_loss: 0.5526\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5783 - val_loss: 0.5253\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5469 - val_loss: 0.5174\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5649 - val_loss: 0.5212\n",
      "3870/3870 [==============================] - 0s 26us/sample - loss: 0.5651\n",
      "[CV]  learning_rate=0.006010328378268217, n_hidden=0, n_neurons=2, total=  12.1s\n",
      "[CV] learning_rate=0.006010328378268217, n_hidden=0, n_neurons=2 .....\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 75us/sample - loss: 1.7310 - val_loss: 0.5963\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5929 - val_loss: 0.5622\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5728 - val_loss: 0.5462\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5642 - val_loss: 0.5396\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5614 - val_loss: 0.5356\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5593 - val_loss: 0.5277\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5725 - val_loss: 0.5556\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5797 - val_loss: 0.5280\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5604 - val_loss: 0.5437\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5679 - val_loss: 0.5389\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5573 - val_loss: 0.5239\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5594 - val_loss: 0.5252\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5552 - val_loss: 0.5243\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5704 - val_loss: 0.5236\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5544 - val_loss: 0.5336\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5599 - val_loss: 0.5365\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5596 - val_loss: 0.5239\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5642 - val_loss: 0.5205\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5845 - val_loss: 0.5221\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5620 - val_loss: 0.5214\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5930 - val_loss: 0.5259\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5632 - val_loss: 0.5229\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5535 - val_loss: 0.5515\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5489 - val_loss: 0.5195\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5575 - val_loss: 0.5371\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5832 - val_loss: 0.5230\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5707 - val_loss: 0.5214\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5849 - val_loss: 0.5231\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5666 - val_loss: 0.5229\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5642 - val_loss: 0.5323\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5835 - val_loss: 0.5937\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5898 - val_loss: 0.5463\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5669 - val_loss: 0.5489\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5566 - val_loss: 0.5216\n",
      "3870/3870 [==============================] - 0s 26us/sample - loss: 0.5221\n",
      "[CV]  learning_rate=0.006010328378268217, n_hidden=0, n_neurons=2, total=  14.4s\n",
      "[CV] learning_rate=0.006010328378268217, n_hidden=0, n_neurons=2 .....\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 74us/sample - loss: 2.1880 - val_loss: 0.9718\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 1.6625 - val_loss: 2.6979\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 6.6162 - val_loss: 13.1450\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 34.4843 - val_loss: 70.9778\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 195.6625 - val_loss: 394.3004\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 1141.0096 - val_loss: 2151.2582\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 6496.7350 - val_loss: 12087.0815\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 36422.2432 - val_loss: 68087.5554\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 199639.4648 - val_loss: 385015.9009\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 1134481.6582 - val_loss: 2154398.2483\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 6345688.3628 - val_loss: 12118039.7328\n",
      "3870/3870 [==============================] - 0s 26us/sample - loss: 18935417.5716\n",
      "[CV]  learning_rate=0.006010328378268217, n_hidden=0, n_neurons=2, total=   4.8s\n",
      "[CV] learning_rate=0.008339092654580042, n_hidden=1, n_neurons=38 ....\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 78us/sample - loss: 1.2403 - val_loss: 0.8600\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.7412 - val_loss: 0.5610\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5304 - val_loss: 0.5125\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4921 - val_loss: 0.4832\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4732 - val_loss: 0.4643\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4578 - val_loss: 0.4542\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4493 - val_loss: 0.4490\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4431 - val_loss: 0.4420\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4344 - val_loss: 0.4374\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4311 - val_loss: 0.4371\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4262 - val_loss: 0.4299\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4194 - val_loss: 0.4256\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4164 - val_loss: 0.4197\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4116 - val_loss: 0.4229\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4087 - val_loss: 0.4166\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4053 - val_loss: 0.4151\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4023 - val_loss: 0.4066\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4007 - val_loss: 0.4051\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3973 - val_loss: 0.4031\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3938 - val_loss: 0.4012\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3931 - val_loss: 0.4040\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3915 - val_loss: 0.4069\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3896 - val_loss: 0.3991\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3870 - val_loss: 0.3964\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3844 - val_loss: 0.3969\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3823 - val_loss: 0.3926\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3817 - val_loss: 0.3908\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3782 - val_loss: 0.3879\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3769 - val_loss: 0.3868\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3755 - val_loss: 0.3838\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3740 - val_loss: 0.3945\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3814 - val_loss: 0.3898\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3720 - val_loss: 0.3824\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3701 - val_loss: 0.3803\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3687 - val_loss: 0.3962\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3692 - val_loss: 0.3829\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3684 - val_loss: 0.3758\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3678 - val_loss: 0.3801\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3640 - val_loss: 0.3744\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3648 - val_loss: 0.3882\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3722 - val_loss: 0.3844\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3614 - val_loss: 0.3847\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3597 - val_loss: 0.3728\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3594 - val_loss: 0.3727\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3583 - val_loss: 0.3748\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3671 - val_loss: 0.3749\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3566 - val_loss: 0.3739\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3558 - val_loss: 0.3717\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3536 - val_loss: 0.3727\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3590 - val_loss: 0.3684\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3528 - val_loss: 0.3785\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3524 - val_loss: 0.3692\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3533 - val_loss: 0.3667\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3500 - val_loss: 0.3714\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3526 - val_loss: 0.3642\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3498 - val_loss: 0.3770\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3498 - val_loss: 0.3678\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3494 - val_loss: 0.3656\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3630 - val_loss: 0.3810\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3551 - val_loss: 0.3643\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3490 - val_loss: 0.3621\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3497 - val_loss: 0.3684\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3455 - val_loss: 0.3719\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3465 - val_loss: 0.3623\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3474 - val_loss: 0.3723\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3439 - val_loss: 0.3634\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3432 - val_loss: 0.3650\n",
      "Epoch 68/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3529 - val_loss: 0.3603\n",
      "Epoch 69/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3421 - val_loss: 0.3668\n",
      "Epoch 70/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3416 - val_loss: 0.3702\n",
      "Epoch 71/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3443 - val_loss: 0.3568\n",
      "Epoch 72/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3426 - val_loss: 0.3561\n",
      "Epoch 73/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3542 - val_loss: 0.3642\n",
      "Epoch 74/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3509 - val_loss: 0.3656\n",
      "Epoch 75/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3466 - val_loss: 0.3948\n",
      "Epoch 76/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3458 - val_loss: 0.3572\n",
      "Epoch 77/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3650 - val_loss: 0.3579\n",
      "Epoch 78/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3418 - val_loss: 0.3692\n",
      "Epoch 79/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3565 - val_loss: 0.3635\n",
      "Epoch 80/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3442 - val_loss: 0.3658\n",
      "Epoch 81/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3439 - val_loss: 0.3637\n",
      "Epoch 82/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3419 - val_loss: 0.3691\n",
      "3870/3870 [==============================] - 0s 28us/sample - loss: 0.3946\n",
      "[CV]  learning_rate=0.008339092654580042, n_hidden=1, n_neurons=38, total=  35.2s\n",
      "[CV] learning_rate=0.008339092654580042, n_hidden=1, n_neurons=38 ....\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 78us/sample - loss: 0.9207 - val_loss: 0.6292\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.6300 - val_loss: 0.5238\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5176 - val_loss: 0.4851\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5120 - val_loss: 0.4746\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4877 - val_loss: 0.4699\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4689 - val_loss: 0.4400\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4744 - val_loss: 1.3593\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5015 - val_loss: 0.4312\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4626 - val_loss: 0.4416\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4701 - val_loss: 0.4515\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4465 - val_loss: 0.4316\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4344 - val_loss: 0.4230\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4287 - val_loss: 0.4234\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4318 - val_loss: 0.4165\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4252 - val_loss: 0.4118\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4192 - val_loss: 0.4055\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4149 - val_loss: 0.4038\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4106 - val_loss: 0.4021\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4080 - val_loss: 0.3969\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4055 - val_loss: 0.3932\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4177 - val_loss: 0.3944\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4167 - val_loss: 0.3987\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4044 - val_loss: 0.3966\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4232 - val_loss: 0.3886\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4197 - val_loss: 0.4041\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4114 - val_loss: 0.3918\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3933 - val_loss: 0.3840\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3909 - val_loss: 0.3836\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3882 - val_loss: 0.3850\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3872 - val_loss: 0.3781\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3843 - val_loss: 0.3815\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3908 - val_loss: 0.3843\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3810 - val_loss: 0.3745\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3795 - val_loss: 0.3731\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3818 - val_loss: 0.3745\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3874 - val_loss: 0.4229\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4132 - val_loss: 0.3739\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3790 - val_loss: 0.3744\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3737 - val_loss: 0.3666\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3713 - val_loss: 0.3647\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3729 - val_loss: 0.3629\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3728 - val_loss: 0.3639\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3757 - val_loss: 0.3607\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3681 - val_loss: 0.3673\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3936 - val_loss: 0.4554\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4678 - val_loss: 0.3741\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4174 - val_loss: 0.3731\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3707 - val_loss: 0.3676\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3667 - val_loss: 0.3608\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3640 - val_loss: 0.3650\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3623 - val_loss: 0.3651\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3625 - val_loss: 0.3754\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3675 - val_loss: 0.3580\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3619 - val_loss: 0.3590\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3577 - val_loss: 0.3527\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3565 - val_loss: 0.3602\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3549 - val_loss: 0.3547\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3564 - val_loss: 0.3501\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3756 - val_loss: 0.3609\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3651 - val_loss: 0.3543\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3582 - val_loss: 0.3538\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3541 - val_loss: 0.3524\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3525 - val_loss: 0.3487\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3507 - val_loss: 0.3497\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3516 - val_loss: 0.3499\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3495 - val_loss: 0.3486\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3474 - val_loss: 0.3478\n",
      "Epoch 68/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3529 - val_loss: 0.3504\n",
      "Epoch 69/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3460 - val_loss: 0.3485\n",
      "Epoch 70/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3460 - val_loss: 0.3451\n",
      "Epoch 71/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3450 - val_loss: 0.3499\n",
      "Epoch 72/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3446 - val_loss: 0.3441\n",
      "Epoch 73/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3579 - val_loss: 0.3510\n",
      "Epoch 74/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3451 - val_loss: 0.3472\n",
      "Epoch 75/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3525 - val_loss: 0.5041\n",
      "Epoch 76/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4000 - val_loss: 0.3694\n",
      "Epoch 77/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3535 - val_loss: 0.3547\n",
      "Epoch 78/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3485 - val_loss: 0.3563\n",
      "Epoch 79/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3571 - val_loss: 0.3537\n",
      "Epoch 80/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3421 - val_loss: 0.3403\n",
      "Epoch 81/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3413 - val_loss: 0.3525\n",
      "Epoch 82/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3405 - val_loss: 0.3424\n",
      "Epoch 83/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3408 - val_loss: 0.3545\n",
      "Epoch 84/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3543 - val_loss: 0.3376\n",
      "Epoch 85/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3445 - val_loss: 0.3518\n",
      "Epoch 86/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3428 - val_loss: 0.3375\n",
      "Epoch 87/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3384 - val_loss: 0.3563\n",
      "Epoch 88/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3527 - val_loss: 0.3421\n",
      "Epoch 89/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3374 - val_loss: 0.3449\n",
      "Epoch 90/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3387 - val_loss: 0.3404\n",
      "Epoch 91/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3371 - val_loss: 0.3472\n",
      "Epoch 92/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3402 - val_loss: 0.3404\n",
      "Epoch 93/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3405 - val_loss: 0.3543\n",
      "Epoch 94/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3400 - val_loss: 0.3376\n",
      "Epoch 95/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3395 - val_loss: 0.3697\n",
      "Epoch 96/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3374 - val_loss: 0.3345\n",
      "Epoch 97/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3428 - val_loss: 0.3510\n",
      "Epoch 98/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3402 - val_loss: 0.3380\n",
      "Epoch 99/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3342 - val_loss: 0.3465\n",
      "Epoch 100/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3388 - val_loss: 0.3338\n",
      "3870/3870 [==============================] - 0s 27us/sample - loss: 0.3784\n",
      "[CV]  learning_rate=0.008339092654580042, n_hidden=1, n_neurons=38, total=  42.9s\n",
      "[CV] learning_rate=0.008339092654580042, n_hidden=1, n_neurons=38 ....\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 1s 79us/sample - loss: 0.9976 - val_loss: 0.6117\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.6886 - val_loss: 0.9469\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 1.7799 - val_loss: 1.3336\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5729 - val_loss: 0.4856\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4720 - val_loss: 0.4368\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4521 - val_loss: 0.4274\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4424 - val_loss: 0.4209\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4338 - val_loss: 0.4156\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4265 - val_loss: 0.4200\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4204 - val_loss: 0.4099\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4166 - val_loss: 0.4141\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4128 - val_loss: 0.4015\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4094 - val_loss: 0.4059\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4058 - val_loss: 0.3969\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4018 - val_loss: 0.3948\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4002 - val_loss: 0.3996\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3971 - val_loss: 0.3941\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3924 - val_loss: 0.3911\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3912 - val_loss: 0.3892\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3892 - val_loss: 0.3846\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3864 - val_loss: 0.3892\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3849 - val_loss: 0.3802\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3827 - val_loss: 0.3788\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3788 - val_loss: 0.3756\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3781 - val_loss: 0.3744\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3768 - val_loss: 0.3714\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3745 - val_loss: 0.3756\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3733 - val_loss: 0.3724\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3724 - val_loss: 0.3686\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3689 - val_loss: 0.3643\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3679 - val_loss: 0.3671\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3676 - val_loss: 0.3626\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3653 - val_loss: 0.3638\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3632 - val_loss: 0.3653\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3608 - val_loss: 0.3571\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3612 - val_loss: 0.3630\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3612 - val_loss: 0.3552\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3576 - val_loss: 0.3592\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3565 - val_loss: 0.3580\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3559 - val_loss: 0.3553\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3576 - val_loss: 0.3514\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3538 - val_loss: 0.3526\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3524 - val_loss: 0.3525\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3511 - val_loss: 0.3492\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3489 - val_loss: 0.3504\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3496 - val_loss: 0.3480\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3478 - val_loss: 0.3537\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3549 - val_loss: 0.3470\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3458 - val_loss: 0.3437\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3460 - val_loss: 0.3453\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3437 - val_loss: 0.3457\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3455 - val_loss: 0.3419\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3412 - val_loss: 0.3434\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3424 - val_loss: 0.3427\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3600 - val_loss: 0.3430\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3425 - val_loss: 0.3462\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3396 - val_loss: 0.3434\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3404 - val_loss: 0.3366\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3424 - val_loss: 0.3403\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3401 - val_loss: 0.3398\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3365 - val_loss: 0.3484\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3366 - val_loss: 0.3379\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3353 - val_loss: 0.3400\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3356 - val_loss: 0.3393\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3342 - val_loss: 0.3426\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3463 - val_loss: 0.3375\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3342 - val_loss: 0.3437\n",
      "Epoch 68/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3343 - val_loss: 0.3329\n",
      "Epoch 69/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3358 - val_loss: 0.3303\n",
      "Epoch 70/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3379 - val_loss: 0.3323\n",
      "Epoch 71/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3381 - val_loss: 0.3388\n",
      "Epoch 72/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3320 - val_loss: 0.3304\n",
      "Epoch 73/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3300 - val_loss: 0.3513\n",
      "Epoch 74/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3300 - val_loss: 0.3334\n",
      "Epoch 75/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3302 - val_loss: 0.3274\n",
      "Epoch 76/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3298 - val_loss: 0.3470\n",
      "Epoch 77/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3331 - val_loss: 0.3378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3267 - val_loss: 0.3345\n",
      "Epoch 79/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3275 - val_loss: 0.3308\n",
      "Epoch 80/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3257 - val_loss: 0.3272\n",
      "Epoch 81/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3250 - val_loss: 0.3325\n",
      "Epoch 82/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3307 - val_loss: 0.3268\n",
      "Epoch 83/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3249 - val_loss: 0.3316\n",
      "Epoch 84/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3256 - val_loss: 0.3232\n",
      "Epoch 85/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3244 - val_loss: 0.3372\n",
      "Epoch 86/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3234 - val_loss: 0.3257\n",
      "Epoch 87/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3223 - val_loss: 0.3222\n",
      "Epoch 88/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3218 - val_loss: 0.3277\n",
      "Epoch 89/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3250 - val_loss: 0.3236\n",
      "Epoch 90/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3209 - val_loss: 0.3237\n",
      "Epoch 91/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3217 - val_loss: 0.3228\n",
      "Epoch 92/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3235 - val_loss: 0.3264\n",
      "Epoch 93/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3207 - val_loss: 0.3205\n",
      "Epoch 94/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3270 - val_loss: 0.3202\n",
      "Epoch 95/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3215 - val_loss: 0.3254\n",
      "Epoch 96/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3215 - val_loss: 0.3212\n",
      "Epoch 97/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3200 - val_loss: 0.3216\n",
      "Epoch 98/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3193 - val_loss: 0.3227\n",
      "Epoch 99/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3223 - val_loss: 0.3212\n",
      "Epoch 100/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3241 - val_loss: 0.3215\n",
      "3870/3870 [==============================] - 0s 28us/sample - loss: 0.3179\n",
      "[CV]  learning_rate=0.008339092654580042, n_hidden=1, n_neurons=38, total=  43.0s\n",
      "[CV] learning_rate=0.00030107783636342726, n_hidden=3, n_neurons=21 ..\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 88us/sample - loss: 4.8487 - val_loss: 3.2705\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 2.3599 - val_loss: 1.6378\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 1.3064 - val_loss: 1.0556\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.9899 - val_loss: 0.8915\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.8758 - val_loss: 0.8157\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.8077 - val_loss: 0.7641\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.7603 - val_loss: 0.7286\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.7269 - val_loss: 0.7009\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.7022 - val_loss: 0.6811\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6833 - val_loss: 0.6645\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.6685 - val_loss: 0.6514\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6565 - val_loss: 0.6410\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6463 - val_loss: 0.6316\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6375 - val_loss: 0.6236\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6295 - val_loss: 0.6166\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6222 - val_loss: 0.6099\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6155 - val_loss: 0.6038\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6092 - val_loss: 0.5980\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6033 - val_loss: 0.5927\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5975 - val_loss: 0.5876\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5919 - val_loss: 0.5831\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5868 - val_loss: 0.5782\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5817 - val_loss: 0.5736\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5768 - val_loss: 0.5693\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5721 - val_loss: 0.5651\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 1s 65us/sample - loss: 0.5675 - val_loss: 0.5612\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5631 - val_loss: 0.5570\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5588 - val_loss: 0.5532\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5547 - val_loss: 0.5496\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5507 - val_loss: 0.5460\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5469 - val_loss: 0.5426\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5432 - val_loss: 0.5393\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5396 - val_loss: 0.5361\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5361 - val_loss: 0.5329\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.5328 - val_loss: 0.5297\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.5296 - val_loss: 0.5269\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5264 - val_loss: 0.5240\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5234 - val_loss: 0.5212\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5205 - val_loss: 0.5185\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5177 - val_loss: 0.5159\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5149 - val_loss: 0.5135\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5124 - val_loss: 0.5111\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5098 - val_loss: 0.5089\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.5074 - val_loss: 0.5067\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5051 - val_loss: 0.5044\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5028 - val_loss: 0.5024\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5007 - val_loss: 0.5005\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4987 - val_loss: 0.4986\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4967 - val_loss: 0.4967\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4947 - val_loss: 0.4949\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4930 - val_loss: 0.4932\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4911 - val_loss: 0.4916\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4894 - val_loss: 0.4901\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.4878 - val_loss: 0.4885\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4862 - val_loss: 0.4871\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4847 - val_loss: 0.4857\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.4832 - val_loss: 0.4843\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4818 - val_loss: 0.4830\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4804 - val_loss: 0.4818\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: 0.4791 - val_loss: 0.4805\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4777 - val_loss: 0.4794\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4765 - val_loss: 0.4782\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4753 - val_loss: 0.4770\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4740 - val_loss: 0.4760\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4728 - val_loss: 0.4750\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4718 - val_loss: 0.4739\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4706 - val_loss: 0.4729\n",
      "Epoch 68/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4695 - val_loss: 0.4720\n",
      "Epoch 69/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4684 - val_loss: 0.4711\n",
      "Epoch 70/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4673 - val_loss: 0.4702\n",
      "Epoch 71/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4662 - val_loss: 0.4693\n",
      "Epoch 72/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4654 - val_loss: 0.4681\n",
      "Epoch 73/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4643 - val_loss: 0.4675\n",
      "Epoch 74/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4634 - val_loss: 0.4666\n",
      "Epoch 75/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4623 - val_loss: 0.4659\n",
      "Epoch 76/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4615 - val_loss: 0.4649\n",
      "Epoch 77/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4605 - val_loss: 0.4640\n",
      "Epoch 78/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4597 - val_loss: 0.4632\n",
      "Epoch 79/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4588 - val_loss: 0.4626\n",
      "Epoch 80/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4578 - val_loss: 0.4619\n",
      "Epoch 81/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4570 - val_loss: 0.4611\n",
      "Epoch 82/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4561 - val_loss: 0.4602\n",
      "Epoch 83/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4552 - val_loss: 0.4595\n",
      "Epoch 84/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4544 - val_loss: 0.4588\n",
      "Epoch 85/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4536 - val_loss: 0.4580\n",
      "Epoch 86/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4527 - val_loss: 0.4573\n",
      "Epoch 87/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4520 - val_loss: 0.4567\n",
      "Epoch 88/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4511 - val_loss: 0.4560\n",
      "Epoch 89/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4503 - val_loss: 0.4551\n",
      "Epoch 90/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4496 - val_loss: 0.4547\n",
      "Epoch 91/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4490 - val_loss: 0.4541\n",
      "Epoch 92/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4482 - val_loss: 0.4535\n",
      "Epoch 93/100\n",
      "7740/7740 [==============================] - 0s 65us/sample - loss: 0.4474 - val_loss: 0.4529\n",
      "Epoch 94/100\n",
      "7740/7740 [==============================] - 1s 67us/sample - loss: 0.4468 - val_loss: 0.4522\n",
      "Epoch 95/100\n",
      "7740/7740 [==============================] - 1s 68us/sample - loss: 0.4461 - val_loss: 0.4516\n",
      "Epoch 96/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4452 - val_loss: 0.4513\n",
      "Epoch 97/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4448 - val_loss: 0.4504\n",
      "Epoch 98/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4440 - val_loss: 0.4499\n",
      "Epoch 99/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4431 - val_loss: 0.4497\n",
      "Epoch 100/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4428 - val_loss: 0.4488\n",
      "3870/3870 [==============================] - 0s 29us/sample - loss: 0.4872\n",
      "[CV]  learning_rate=0.00030107783636342726, n_hidden=3, n_neurons=21, total=  47.8s\n",
      "[CV] learning_rate=0.00030107783636342726, n_hidden=3, n_neurons=21 ..\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 115us/sample - loss: 4.1179 - val_loss: 3.0440\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 2.3375 - val_loss: 1.8279\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 1.5685 - val_loss: 1.3710\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 1.2778 - val_loss: 1.1838\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 1.1287 - val_loss: 1.0684\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 1.0343 - val_loss: 0.9872\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.9708 - val_loss: 0.9294\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.9249 - val_loss: 0.8861\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.8903 - val_loss: 0.8532\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.8631 - val_loss: 0.8264\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.8414 - val_loss: 0.8052\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.8230 - val_loss: 0.7874\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.8074 - val_loss: 0.7716\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.7933 - val_loss: 0.7579\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.7806 - val_loss: 0.7453\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.7692 - val_loss: 0.7344\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.7587 - val_loss: 0.7244\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.7489 - val_loss: 0.7149\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.7402 - val_loss: 0.7064\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.7317 - val_loss: 0.6986\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.7237 - val_loss: 0.6915\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.7165 - val_loss: 0.6843\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.7095 - val_loss: 0.6778\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.7027 - val_loss: 0.6714\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6964 - val_loss: 0.6653\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 64us/sample - loss: 0.6901 - val_loss: 0.6597\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6842 - val_loss: 0.6540\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6785 - val_loss: 0.6485\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.6729 - val_loss: 0.6430\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6672 - val_loss: 0.6377\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.6615 - val_loss: 0.6327\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6561 - val_loss: 0.6276\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6509 - val_loss: 0.6231\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6458 - val_loss: 0.6181\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.6409 - val_loss: 0.6136\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6362 - val_loss: 0.6093\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6315 - val_loss: 0.6054\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6270 - val_loss: 0.6012\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.6227 - val_loss: 0.5968\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6184 - val_loss: 0.5928\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.6142 - val_loss: 0.5891\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6102 - val_loss: 0.5850\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6062 - val_loss: 0.5814\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.6022 - val_loss: 0.5776\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5984 - val_loss: 0.5738\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.5947 - val_loss: 0.5702\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.5908 - val_loss: 0.5672\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5873 - val_loss: 0.5635\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5837 - val_loss: 0.5600\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5803 - val_loss: 0.5568\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.5769 - val_loss: 0.5533\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5735 - val_loss: 0.5501\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5702 - val_loss: 0.5471\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5670 - val_loss: 0.5437\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.5638 - val_loss: 0.5406\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5607 - val_loss: 0.5377\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5576 - val_loss: 0.5348\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5546 - val_loss: 0.5318\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.5517 - val_loss: 0.5292\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 1s 66us/sample - loss: 0.5488 - val_loss: 0.5264\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5460 - val_loss: 0.5240\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.5433 - val_loss: 0.5215\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5406 - val_loss: 0.5189\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5381 - val_loss: 0.5164\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5355 - val_loss: 0.5144\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5331 - val_loss: 0.5119\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5306 - val_loss: 0.5099\n",
      "Epoch 68/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.5284 - val_loss: 0.5077\n",
      "Epoch 69/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5261 - val_loss: 0.5059\n",
      "Epoch 70/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5240 - val_loss: 0.5039\n",
      "Epoch 71/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5218 - val_loss: 0.5020\n",
      "Epoch 72/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.5197 - val_loss: 0.4997\n",
      "Epoch 73/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5177 - val_loss: 0.4978\n",
      "Epoch 74/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5159 - val_loss: 0.4962\n",
      "Epoch 75/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5139 - val_loss: 0.4946\n",
      "Epoch 76/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5120 - val_loss: 0.4930\n",
      "Epoch 77/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5102 - val_loss: 0.4912\n",
      "Epoch 78/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5084 - val_loss: 0.4895\n",
      "Epoch 79/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5067 - val_loss: 0.4881\n",
      "Epoch 80/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5050 - val_loss: 0.4866\n",
      "Epoch 81/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5033 - val_loss: 0.4853\n",
      "Epoch 82/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5017 - val_loss: 0.4839\n",
      "Epoch 83/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5000 - val_loss: 0.4822\n",
      "Epoch 84/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4986 - val_loss: 0.4809\n",
      "Epoch 85/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4971 - val_loss: 0.4799\n",
      "Epoch 86/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4955 - val_loss: 0.4781\n",
      "Epoch 87/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4940 - val_loss: 0.4772\n",
      "Epoch 88/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4926 - val_loss: 0.4760\n",
      "Epoch 89/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4912 - val_loss: 0.4745\n",
      "Epoch 90/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4899 - val_loss: 0.4736\n",
      "Epoch 91/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4886 - val_loss: 0.4726\n",
      "Epoch 92/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4873 - val_loss: 0.4712\n",
      "Epoch 93/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4860 - val_loss: 0.4703\n",
      "Epoch 94/100\n",
      "7740/7740 [==============================] - 1s 65us/sample - loss: 0.4847 - val_loss: 0.4698\n",
      "Epoch 95/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4835 - val_loss: 0.4685\n",
      "Epoch 96/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4824 - val_loss: 0.4677\n",
      "Epoch 97/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4813 - val_loss: 0.4666\n",
      "Epoch 98/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4802 - val_loss: 0.4656\n",
      "Epoch 99/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4791 - val_loss: 0.4650\n",
      "Epoch 100/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4780 - val_loss: 0.4640\n",
      "3870/3870 [==============================] - 0s 29us/sample - loss: 0.4652\n",
      "[CV]  learning_rate=0.00030107783636342726, n_hidden=3, n_neurons=21, total=  47.9s\n",
      "[CV] learning_rate=0.00030107783636342726, n_hidden=3, n_neurons=21 ..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 90us/sample - loss: 4.2984 - val_loss: 2.9528\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 2.2613 - val_loss: 1.7212\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 1.5068 - val_loss: 1.3127\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 1.2163 - val_loss: 1.1095\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 1.0437 - val_loss: 0.9690\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.9242 - val_loss: 0.8707\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.8428 - val_loss: 0.8024\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.7883 - val_loss: 0.7552\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.7515 - val_loss: 0.7222\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.7259 - val_loss: 0.6976\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.7067 - val_loss: 0.6790\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.6918 - val_loss: 0.6639\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.6793 - val_loss: 0.6513\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.6685 - val_loss: 0.6404\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.6591 - val_loss: 0.6308\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6505 - val_loss: 0.6223\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.6428 - val_loss: 0.6146\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6356 - val_loss: 0.6077\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6291 - val_loss: 0.6011\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.6229 - val_loss: 0.5951\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.6171 - val_loss: 0.5898\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.6118 - val_loss: 0.5844\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6066 - val_loss: 0.5795\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6018 - val_loss: 0.5749\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5972 - val_loss: 0.5705\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.5929 - val_loss: 0.5663\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 1s 65us/sample - loss: 0.5885 - val_loss: 0.5623\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.5847 - val_loss: 0.5585\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5808 - val_loss: 0.5549\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5771 - val_loss: 0.5514\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.5734 - val_loss: 0.5482\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.5700 - val_loss: 0.5450\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5667 - val_loss: 0.5417\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5634 - val_loss: 0.5386\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5602 - val_loss: 0.5355\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5572 - val_loss: 0.5326\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5541 - val_loss: 0.5298\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5511 - val_loss: 0.5271\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5483 - val_loss: 0.5246\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5456 - val_loss: 0.5220\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5429 - val_loss: 0.5195\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5402 - val_loss: 0.5172\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5377 - val_loss: 0.5149\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5351 - val_loss: 0.5127\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5326 - val_loss: 0.5106\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5302 - val_loss: 0.5084\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5279 - val_loss: 0.5062\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5255 - val_loss: 0.5043\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5233 - val_loss: 0.5020\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5211 - val_loss: 0.5002\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5189 - val_loss: 0.4986\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5169 - val_loss: 0.4964\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.5148 - val_loss: 0.4948\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5128 - val_loss: 0.4931\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.5108 - val_loss: 0.4914\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5089 - val_loss: 0.4900\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5069 - val_loss: 0.4885\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5051 - val_loss: 0.4866\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5033 - val_loss: 0.4851\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.5015 - val_loss: 0.4837\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: 0.4998 - val_loss: 0.4820\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4981 - val_loss: 0.4807\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4964 - val_loss: 0.4793\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4947 - val_loss: 0.4783\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4932 - val_loss: 0.4769\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4917 - val_loss: 0.4756\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4901 - val_loss: 0.4740\n",
      "Epoch 68/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4887 - val_loss: 0.4729\n",
      "Epoch 69/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4873 - val_loss: 0.4721\n",
      "Epoch 70/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4859 - val_loss: 0.4708\n",
      "Epoch 71/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4844 - val_loss: 0.4695\n",
      "Epoch 72/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4831 - val_loss: 0.4684\n",
      "Epoch 73/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4817 - val_loss: 0.4676\n",
      "Epoch 74/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4805 - val_loss: 0.4664\n",
      "Epoch 75/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4792 - val_loss: 0.4652\n",
      "Epoch 76/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4779 - val_loss: 0.4641\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4767 - val_loss: 0.4631\n",
      "Epoch 78/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4755 - val_loss: 0.4622\n",
      "Epoch 79/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4743 - val_loss: 0.4612\n",
      "Epoch 80/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4731 - val_loss: 0.4607\n",
      "Epoch 81/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4720 - val_loss: 0.4597\n",
      "Epoch 82/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4708 - val_loss: 0.4586\n",
      "Epoch 83/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4697 - val_loss: 0.4580\n",
      "Epoch 84/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4686 - val_loss: 0.4568\n",
      "Epoch 85/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4675 - val_loss: 0.4560\n",
      "Epoch 86/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4664 - val_loss: 0.4549\n",
      "Epoch 87/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4652 - val_loss: 0.4538\n",
      "Epoch 88/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4642 - val_loss: 0.4531\n",
      "Epoch 89/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4632 - val_loss: 0.4522\n",
      "Epoch 90/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4622 - val_loss: 0.4515\n",
      "Epoch 91/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4612 - val_loss: 0.4508\n",
      "Epoch 92/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4602 - val_loss: 0.4502\n",
      "Epoch 93/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4591 - val_loss: 0.4493\n",
      "Epoch 94/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.4581 - val_loss: 0.4484\n",
      "Epoch 95/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: 0.4571 - val_loss: 0.4483\n",
      "Epoch 96/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4562 - val_loss: 0.4470\n",
      "Epoch 97/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4552 - val_loss: 0.4464\n",
      "Epoch 98/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4543 - val_loss: 0.4458\n",
      "Epoch 99/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4534 - val_loss: 0.4452\n",
      "Epoch 100/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4525 - val_loss: 0.4445\n",
      "3870/3870 [==============================] - 0s 29us/sample - loss: 0.4531\n",
      "[CV]  learning_rate=0.00030107783636342726, n_hidden=3, n_neurons=21, total=  47.7s\n",
      "[CV] learning_rate=0.005153286333701512, n_hidden=1, n_neurons=22 ....\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 79us/sample - loss: 1.6484 - val_loss: 0.6968\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.6277 - val_loss: 0.5625\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5522 - val_loss: 0.5355\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5281 - val_loss: 0.5123\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4999 - val_loss: 0.4958\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4887 - val_loss: 0.4854\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4807 - val_loss: 0.4758\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4724 - val_loss: 0.4694\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4616 - val_loss: 0.4631\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4562 - val_loss: 0.4584\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4516 - val_loss: 0.4543\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4474 - val_loss: 0.4518\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4433 - val_loss: 0.4447\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4403 - val_loss: 0.4462\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4366 - val_loss: 0.4435\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4353 - val_loss: 0.4400\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4308 - val_loss: 0.4330\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4279 - val_loss: 0.4332\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4275 - val_loss: 0.4294\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4243 - val_loss: 0.4296\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4235 - val_loss: 0.4294\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4201 - val_loss: 0.4320\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4200 - val_loss: 0.4261\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4182 - val_loss: 0.4205\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4131 - val_loss: 0.4237\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4111 - val_loss: 0.4177\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4138 - val_loss: 0.4171\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4084 - val_loss: 0.4148\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4066 - val_loss: 0.4132\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4076 - val_loss: 0.4109\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4053 - val_loss: 0.4265\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4097 - val_loss: 0.4144\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4046 - val_loss: 0.4180\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3999 - val_loss: 0.4043\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3992 - val_loss: 0.4095\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3974 - val_loss: 0.4123\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4054 - val_loss: 0.4034\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3929 - val_loss: 0.4038\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3920 - val_loss: 0.3979\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3946 - val_loss: 0.4058\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3959 - val_loss: 0.3984\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3893 - val_loss: 0.4007\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3873 - val_loss: 0.3931\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3867 - val_loss: 0.3940\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3835 - val_loss: 0.3890\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3838 - val_loss: 0.3908\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3824 - val_loss: 0.3943\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3836 - val_loss: 0.3891\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3798 - val_loss: 0.4057\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3856 - val_loss: 0.3885\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3777 - val_loss: 0.3876\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3762 - val_loss: 0.3954\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3760 - val_loss: 0.3812\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3771 - val_loss: 0.3871\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3748 - val_loss: 0.3789\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3725 - val_loss: 0.3860\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3815 - val_loss: 0.3847\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3765 - val_loss: 0.3764\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3836 - val_loss: 0.3813\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3903 - val_loss: 0.3791\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3780 - val_loss: 0.3787\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3903 - val_loss: 0.3817\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5034 - val_loss: 0.3887\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5258 - val_loss: 0.3959\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4422 - val_loss: 0.3916\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3792 - val_loss: 0.3867\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3802 - val_loss: 0.3957\n",
      "Epoch 68/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3779 - val_loss: 0.3793\n",
      "3870/3870 [==============================] - 0s 27us/sample - loss: 0.4175\n",
      "[CV]  learning_rate=0.005153286333701512, n_hidden=1, n_neurons=22, total=  29.4s\n",
      "[CV] learning_rate=0.005153286333701512, n_hidden=1, n_neurons=22 ....\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 79us/sample - loss: 1.1876 - val_loss: 0.6571\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.6474 - val_loss: 0.5878\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5965 - val_loss: 0.5504\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5661 - val_loss: 0.5343\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5463 - val_loss: 0.5117\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5303 - val_loss: 0.4995\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5246 - val_loss: 0.4937\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5131 - val_loss: 0.4837\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5060 - val_loss: 0.4890\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5010 - val_loss: 0.4814\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4995 - val_loss: 0.4774\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4915 - val_loss: 0.4723\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4866 - val_loss: 0.4679\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4845 - val_loss: 0.4655\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4804 - val_loss: 0.4617\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4779 - val_loss: 0.4574\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4736 - val_loss: 0.4552\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4695 - val_loss: 0.4543\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4667 - val_loss: 0.4501\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4656 - val_loss: 0.4486\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4612 - val_loss: 0.4461\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4582 - val_loss: 0.4462\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4580 - val_loss: 0.4514\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4554 - val_loss: 0.4443\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4547 - val_loss: 0.4403\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4482 - val_loss: 0.4407\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4467 - val_loss: 0.4345\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4434 - val_loss: 0.4350\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4418 - val_loss: 0.4345\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4394 - val_loss: 0.4305\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4373 - val_loss: 0.4266\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4352 - val_loss: 0.4290\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4338 - val_loss: 0.4245\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4315 - val_loss: 0.4231\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4303 - val_loss: 0.4240\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4286 - val_loss: 0.4204\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4273 - val_loss: 0.4204\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4252 - val_loss: 0.4190\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4234 - val_loss: 0.4160\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4227 - val_loss: 0.4124\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4209 - val_loss: 0.4120\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4194 - val_loss: 0.4112\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4180 - val_loss: 0.4134\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4168 - val_loss: 0.4098\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4149 - val_loss: 0.4082\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4160 - val_loss: 0.4078\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4122 - val_loss: 0.4067\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4115 - val_loss: 0.4078\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4090 - val_loss: 0.4021\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4082 - val_loss: 0.4039\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4070 - val_loss: 0.4026\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4056 - val_loss: 0.3977\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4057 - val_loss: 0.3986\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4034 - val_loss: 0.3977\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4009 - val_loss: 0.3937\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4003 - val_loss: 0.3981\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3993 - val_loss: 0.3926\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3987 - val_loss: 0.3914\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3962 - val_loss: 0.3948\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3958 - val_loss: 0.3895\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3950 - val_loss: 0.3909\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3960 - val_loss: 0.3929\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3932 - val_loss: 0.3864\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3914 - val_loss: 0.3871\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3908 - val_loss: 0.3866\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3906 - val_loss: 0.3846\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3894 - val_loss: 0.3848\n",
      "Epoch 68/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3889 - val_loss: 0.3854\n",
      "Epoch 69/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3874 - val_loss: 0.3843\n",
      "Epoch 70/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3883 - val_loss: 0.3830\n",
      "Epoch 71/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3858 - val_loss: 0.3854\n",
      "Epoch 72/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3847 - val_loss: 0.3787\n",
      "Epoch 73/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3839 - val_loss: 0.3804\n",
      "Epoch 74/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3832 - val_loss: 0.3814\n",
      "Epoch 75/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3821 - val_loss: 0.3775\n",
      "Epoch 76/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3823 - val_loss: 0.3771\n",
      "Epoch 77/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3812 - val_loss: 0.3769\n",
      "Epoch 78/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3805 - val_loss: 0.3767\n",
      "Epoch 79/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3798 - val_loss: 0.3799\n",
      "Epoch 80/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3793 - val_loss: 0.3759\n",
      "Epoch 81/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3788 - val_loss: 0.3807\n",
      "Epoch 82/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3791 - val_loss: 0.3742\n",
      "Epoch 83/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3757 - val_loss: 0.3758\n",
      "Epoch 84/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3773 - val_loss: 0.3709\n",
      "Epoch 85/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3765 - val_loss: 0.3755\n",
      "Epoch 86/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3798 - val_loss: 0.3704\n",
      "Epoch 87/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3747 - val_loss: 0.3728\n",
      "Epoch 88/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3762 - val_loss: 0.3737\n",
      "Epoch 89/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3746 - val_loss: 0.3678\n",
      "Epoch 90/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3737 - val_loss: 0.3701\n",
      "Epoch 91/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3722 - val_loss: 0.3664\n",
      "Epoch 92/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3709 - val_loss: 0.3670\n",
      "Epoch 93/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3711 - val_loss: 0.3669\n",
      "Epoch 94/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3747 - val_loss: 0.3693\n",
      "Epoch 95/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3706 - val_loss: 0.3747\n",
      "Epoch 96/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3710 - val_loss: 0.3659\n",
      "Epoch 97/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3714 - val_loss: 0.3646\n",
      "Epoch 98/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3678 - val_loss: 0.3639\n",
      "Epoch 99/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3675 - val_loss: 0.3636\n",
      "Epoch 100/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3673 - val_loss: 0.3611\n",
      "3870/3870 [==============================] - 0s 27us/sample - loss: 0.3563\n",
      "[CV]  learning_rate=0.005153286333701512, n_hidden=1, n_neurons=22, total=  43.2s\n",
      "[CV] learning_rate=0.005153286333701512, n_hidden=1, n_neurons=22 ....\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 78us/sample - loss: 1.4284 - val_loss: 0.9726\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 2.1060 - val_loss: 1.2393\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 2.0705 - val_loss: 1.2908\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.7344 - val_loss: 0.6105\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.6072 - val_loss: 0.5429\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5535 - val_loss: 0.5028\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5159 - val_loss: 0.4697\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4784 - val_loss: 0.4384\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4541 - val_loss: 0.4295\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4405 - val_loss: 0.4171\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4316 - val_loss: 0.4141\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4248 - val_loss: 0.4047\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4192 - val_loss: 0.4057\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4150 - val_loss: 0.3992\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4104 - val_loss: 0.3954\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4072 - val_loss: 0.3949\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4033 - val_loss: 0.3941\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3992 - val_loss: 0.3879\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3965 - val_loss: 0.3833\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3939 - val_loss: 0.3812\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3912 - val_loss: 0.3800\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3890 - val_loss: 0.3759\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3869 - val_loss: 0.3751\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3849 - val_loss: 0.3742\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3831 - val_loss: 0.3720\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3824 - val_loss: 0.3698\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3806 - val_loss: 0.3709\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3792 - val_loss: 0.3708\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3785 - val_loss: 0.3686\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3767 - val_loss: 0.3676\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3752 - val_loss: 0.3698\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3750 - val_loss: 0.3672\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3738 - val_loss: 0.3645\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3726 - val_loss: 0.3657\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3714 - val_loss: 0.3616\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3716 - val_loss: 0.3656\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3705 - val_loss: 0.3606\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3696 - val_loss: 0.3632\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3690 - val_loss: 0.3629\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3683 - val_loss: 0.3600\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3682 - val_loss: 0.3584\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3676 - val_loss: 0.3588\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3671 - val_loss: 0.3593\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3666 - val_loss: 0.3590\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3660 - val_loss: 0.3590\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3653 - val_loss: 0.3665\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3646 - val_loss: 0.3589\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3646 - val_loss: 0.3583\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3636 - val_loss: 0.3559\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3633 - val_loss: 0.3565\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3630 - val_loss: 0.3571\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3629 - val_loss: 0.3554\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3616 - val_loss: 0.3565\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3619 - val_loss: 0.3565\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3620 - val_loss: 0.3562\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3616 - val_loss: 0.3579\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3612 - val_loss: 0.3582\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3607 - val_loss: 0.3493\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3603 - val_loss: 0.3545\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3601 - val_loss: 0.3542\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3587 - val_loss: 0.3540\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3588 - val_loss: 0.3502\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3587 - val_loss: 0.3498\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3580 - val_loss: 0.3525\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3582 - val_loss: 0.3537\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3591 - val_loss: 0.3508\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3578 - val_loss: 0.3499\n",
      "Epoch 68/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3573 - val_loss: 0.3505\n",
      "3870/3870 [==============================] - 0s 27us/sample - loss: 0.3639\n",
      "[CV]  learning_rate=0.005153286333701512, n_hidden=1, n_neurons=22, total=  29.5s\n",
      "[CV] learning_rate=0.0003099230412972121, n_hidden=0, n_neurons=49 ...\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 75us/sample - loss: 6.0440 - val_loss: 4.9588\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 4.4263 - val_loss: 3.7128\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 3.3328 - val_loss: 2.8501\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 2.5774 - val_loss: 2.2438\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 2.0473 - val_loss: 1.8122\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 1.6701 - val_loss: 1.5018\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 1.3989 - val_loss: 1.2770\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 1.2024 - val_loss: 1.1129\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 1.0590 - val_loss: 0.9926\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.9536 - val_loss: 0.9037\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.8756 - val_loss: 0.8376\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.8175 - val_loss: 0.7883\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.7740 - val_loss: 0.7512\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.7411 - val_loss: 0.7230\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.7161 - val_loss: 0.7015\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.6970 - val_loss: 0.6851\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.6821 - val_loss: 0.6721\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.6704 - val_loss: 0.6617\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6610 - val_loss: 0.6535\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.6534 - val_loss: 0.6467\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.6471 - val_loss: 0.6410\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6419 - val_loss: 0.6362\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.6373 - val_loss: 0.6320\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6333 - val_loss: 0.6283\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.6297 - val_loss: 0.6249\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.6265 - val_loss: 0.6219\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6236 - val_loss: 0.6191\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.6208 - val_loss: 0.6164\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6182 - val_loss: 0.6139\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.6158 - val_loss: 0.6115\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6134 - val_loss: 0.6093\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6112 - val_loss: 0.6071\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6090 - val_loss: 0.6050\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6069 - val_loss: 0.6030\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.6049 - val_loss: 0.6010\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6030 - val_loss: 0.5991\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.6010 - val_loss: 0.5972\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5992 - val_loss: 0.5954\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5974 - val_loss: 0.5936\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5956 - val_loss: 0.5918\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5939 - val_loss: 0.5902\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5922 - val_loss: 0.5885\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5906 - val_loss: 0.5869\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5890 - val_loss: 0.5853\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5874 - val_loss: 0.5838\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5859 - val_loss: 0.5823\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5844 - val_loss: 0.5808\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5830 - val_loss: 0.5794\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5816 - val_loss: 0.5780\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5802 - val_loss: 0.5766\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5788 - val_loss: 0.5753\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5775 - val_loss: 0.5740\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5762 - val_loss: 0.5727\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5750 - val_loss: 0.5715\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.5737 - val_loss: 0.5703\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5725 - val_loss: 0.5690\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5713 - val_loss: 0.5678\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5702 - val_loss: 0.5667\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5691 - val_loss: 0.5656\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5680 - val_loss: 0.5645\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5669 - val_loss: 0.5635\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5658 - val_loss: 0.5625\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5648 - val_loss: 0.5614\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5638 - val_loss: 0.5604\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5628 - val_loss: 0.5594\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5619 - val_loss: 0.5585\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5610 - val_loss: 0.5576\n",
      "Epoch 68/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5600 - val_loss: 0.5566\n",
      "Epoch 69/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5592 - val_loss: 0.5558\n",
      "Epoch 70/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5583 - val_loss: 0.5549\n",
      "Epoch 71/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5574 - val_loss: 0.5541\n",
      "Epoch 72/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5566 - val_loss: 0.5532\n",
      "Epoch 73/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5558 - val_loss: 0.5524\n",
      "Epoch 74/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5550 - val_loss: 0.5516\n",
      "Epoch 75/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5542 - val_loss: 0.5509\n",
      "Epoch 76/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5535 - val_loss: 0.5501\n",
      "Epoch 77/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5527 - val_loss: 0.5493\n",
      "Epoch 78/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5520 - val_loss: 0.5485\n",
      "Epoch 79/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5513 - val_loss: 0.5478\n",
      "Epoch 80/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5506 - val_loss: 0.5472\n",
      "Epoch 81/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5499 - val_loss: 0.5466\n",
      "Epoch 82/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5493 - val_loss: 0.5458\n",
      "Epoch 83/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5486 - val_loss: 0.5452\n",
      "Epoch 84/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5480 - val_loss: 0.5445\n",
      "Epoch 85/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5474 - val_loss: 0.5439\n",
      "Epoch 86/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5468 - val_loss: 0.5434\n",
      "Epoch 87/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5462 - val_loss: 0.5428\n",
      "Epoch 88/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5456 - val_loss: 0.5422\n",
      "Epoch 89/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5450 - val_loss: 0.5416\n",
      "Epoch 90/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5445 - val_loss: 0.5411\n",
      "Epoch 91/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5440 - val_loss: 0.5405\n",
      "Epoch 92/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5434 - val_loss: 0.5399\n",
      "Epoch 93/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5429 - val_loss: 0.5395\n",
      "Epoch 94/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5424 - val_loss: 0.5390\n",
      "Epoch 95/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5419 - val_loss: 0.5385\n",
      "Epoch 96/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5415 - val_loss: 0.5380\n",
      "Epoch 97/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5410 - val_loss: 0.5376\n",
      "Epoch 98/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5405 - val_loss: 0.5371\n",
      "Epoch 99/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5401 - val_loss: 0.5367\n",
      "Epoch 100/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5397 - val_loss: 0.5362\n",
      "3870/3870 [==============================] - 0s 26us/sample - loss: 0.5892\n",
      "[CV]  learning_rate=0.0003099230412972121, n_hidden=0, n_neurons=49, total=  42.0s\n",
      "[CV] learning_rate=0.0003099230412972121, n_hidden=0, n_neurons=49 ...\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 75us/sample - loss: 7.1924 - val_loss: 6.3003\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 5.4856 - val_loss: 4.8475\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 4.2336 - val_loss: 3.7757\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 3.3127 - val_loss: 2.9845\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 2.6350 - val_loss: 2.3986\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 2.1351 - val_loss: 1.9640\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 1.7655 - val_loss: 1.6410\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 1.4925 - val_loss: 1.4002\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 1.2899 - val_loss: 1.2206\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 1.1395 - val_loss: 1.0860\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 54us/sample - loss: 1.0276 - val_loss: 0.9847\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.9441 - val_loss: 0.9082\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.8815 - val_loss: 0.8498\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.8343 - val_loss: 0.8050\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.7985 - val_loss: 0.7706\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.7712 - val_loss: 0.7437\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.7504 - val_loss: 0.7227\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.7340 - val_loss: 0.7058\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.7211 - val_loss: 0.6923\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.7109 - val_loss: 0.6812\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.7026 - val_loss: 0.6720\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6958 - val_loss: 0.6642\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.6901 - val_loss: 0.6578\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6853 - val_loss: 0.6520\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6811 - val_loss: 0.6470\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.6773 - val_loss: 0.6425\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6740 - val_loss: 0.6384\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.6709 - val_loss: 0.6347\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6681 - val_loss: 0.6312\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6655 - val_loss: 0.6281\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.6630 - val_loss: 0.6250\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.6606 - val_loss: 0.6223\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.6585 - val_loss: 0.6197\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.6563 - val_loss: 0.6171\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.6542 - val_loss: 0.6146\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.6522 - val_loss: 0.6124\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6502 - val_loss: 0.6102\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.6483 - val_loss: 0.6080\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.6465 - val_loss: 0.6059\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.6447 - val_loss: 0.6040\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.6429 - val_loss: 0.6019\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.6413 - val_loss: 0.6000\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.6395 - val_loss: 0.5983\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.6380 - val_loss: 0.5965\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.6364 - val_loss: 0.5949\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.6348 - val_loss: 0.5933\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.6333 - val_loss: 0.5916\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.6317 - val_loss: 0.5900\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.6303 - val_loss: 0.5886\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.6289 - val_loss: 0.5871\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.6274 - val_loss: 0.5857\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.6261 - val_loss: 0.5843\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.6247 - val_loss: 0.5830\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6235 - val_loss: 0.5816\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6221 - val_loss: 0.5803\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6208 - val_loss: 0.5788\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.6195 - val_loss: 0.5774\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.6183 - val_loss: 0.5762\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.6171 - val_loss: 0.5751\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.6159 - val_loss: 0.5739\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.6148 - val_loss: 0.5728\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.6137 - val_loss: 0.5717\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.6125 - val_loss: 0.5705\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.6114 - val_loss: 0.5694\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.6103 - val_loss: 0.5683\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.6093 - val_loss: 0.5674\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.6082 - val_loss: 0.5665\n",
      "Epoch 68/100\n",
      "7740/7740 [==============================] - 1s 65us/sample - loss: 0.6071 - val_loss: 0.5654\n",
      "Epoch 69/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.6061 - val_loss: 0.5645\n",
      "Epoch 70/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.6052 - val_loss: 0.5637\n",
      "Epoch 71/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.6043 - val_loss: 0.5628\n",
      "Epoch 72/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.6033 - val_loss: 0.5617\n",
      "Epoch 73/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.6024 - val_loss: 0.5608\n",
      "Epoch 74/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.6014 - val_loss: 0.5598\n",
      "Epoch 75/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.6005 - val_loss: 0.5592\n",
      "Epoch 76/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5996 - val_loss: 0.5582\n",
      "Epoch 77/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5988 - val_loss: 0.5573\n",
      "Epoch 78/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.5979 - val_loss: 0.5564\n",
      "Epoch 79/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.5970 - val_loss: 0.5555\n",
      "Epoch 80/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5963 - val_loss: 0.5549\n",
      "Epoch 81/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5954 - val_loss: 0.5543\n",
      "Epoch 82/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.5945 - val_loss: 0.5534\n",
      "Epoch 83/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5939 - val_loss: 0.5526\n",
      "Epoch 84/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5931 - val_loss: 0.5519\n",
      "Epoch 85/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.5923 - val_loss: 0.5513\n",
      "Epoch 86/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.5916 - val_loss: 0.5507\n",
      "Epoch 87/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5909 - val_loss: 0.5500\n",
      "Epoch 88/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5902 - val_loss: 0.5494\n",
      "Epoch 89/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.5894 - val_loss: 0.5486\n",
      "Epoch 90/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5888 - val_loss: 0.5481\n",
      "Epoch 91/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5881 - val_loss: 0.5476\n",
      "Epoch 92/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5874 - val_loss: 0.5468\n",
      "Epoch 93/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.5868 - val_loss: 0.5463\n",
      "Epoch 94/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5861 - val_loss: 0.5458\n",
      "Epoch 95/100\n",
      "7740/7740 [==============================] - 1s 66us/sample - loss: 0.5855 - val_loss: 0.5453\n",
      "Epoch 96/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.5849 - val_loss: 0.5448\n",
      "Epoch 97/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5843 - val_loss: 0.5442\n",
      "Epoch 98/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5837 - val_loss: 0.5436\n",
      "Epoch 99/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.5830 - val_loss: 0.5431\n",
      "Epoch 100/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5825 - val_loss: 0.5426\n",
      "3870/3870 [==============================] - 0s 28us/sample - loss: 0.5481\n",
      "[CV]  learning_rate=0.0003099230412972121, n_hidden=0, n_neurons=49, total=  43.7s\n",
      "[CV] learning_rate=0.0003099230412972121, n_hidden=0, n_neurons=49 ...\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 82us/sample - loss: 8.8738 - val_loss: 6.9099\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 6.3460 - val_loss: 5.0147\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 4.6335 - val_loss: 3.7206\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 3.4590 - val_loss: 2.8258\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 2.6446 - val_loss: 2.1995\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 2.0745 - val_loss: 1.7574\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 1.6719 - val_loss: 1.4423\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 1.3855 - val_loss: 1.2161\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 1.1804 - val_loss: 1.0528\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 1.0329 - val_loss: 0.9341\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.9259 - val_loss: 0.8473\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.8480 - val_loss: 0.7835\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.7909 - val_loss: 0.7361\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.7487 - val_loss: 0.7009\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.7174 - val_loss: 0.6744\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.6940 - val_loss: 0.6544\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.6764 - val_loss: 0.6392\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.6630 - val_loss: 0.6274\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.6526 - val_loss: 0.6182\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.6446 - val_loss: 0.6110\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.6382 - val_loss: 0.6052\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.6330 - val_loss: 0.6005\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.6288 - val_loss: 0.5966\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.6253 - val_loss: 0.5933\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.6222 - val_loss: 0.5904\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.6196 - val_loss: 0.5879\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.6172 - val_loss: 0.5857\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6151 - val_loss: 0.5836\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.6132 - val_loss: 0.5817\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.6114 - val_loss: 0.5800\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6096 - val_loss: 0.5784\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6080 - val_loss: 0.5768\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.6065 - val_loss: 0.5753\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.6050 - val_loss: 0.5739\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.6036 - val_loss: 0.5725\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.6022 - val_loss: 0.5712\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.6008 - val_loss: 0.5699\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.5995 - val_loss: 0.5687\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5983 - val_loss: 0.5675\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5970 - val_loss: 0.5663\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5958 - val_loss: 0.5651\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5946 - val_loss: 0.5640\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5935 - val_loss: 0.5629\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5924 - val_loss: 0.5619\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5913 - val_loss: 0.5608\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.5902 - val_loss: 0.5598\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.5891 - val_loss: 0.5588\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5881 - val_loss: 0.5578\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5871 - val_loss: 0.5569\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5861 - val_loss: 0.5559\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5851 - val_loss: 0.5550\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5842 - val_loss: 0.5541\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5832 - val_loss: 0.5532\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5823 - val_loss: 0.5524\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5814 - val_loss: 0.5515\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5805 - val_loss: 0.5507\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5797 - val_loss: 0.5499\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5788 - val_loss: 0.5491\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5780 - val_loss: 0.5483\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5772 - val_loss: 0.5476\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5764 - val_loss: 0.5468\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5757 - val_loss: 0.5461\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5749 - val_loss: 0.5454\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5742 - val_loss: 0.5447\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5734 - val_loss: 0.5440\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5727 - val_loss: 0.5434\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5720 - val_loss: 0.5427\n",
      "Epoch 68/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5713 - val_loss: 0.5421\n",
      "Epoch 69/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5707 - val_loss: 0.5414\n",
      "Epoch 70/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5700 - val_loss: 0.5408\n",
      "Epoch 71/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5694 - val_loss: 0.5402\n",
      "Epoch 72/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5688 - val_loss: 0.5396\n",
      "Epoch 73/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5682 - val_loss: 0.5391\n",
      "Epoch 74/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5676 - val_loss: 0.5385\n",
      "Epoch 75/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5670 - val_loss: 0.5380\n",
      "Epoch 76/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5664 - val_loss: 0.5374\n",
      "Epoch 77/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5658 - val_loss: 0.5369\n",
      "Epoch 78/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.5653 - val_loss: 0.5364\n",
      "Epoch 79/100\n",
      "7740/7740 [==============================] - 1s 65us/sample - loss: 0.5647 - val_loss: 0.5359\n",
      "Epoch 80/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.5642 - val_loss: 0.5354\n",
      "Epoch 81/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5637 - val_loss: 0.5349\n",
      "Epoch 82/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5632 - val_loss: 0.5344\n",
      "Epoch 83/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5627 - val_loss: 0.5340\n",
      "Epoch 84/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5622 - val_loss: 0.5335\n",
      "Epoch 85/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5617 - val_loss: 0.5330\n",
      "Epoch 86/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5612 - val_loss: 0.5326\n",
      "Epoch 87/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5608 - val_loss: 0.5322\n",
      "Epoch 88/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5603 - val_loss: 0.5317\n",
      "Epoch 89/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5599 - val_loss: 0.5313\n",
      "Epoch 90/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5594 - val_loss: 0.5309\n",
      "Epoch 91/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5590 - val_loss: 0.5305\n",
      "Epoch 92/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5586 - val_loss: 0.5302\n",
      "Epoch 93/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.5582 - val_loss: 0.5298\n",
      "Epoch 94/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5578 - val_loss: 0.5294\n",
      "Epoch 95/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5574 - val_loss: 0.5291\n",
      "Epoch 96/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.5570 - val_loss: 0.5287\n",
      "Epoch 97/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.5566 - val_loss: 0.5284\n",
      "Epoch 98/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5563 - val_loss: 0.5281\n",
      "Epoch 99/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.5559 - val_loss: 0.5278\n",
      "Epoch 100/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5556 - val_loss: 0.5274\n",
      "3870/3870 [==============================] - 0s 29us/sample - loss: 0.5454\n",
      "[CV]  learning_rate=0.0003099230412972121, n_hidden=0, n_neurons=49, total=  42.9s\n",
      "[CV] learning_rate=0.0033625641252688094, n_hidden=2, n_neurons=42 ...\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 86us/sample - loss: 1.2816 - val_loss: 0.6602\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 1s 65us/sample - loss: 0.6274 - val_loss: 0.5918\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5690 - val_loss: 0.5514\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.5298 - val_loss: 0.5219\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.5043 - val_loss: 0.4964\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4829 - val_loss: 0.4813\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4695 - val_loss: 0.4705\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 1s 65us/sample - loss: 0.4577 - val_loss: 0.4597\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.4485 - val_loss: 0.4515\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4409 - val_loss: 0.4458\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4342 - val_loss: 0.4407\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4293 - val_loss: 0.4379\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.4252 - val_loss: 0.4310\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4232 - val_loss: 0.4329\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.4175 - val_loss: 0.4295\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4150 - val_loss: 0.4256\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4107 - val_loss: 0.4177\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4079 - val_loss: 0.4155\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4046 - val_loss: 0.4125\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4013 - val_loss: 0.4109\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3991 - val_loss: 0.4126\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.3963 - val_loss: 0.4111\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3941 - val_loss: 0.4045\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.3900 - val_loss: 0.4021\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.3874 - val_loss: 0.4024\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.3844 - val_loss: 0.3975\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3824 - val_loss: 0.3962\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.3797 - val_loss: 0.3917\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 1s 66us/sample - loss: 0.3778 - val_loss: 0.3899\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.3754 - val_loss: 0.3866\n",
      "Epoch 31/100\n",
      "1184/7740 [===>..........................] - ETA: 0s - loss: 0.3662"
     ]
    }
   ],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [0, 1, 2, 3],\n",
    "    \"n_neurons\": np.arange(1, 100),\n",
    "    \"learning_rate\": reciprocal(3e-4, 3e-2),\n",
    "}\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3, verbose=2)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
    "                  validation_data=(X_valid, y_valid),\n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RandomizedSearchCXV utiliza validación cruzada de K iteraciones, así no ua X_valid ni y y_valid que se emplean sol para detención temprana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La exploración puede durar horas, dependiendo del hardware, el tamaño del conjunto de datos la complejidad el modelo y los valores de n_iter y cv. \n",
    "- Cuando termina podemos acceder a los mejores params encontrados, la mejor puntuación y el modelo keras entrenado así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search_cv.best_estimator_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
